% Slides for fys4411

\documentclass[compress]{beamer}


% Try the class options [notes], [notes=only], [trans], [handout],
% [red], [compress], [draft], [class=article] and see what happens!

% For a green structure color use:
%\colorlet{structure}{green!50!black}

\mode<article> % only for the article version
{
  \usepackage{beamerbasearticle}
  \usepackage{fullpage}
  \usepackage{hyperref}
}

\beamertemplateshadingbackground{red!10}{blue!10}

%\usetheme{Hannover}

\setbeamertemplate{footline}[page number]


%\usepackage{beamerthemeshadow}



\usepackage{pgf,pgfarrows,pgfnodes,pgfautomata,pgfheaps,pgfshade}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage[latin1]{inputenc}
\usepackage{colortbl}
\usepackage[english]{babel}
\usepackage{listings}
\usepackage{shadow}
\lstset{language=c++}
\lstset{alsolanguage=[90]Fortran}
\lstset{basicstyle=\small}
%\lstset{backgroundcolor=\color{white}}
%\lstset{frame=single}
\lstset{stringstyle=\ttfamily}
%\lstset{keywordstyle=\color{red}\bfseries}
%\lstset{commentstyle=\itshape\color{blue}}
\lstset{showspaces=false}
\lstset{showstringspaces=false}
\lstset{showtabs=false}
\lstset{breaklines}
\usepackage{times}

% Use some nice templates
\beamertemplatetransparentcovereddynamic

% own commands

\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
%\newcommand{\bra}[1]{\left\langle #1 \right|}
%\newcommand{\ket}[1]{\left| # \right\rangle}
\newcommand{\braket}[2]{\left\langle #1 \right| #2 \right\rangle}
\newcommand{\OP}[1]{{\bf\widehat{#1}}}
\newcommand{\matr}[1]{{\bf \cal{#1}}}
\newcommand{\beN}{\begin{equation*}}
\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\beaN}{\begin{eqnarray*}}
\newcommand{\eeN}{\end{equation*}}
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\eeaN}{\end{eqnarray*}}
\newcommand{\bdm}{\begin{displaymath}}
\newcommand{\edm}{\end{displaymath}}
\newcommand{\bsubeqs}{\begin{subequations}}
\newcommand{\esubeqs}{\end{subequations}}
\def\psii{\psi_{i}}
\def\psij{\psi_{j}}
\def\psiij{\psi_{ij}}
\def\psisq{\psi^2}
\def\psisqex{\langle \psi^2 \rangle}
\def\psiR{\psi({\bf R})}
\def\psiRk{\psi({\bf R}_k)}
\def\psiiRk{\psi_{i}(\Rveck)}
\def\psijRk{\psi_{j}(\Rveck)}
\def\psiijRk{\psi_{ij}(\Rveck)}
\def\ranglep{\rangle_{\psisq}}
\def\Hpsibypsi{{H \psi \over \psi}}
\def\Hpsiibypsi{{H \psii \over \psi}}
\def\HmEpsibypsi{{(H-E) \psi \over \psi}}
\def\HmEpsiibypsi{{(H-E) \psii \over \psi}}
\def\HmEpsijbypsi{{(H-E) \psij \over \psi}}
\def\psiibypsi{{\psii \over \psi}}
\def\psijbypsi{{\psij \over \psi}}
\def\psiijbypsi{{\psiij \over \psi}}
\def\psiibypsiRk{{\psii(\Rveck) \over \psi(\Rveck)}}
\def\psijbypsiRk{{\psij(\Rveck) \over \psi(\Rveck)}}
\def\psiijbypsiRk{{\psiij(\Rveck) \over \psi(\Rveck)}}
\def\EL{E_{\rm L}}
\def\ELi{E_{{\rm L},i}}
\def\ELj{E_{{\rm L},j}}
\def\ELRk{E_{\rm L}(\Rveck)}
\def\ELiRk{E_{{\rm L},i}(\Rveck)}
\def\ELjRk{E_{{\rm L},j}(\Rveck)}
\def\Ebar{\bar{E}}
\def\Ei{\Ebar_{i}}
\def\Ej{\Ebar_{j}}
\def\Ebar{\bar{E}}
\def\Rvec{{\bf R}}
\def\Rveck{{\bf R}_k}
\def\Rvecl{{\bf R}_l}
\def\NMC{N_{\rm MC}}
\def\sumMC{\sum_{k=1}^{\NMC}}
\def\MC{Monte Carlo}
\def\adiag{a_{\rm diag}}
\def\tcorr{T_{\rm corr}}
\def\intR{{\int {\rm d}^{3N}\!\!R\;}}

\def\ul{\underline}
\def\beq{\begin{eqnarray}}
\def\eeq{\end{eqnarray}}

\newcommand{\eqbrace}[4]{\left\{
\begin{array}{ll}
#1 & #2 \\[0.5cm]
#3 & #4
\end{array}\right.}
\newcommand{\eqbraced}[4]{\left\{
\begin{array}{ll}
#1 & #2 \\[0.5cm]
#3 & #4
\end{array}\right\}}
\newcommand{\eqbracetriple}[6]{\left\{
\begin{array}{ll}
#1 & #2 \\
#3 & #4 \\
#5 & #6
\end{array}\right.}
\newcommand{\eqbracedtriple}[6]{\left\{
\begin{array}{ll}
#1 & #2 \\
#3 & #4 \\
#5 & #6
\end{array}\right\}}

\newcommand{\mybox}[3]{\mbox{\makebox[#1][#2]{$#3$}}}
\newcommand{\myframedbox}[3]{\mbox{\framebox[#1][#2]{$#3$}}}

%% Infinitesimal (and double infinitesimal), useful at end of integrals
%\newcommand{\ud}[1]{\mathrm d#1}
\newcommand{\ud}[1]{d#1}
\newcommand{\udd}[1]{d^2\!#1}

%% Operators, algebraic matrices, algebraic vectors

%% Operator (hat, bold or bold symbol, whichever you like best):
\newcommand{\op}[1]{\widehat{#1}}
%\newcommand{\op}[1]{\mathbf{#1}}
%\newcommand{\op}[1]{\boldsymbol{#1}}

%% Vector:
\renewcommand{\vec}[1]{\boldsymbol{#1}}

%% Matrix symbol:
%\newcommand{\matr}[1]{\boldsymbol{#1}}
%\newcommand{\bb}[1]{\mathbb{#1}}

%% Determinant symbol:
\renewcommand{\det}[1]{|#1|}

%% Means (expectation values) of varius sizes
\newcommand{\mean}[1]{\langle #1 \rangle}
\newcommand{\meanb}[1]{\big\langle #1 \big\rangle}
\newcommand{\meanbb}[1]{\Big\langle #1 \Big\rangle}
\newcommand{\meanbbb}[1]{\bigg\langle #1 \bigg\rangle}
\newcommand{\meanbbbb}[1]{\Bigg\langle #1 \Bigg\rangle}

%% Shorthands for text set in roman font
\newcommand{\prob}[0]{\mathrm{Prob}} %probability
\newcommand{\cov}[0]{\mathrm{Cov}}   %covariance
\newcommand{\var}[0]{\mathrm{Var}}   %variancd

%% Big-O (typically for specifying the speed scaling of an algorithm)
\newcommand{\bigO}{\mathcal{O}}

%% Real value of a complex number
\newcommand{\real}[1]{\mathrm{Re}\!\left\{#1\right\}}

%% Quantum mechanical state vectors and matrix elements (of different sizes)
%\newcommand{\bra}[1]{\langle #1 |}
\newcommand{\bfv}[1]{\boldsymbol{#1}}                     % vector written as a boldface symbol
\newcommand{\Div}[1]{\nabla \bullet \vbf{#1}}           % define divergence
\newcommand{\Grad}[1]{\boldsymbol{\nabla}{#1}}

%%% DEFINITIOS FOR QUANTUM MECHANICS %%%
\newcommand{\Op}[1]{{\bf\widehat{#1}}}                    % define operator
\newcommand{\Obs}[1]{\langle{\Op{#1}\rangle}}             % define observable
\newcommand{\be}{\begin{equation}}                        % begin equation
\newcommand{\ee}{\end{equation}}                          % end equation
\newcommand{\PsiT}{\bfv{\Psi_T}(\bfv{R})}                       % symbol for trial wave function
\newcommand{\braket}[2]{\langle{#1}|\Op{#2}|{#1}\rangle}
\newcommand{\Det}[1]{{|\bfv{#1}|}}

\newcommand{\brab}[1]{\big\langle #1 \big|}
\newcommand{\brabb}[1]{\Big\langle #1 \Big|}
\newcommand{\brabbb}[1]{\bigg\langle #1 \bigg|}
\newcommand{\brabbbb}[1]{\Bigg\langle #1 \Bigg|}
%\newcommand{\ket}[1]{| #1 \rangle}
\newcommand{\ketb}[1]{\big| #1 \big\rangle}
\newcommand{\ketbb}[1]{\Big| #1 \Big\rangle}
\newcommand{\ketbbb}[1]{\bigg| #1 \bigg\rangle}
\newcommand{\ketbbbb}[1]{\Bigg| #1 \Bigg\rangle}
\newcommand{\overlap}[2]{\langle #1 | #2 \rangle}
\newcommand{\overlapb}[2]{\big\langle #1 \big| #2 \big\rangle}
\newcommand{\overlapbb}[2]{\Big\langle #1 \Big| #2 \Big\rangle}
\newcommand{\overlapbbb}[2]{\bigg\langle #1 \bigg| #2 \bigg\rangle}
\newcommand{\overlapbbbb}[2]{\Bigg\langle #1 \Bigg| #2 \Bigg\rangle}
\newcommand{\bracket}[3]{\langle #1 | #2 | #3 \rangle}
\newcommand{\bracketb}[3]{\big\langle #1 \big| #2 \big| #3 \big\rangle}
\newcommand{\bracketbb}[3]{\Big\langle #1 \Big| #2 \Big| #3 \Big\rangle}
\newcommand{\bracketbbb}[3]{\bigg\langle #1 \bigg| #2 \bigg| #3 \bigg\rangle}
\newcommand{\bracketbbbb}[3]{\Bigg\langle #1 \Bigg| #2 \Bigg| #3 \Bigg\rangle}
\newcommand{\projection}[2]
{| #1 \rangle \langle  #2 |}
\newcommand{\projectionb}[2]
{\big| #1 \big\rangle \big\langle #2 \big|}
\newcommand{\projectionbb}[2]
{ \Big| #1 \Big\rangle \Big\langle #2 \Big|}
\newcommand{\projectionbbb}[2]
{ \bigg| #1 \bigg\rangle \bigg\langle #2 \bigg|}
\newcommand{\projectionbbbb}[2]
{ \Bigg| #1 \Bigg\rangle \Bigg\langle #2 \Bigg|}


%% If you run out of greek symbols, here's another one you haven't
%% thought of:
\newcommand{\Feta}{\hspace{0.6ex}\begin{turn}{180}
        {\raisebox{-\height}{\parbox[c]{1mm}{F}}}\end{turn}}
\newcommand{\feta}{\hspace{-1.6ex}\begin{turn}{180}
        {\raisebox{-\height}{\parbox[b]{4mm}{f}}}\end{turn}}




\title[FYS4411]{Slides from FYS4411 Lectures}
\author[Computational Physics II]{%
  Morten Hjorth-Jensen}
\institute[ORNL, University of Oslo and MSU]{
  \inst{1}
  Department of Physics and Center of Mathematics for Applications\\
  University of Oslo, N-0316 Oslo, Norway}

  
\date[UiO]{Spring  2011}
\subject{FYS4411 Computational Physics II, Computational Quantum Mechanics}


\pgfdeclareimage[width=6cm,angle=270]{pi}{pi}
\pgfdeclareimage[width=4cm,angle=270]{gropp}{gropp}
\pgfdeclareimage[width=5cm,angle=270]{rothman}{rothman}
\pgfdeclareimage[width=4cm,angle=270]{thijssen}{thijssen}
\pgfdeclareimage[width=10cm,angle=270]{BEC_three_peaks}{BEC_three_peaks}

\begin{document}



\frame{\titlepage}




\section[Week 3]{Weeks 3 and 4}
\frame
{
  \frametitle{Topics for Week 4, January 24-28}
  \begin{block}{Introduction, Parallelization, MPI and Variational Monte Carlo}
\begin{itemize}
\item Presentation of topics to be covered and introduction to Many-Body 
physics (Lecture notes chapter 14-19,  Raimes chapter 1 or Thijssen chapter 4).
\item Variational Monte Carlo theory and presentation of project 1. (lecture notes chapter 14, Thijssen chapter 12) 
\item Next week: Introduction to Message Passing Interface (MPI) and parallelization.
(lecture notes chapter 4.7)
\item Assignment for next week: study chapter 14 of Lecture notes or Chapter 12 of Thijssen. 
\end{itemize}
  \end{block}
} 




\frame
{
  \frametitle{24 January - 31  May}
  \begin{block}{Course overview, Computational aspects}
\begin{itemize}
\item Parallelization (MPI), high-performance computing topics and object orientation. Choose
between F95 and/or C++ as programming languages. Python also possible as programming language. 
\item Algorithms for Monte Carlo Simulations (multidimensional integrals), Metropolis-Hastings and importance sampling
algorithms. 
Improved Monte Carlo methods (part 1 of project 1)
\item Statistical analysis of data  from Monte Carlo calculations, blocking method. (part 1 of project 1)
\end{itemize}
  \end{block}
} 





\frame
{
  \frametitle{24 January - 31  May}
  \begin{block}{Course overview, Computational aspects}
\begin{itemize}
\item Search for minima in multidimensional spaces (conjugate gradient method, steepest descent method, quasi-Newton-Raphson, Broyden-Jacobian), part 2 of project 1.
\item Iterative methods for solutions of non-linear equations, part 2 of project 1.
\item Object orientation, both parts
\item Solutions of coupled differential equations  for density functional 
calculations, part 2 of project 1.
\end{itemize}
  \end{block}
} 





\frame
{
  \frametitle{24 January -31 May}
  \begin{block}{Quantum Mechanical Methods and Systems}
\begin{enumerate}
\item Variational Monte Carlo for 'ab initio' studies of quantum mechanical many-body systems.
\item Simulation of quantum dots with extensions to solids.  It can also be extended to three-dimensional systems like atoms or molecules. 
\item Aim of part 1 of the project: understand how to simulate qauntum mechanical systems with many interacting particles using variational Monte Carlo methods.
\end{enumerate}
The methods are relevant for 
atomic, molecular,solid state, materials science, nanotechnology, quantum chemistry  and nuclear physics. 

  \end{block}
} 


\frame
{
  \frametitle{24 January -31 May}
  \begin{block}{Quantum Mechanical Methods and Systems}
\begin{enumerate}
\item Part 2 of the project solves much of the same systems as in part 1 but 
introduces density functional theory.
\item We will compare density functional theory with the VMC results
\item The VMC results will then  be used to constrain a density functional (actual research and could lead to an article)
\item We will also end up writing a density functional code and use this to
compute properties of solids (atoms in a lattice).
\end{enumerate}
DFT and Hartree-Fock theory are covered by the lectures notes and chapters 4-6 of Thijssen.
  \end{block}
} 


\frame
{
  \frametitle{24 January -31 May, project 2}
  \begin{block}{Quantum Mechanical Methods and Systems}
\begin{enumerate}
\item The final part (part 2) of project 1  is however not yet determined.  
Depending on the interest of the participants we may extend project 1 
to parameterize a density functional from Monte Carlo calculations and compare with existing functionals. It may open up for the possibility of a writing a scientific article.
\end{enumerate}
  \end{block}
} 




\frame
{
  \frametitle{24 January -31 May}
  \begin{block}{Projects, deadlines and oral exam}
\begin{enumerate}
\item Deadline part 1: March  21
\item Deadline full project: 31 May
\item Oral exam: week 24 (8-12 June), most likely Friday June 10.
\end{enumerate}
The oral exam is based on your presentation of the project.
  \end{block}
} 


\frame
{
  \frametitle{24 January -31 May}
  \begin{block}{More on projects}
\begin{enumerate}
\item Keep  a logbook, important for keeping track of all your changes etc etc.
\item The projects should be written as a regular scientific article, with introduction, formalism, codes which have been developed and discussion of results.
Conclusions and references should also be included.  An example can be found on the webpage of the course.  
\item The link with the article example  contains also an article on how to use latex and write good scientific articles!
\end{enumerate}


  \end{block}
} 







\frame
{
  \frametitle{Lectures and ComputerLab}
  \begin{block}{}
\begin{itemize}
\item Lectures: Thursday (14.15-16, room FV329)
       \item Detailed lecture notes, all programs presented and projects
can be found at the homepage of the course.
       \item Computerlab: 16-19 thursday, room FV329
       \item Weekly plans and relevant information are on the official webpage.
\item Chapters 4, 11-18 of the FYS3150 lecture notes give a good 
starting point.   We recommend also J.~M.~Thijssen  text {\em Computational Physics} and the text of Raimes as background. 
For MPI we recommend Gropp, Lusk and Sjellum's text.
\end{itemize}
  \end{block}
}




\frame
{
  \frametitle{Thijssen's text}
\begin{columns}
\column{5.5cm}
%\begin{center}
\begin{pgfpicture}{-2.25cm}{0.5cm}{5cm}{0.5cm}
   {\pgfbox[center,center]{\pgfuseimage{thijssen}}}
\end{pgfpicture}
%\begin{figure}
%\includegraphics[scale=0.3]{he6}
%\end{figure}
%\end{center}
\column{4.5cm}
  \begin{block}{J.~M.~Thijssen's text}
\begin{itemize}
\item Computational  Physics
\item Chapters 3-6 and 12, possibly also chapter 8-9
\item see \url{http://www.tn.tudelft.nl/tn/People/Staff/Thijssen/comphybook.html}
\end{itemize}
  \end{block}
\end{columns}
}






\frame
{
  \frametitle{MPI text}
\begin{columns}
\column{5.5cm}
%\begin{center}
\begin{pgfpicture}{-2.25cm}{0.5cm}{5cm}{0.5cm}
   {\pgfbox[center,center]{\pgfuseimage{gropp}}}
\end{pgfpicture}
%\begin{figure}
%\includegraphics[scale=0.3]{he6}
%\end{figure}
%\end{center}
\column{4.5cm}
  \begin{block}{Gropp, Lusk and Sjellum}
\begin{itemize}
\item Using MPI
\item Chapters 1-5 
\item see \url{http://mitpress.mit.edu/catalog/item/default.asp?ttype=2&tid=10761}
\end{itemize}
  \end{block}
\end{columns}
}



\frame
{
  \frametitle<presentation>{Selected Texts and lectures on C/C++}
 \begin{small}
 {\scriptsize

  \beamertemplatebookbibitems

  \begin{thebibliography}{10}
   \bibitem{ref1} J.~J.~Barton and L.~R.~Nackman,{\em Scientific and Engineering C++}, Addison Wesley, 3rd edition 2000.
   \bibitem{ref2} B.~Stoustrup, {\em The C++ programming language}, Pearson, 1997. 
   \bibitem{ref3}George Em Karniadakis and Robert M. Kirby II, {\em Parallel Scientific Computing in C++ and MPI}  \url{http://www.cambridge.org/catalogue/catalogue.asp?isbn=9780521520805}
   \bibitem{ref4} D.~Yang, {\em C++ and Object-oriented Numeric Computing for
Scientists and Engineers}, Springer 2000.
\bibitem{ref5} More books reviewed at \url{http:://www.accu.org/} and 
\url{http://www.comeaucomputing.com/booklist/}
\end{thebibliography}
 }
 \end{small}
}


\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
The Schr\"odinger equation reads 
\begin{equation}
\hat{H}({\bf r}_1, {\bf r}_2, \hdots , {\bf r}_N) \Psi_{\lambda}({\bf r}_1, {\bf r}_2, \dots , {\bf r}_N) = 
E_\lambda  \Psi_\lambda({\bf r}_1, {\bf r}_2, \hdots , {\bf r}_N), 
\label{eq:basicSE1}
\end{equation}
where the vector ${\bf r}_i$ represents the coordinates (spatial and spin) of particle $i$, $\lambda$ stands  for all the quantum
numbers needed to classify a given $N$-particle state and $\Psi_{\lambda}$ is the pertaining eigenfunction.  Throughout this course,
$\Psi$ refers to the exact eigenfunction, unless otherwise stated.
}
\end{small}
}

\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
We write the Hamilton operator, or Hamiltonian,  in a generic way 
\[
	\hat{H} = \hat{T} + \hat{V} 
\]
where $\hat{T}$  represents the kinetic energy of the system
\[
	\hat{T} = \sum_{i=1}^N \frac{\mathbf{p}_i^2}{2m_i} = \sum_{i=1}^N \left( -\frac{\hbar^2}{2m_i} \mathbf{\nabla_i}^2 \right) =
		\sum_{i=1}^N t({\bf r}_i)
\]
while the operator $\hat{V}$ for the potential energy is given by
\begin{equation}
	\hat{V} = \sum_{i=1}^N u({\bf r}_i) + \sum_{ji=1}^N v({\bf r}_i,{\bf r}_j)+\sum_{ijk=1}^Nv({\bf r}_i,{\bf r}_j,{\bf r}_k)+\dots
\label{eq:firstv}
\end{equation}
Hereafter we use natural units, viz.~$\hbar=c=e=1$, with $e$ the elementary chargeand $c$ the speed of light. This means that momenta and masses
have dimension energy. 
}
\end{small}
}
\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
If one does quantum chemistry, after having introduced the  Born-Oppenheimer approximation which effectively freezes out the nucleonic degrees
of freedom, the Hamiltonian for $N=N_e$ electrons takes the following form 
\[
  \hat{H} = \sum_{i=1}^{N_e} t({\bf r}_i) 
  - \sum_{i=1}^{N_e} k\frac{Z}{r_i} + \sum_{i<j}^{N_e} \frac{k}{r_{ij}},
\]
with $k=1.44$ eVnm
}
\end{small}
}

\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
 We can rewrite this as
\begin{equation}
    \hat{H} = \hat{H_0} + \hat{H_1} 
    = \sum_{i=1}^{N_e}\hat{h_i} + \sum_{i<j=1}^{N_e}\frac{1}{r_{ij}},
\label{H1H2}
\end{equation}
where  we have defined $r_{ij}=| {\bf r}_i-{\bf r}_j|$ and
\begin{equation}
  \hat{h_i} =  t({\bf r}_i) - \frac{Z}{r_i}.
\label{hi}
\end{equation}
The first term of eq.~(\ref{H1H2}), $H_0$, is the sum of the $A$ or $n$
\emph{one-body} Hamiltonians $\hat{h_i}$. Each individual
Hamiltonian $\hat{h_i}$ contains the kinetic energy operator of an
electron and its potential energy due to the attraction of the
nucleus. The second term, $H_1$, is the sum of the $N_e(N_e-1)/2$
two-body interactions between each pair of electrons. Note that the double sum carries a restriction $i<j$.
}
\end{small}
}

\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
The potential energy term due to the attraction of the nucleus defines the onebody field $u_i=u({\bf r}_i)$ of Eq.~(\ref{eq:firstv}).
We have moved this term into the $\hat{H}_0$ part of the Hamiltonian, instead of keeping  it in $\hat{V}$ as in  Eq.~(\ref{eq:firstv}).
The reason is that we will hereafter treat $\hat{H}_0$ as our non-interacting  Hamiltonian. For a many-body wavefunction $\Phi_{\lambda}$ defined by an  
appropriate single-particle basis, we may solve exactly the non-interacting eigenvalue problem 
\[
\hat{H}_0\Phi_{\lambda}= e_{\lambda}\Phi_{\lambda},
\]
with $e_{\lambda}$ being the non-interacting energy. This energy is defined by the sum over single-particle energies to be defined below.
For atoms the single-particle energies could be the hydrogen-like single-particle energies corrected for the charge $Z$. For nuclei and quantum
dots, these energies could be given by the harmonic oscillator in three and two dimensions, respectively.
}
\end{small}
}

\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
We will assume that the interacting part of the Hamiltonian
can be approximated by a two-body interaction.
This means that our Hamiltonian is written as 
\begin{equation}
    \hat{H} = \hat{H_0} + \hat{H_1} 
    = \sum_{i=1}^N h_i + \sum_{i<j=1}^N V(r_{ij}),
\label{Hnuclei}
\end{equation}
with 
\begin{equation}
  H_0=\sum_{i=1}^N h_i =  \sum_{i=1}^N\left(t({\bf r}_i) + u({\bf r}_i)\right).
\label{hinuclei}
\end{equation}
The onebody part $u({\bf r}_i)$ is normally approximated by a harmonic oscillator potential or the Coulomb interaction an electron feels from the nucleus. However, other potentials are fully possible, such as 
those derived from the self-consistent solution of Hartree-Fock or Kohn-Sham 
equations.
}
\end{small}
}

\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
Our Hamiltonian is invariant under the permutation (interchange) of two particles. % (exercise here, prove it)
Since we deal with fermions however, the total wave function is antisymmetric.
Let $\hat{P}$ be an operator which interchanges two particles.
Due to the symmetries we have ascribed to our Hamiltonian, this operator commutes with the total Hamiltonian,
\[
[\hat{H},\hat{P}] = 0,
\]
meaning that $\Psi_{\lambda}({\bf r}_1, {\bf r}_2, \dots , {\bf r}_N)$ is an eigenfunction of 
$\hat{P}$ as well, that is
\[
\hat{P}_{ij}\Psi_{\lambda}({\bf r}_1, {\bf r}_2, \dots,{\bf r}_i,\dots,{\bf r}_j,\dots,{\bf r}_N)=
\Psi_{\lambda}({\bf r}_1, {\bf r}_2, \dots,{\bf r}_j,\dots,{\bf r}_i,\dots,{\bf r}_N).
\]
We have introduced the suffix $ij$ in order to indicate that we permute particles $i$ and $j$.
The Pauli principle tells us that the total wave function for a system of fermions
has to be antisymmetric. What does that mean for the above permutation?
}
\end{small}
}

\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
In our case we assume that  we can approximate the exact eigenfunction with a Slater determinant
\be
   \Phi({\bf r}_1, {\bf r}_2,\dots ,{\bf r}_N,\alpha,\beta,\dots, \sigma)=\frac{1}{\sqrt{N!}}
\left| \begin{array}{ccccc} \psi_{\alpha}({\bf r}_1)& \psi_{\alpha}({\bf r}_2)& \dots & \dots & \psi_{\alpha}({\bf r}_N)\\
                            \psi_{\beta}({\bf r}_1)&\psi_{\beta}({\bf r}_2)& \dots & \dots & \psi_{\beta}({\bf r}_N)\\  
                            \dots & \dots & \dots & \dots & \dots \\
                            \dots & \dots & \dots & \dots & \dots \\
                     \psi_{\sigma}({\bf r}_1)&\psi_{\sigma}({\bf r}_2)& \dots & \dots & \psi_{\gamma}({\bf r}_N)\end{array} \right|, 
\label{HartreeFockDet}
\ee 
where  ${\bf r}_i$  stand for the coordinates and spin values of a particle $i$ and $\alpha,\beta,\dots, \gamma$ 
are quantum numbers needed to describe remaining quantum numbers.  
}
\end{small}
}

\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
The single-particle function $\psi_{\alpha}({\bf r}_i)$  are eigenfunctions of the onebody
Hamiltonian $h_i$, that is
\[
h_i=h({\bf r}_i)=t({\bf r}_i) + u({\bf r}_i),
\]
with eigenvalues 
\[
 h_i\psi_{\alpha}({\bf r}_i)=t({\bf r}_i) + u({\bf r}_i)\psi_{\alpha}({\bf r}_i)=\varepsilon_{\alpha}\psi_{\alpha}({\bf r}_i).
\]
The energies $\varepsilon_{\alpha}$ are the so-called non-interacting single-particle energies, or unperturbed energies. 
The total energy is in this case the sum over all  single-particle energies, if no two-body or more complicated
many-body interactions are present.
}
\end{small}
}

\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
Let us denote the ground state energy by $E_0$. According to the
variational principle we have
\begin{equation*}
  E_0 \le E[\Phi] = \int \Phi^*\hat{H}\Phi d\mathbf{\tau}
\end{equation*}
where $\Phi$ is a trial function which we assume to be normalized
\begin{equation*}
  \int \Phi^*\Phi d\mathbf{\tau} = 1,
\end{equation*}
where we have used the shorthand $d\mathbf{\tau}=d\mathbf{r}_1d\mathbf{r}_2\dots d\mathbf{r}_N$.
}
\end{small}
}

\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
In the Hartree-Fock method (and very similarly the Kohn-Sham approach) the trial function is the Slater
determinant of Eq.~(\ref{HartreeFockDet}) which can be rewritten as 
\begin{equation}
  \Psi(\mathbf{r}_1,\mathbf{r}_2,\dots,\mathbf{r}_N,\alpha,\beta,\dots,\nu) = \frac{1}{\sqrt{N!}}\sum_{P} (-)^P\hat{P}\psi_{\alpha}(\mathbf{r}_1)
    \psi_{\beta}(\mathbf{r}_2)\dots\psi_{\nu}(\mathbf{r}_N)=\sqrt{N!}{\cal A}\Phi_H,
\label{HartreeFockPermutation}
\end{equation}
where we have introduced the antisymmetrization operator ${\cal A}$ defined by the 
summation over all possible permutations of two nucleons.
}
\end{small}
}

\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
It is defined as
\begin{equation}
  {\cal A} = \frac{1}{N!}\sum_{p} (-)^p\hat{P},
\label{antiSymmetryOperator}
\end{equation}
with $p$ standing for the number of permutations. We have introduced for later use the so-called
Hartree-function, defined by the simple product of all possible single-particle functions
\begin{equation*}
  \Phi_H(\mathbf{r}_1,\mathbf{r}_2,\dots,\mathbf{r}_N,\alpha,\beta,\dots,\nu) =
  \psi_{\alpha}(\mathbf{r}_1)
    \psi_{\beta}(\mathbf{r}_2)\dots\psi_{\nu}(\mathbf{r}_N).
\end{equation*}

}
\end{small}
}

\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
Both $\hat{H_0}$ and $\hat{H_1}$ are invariant under all possible permutations of any two particles
and hence commute with ${\cal A}$
\begin{equation}
  [H_0,{\cal A}] = [H_1,{\cal A}] = 0.
  \label{cummutionAntiSym}
\end{equation}
Furthermore, ${\cal A}$ satisfies
\begin{equation}
  {\cal A}^2 = {\cal A},
  \label{AntiSymSquared}
\end{equation}
since every permutation of the Slater
determinant reproduces it. 
}
\end{small}
}

\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
The expectation value of $\hat{H_0}$ 
\[
  \int \Phi^*\hat{H_0}\Phi d\mathbf{\tau} 
  = N! \int \Phi_H^*{\cal A}\hat{H_0}{\cal A}\Phi_H d\mathbf{\tau}
\]
is readily reduced to
\[
  \int \Phi^*\hat{H_0}\Phi d\mathbf{\tau} 
  = N! \int \Phi_H^*\hat{H_0}{\cal A}\Phi_H d\mathbf{\tau},
\]
where we have used eqs.~(\ref{cummutionAntiSym}) and
(\ref{AntiSymSquared}). The next step is to replace the antisymmetrization
operator by its definition Eq.~(\ref{HartreeFockPermutation}) and to
replace $\hat{H_0}$ with the sum of one-body operators
\[
  \int \Phi^*\hat{H_0}\Phi  d\mathbf{\tau}
  = \sum_{i=1}^N \sum_{p} (-)^p\int 
  \Phi_H^*\hat{h_i}\hat{P}\Phi_H d\mathbf{\tau}.
\]

}
\end{small}
}

\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
The integral vanishes if two or more particles are permuted in only one
of the Hartree-functions $\Phi_H$ because the individual single-particle wave functions are
orthogonal. We obtain then
\[
  \int \Phi^*\hat{H}_0\Phi  d\mathbf{\tau}= \sum_{i=1}^N \int \Phi_H^*\hat{h}_i\Phi_H  d\mathbf{\tau}.
\]
Orthogonality of the single-particle functions allows us to further simplify the integral, and we
arrive at the following expression for the expectation values of the
sum of one-body Hamiltonians 
\begin{equation}
  \int \Phi^*\hat{H_0}\Phi  d\mathbf{\tau}
  = \sum_{\mu=1}^N \int \psi_{\mu}^*(\mathbf{r})\hat{h}\psi_{\mu}(\mathbf{r})
  d\mathbf{r}.
  \label{H1Expectation}
\end{equation}

}
\end{small}
}

\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
We introduce the following shorthand for the above integral
\[
\langle \mu | h | \mu \rangle = \int \psi_{\mu}^*(\mathbf{r})\hat{h}\psi_{\mu}(\mathbf{r})d\mathbf{r},
\]
and rewrite Eq.~(\ref{H1Expectation}) as
\begin{equation}
  \int \Phi^*\hat{H_0}\Phi  d\mathbf{\tau}
  = \sum_{\mu=1}^N \langle \mu | h | \mu \rangle.
  \label{H1Expectation1}
\end{equation}

}
\end{small}
}
\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
The expectation value of the two-body Hamiltonian is obtained in a
similar manner. We have
\begin{equation*}
  \int \Phi^*\hat{H_1}\Phi d\mathbf{\tau} 
  = N! \int \Phi_H^*{\cal A}\hat{H_1}{\cal A}\Phi_H d\mathbf{\tau},
\end{equation*}
which reduces to
\begin{equation*}
 \int \Phi^*\hat{H_1}\Phi d\mathbf{\tau} 
  = \sum_{i\le j=1}^N \sum_{p} (-)^p\int 
  \Phi_H^*V(r_{ij})\hat{P}\Phi_H d\mathbf{\tau},
\end{equation*}
by following the same arguments as for the one-body
Hamiltonian. 
}
\end{small}
}
\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
Because of the dependence on the inter-particle distance $r_{ij}$,  permutations of
any two particles no longer vanish, and we get
\begin{equation*}
  \int \Phi^*\hat{H_1}\Phi d\mathbf{\tau} 
  = \sum_{i < j=1}^N \int  
  \Phi_H^*V(r_{ij})(1-P_{ij})\Phi_H d\mathbf{\tau}.
\end{equation*}
where $P_{ij}$ is the permutation operator that interchanges
nucleon $i$ and nucleon $j$. Again we use the assumption that the single-particle wave functions
are orthogonal. 
}
\end{small}
}
\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
We obtain
\begin{equation}
\begin{split}
  \int \Phi^*\hat{H_1}\Phi d\mathbf{\tau} 
  = \frac{1}{2}\sum_{\mu=1}^N\sum_{\nu=1}^N
    &\left[ \int \psi_{\mu}^*(\mathbf{r}_i)\psi_{\nu}^*(\mathbf{r}_j)V(r_{ij})\psi_{\mu}(\mathbf{r}_i)\psi_{\nu}(\mathbf{r}_j)
    d\mathbf{r}_id\mathbf{r}_j \right.\\
  &\left.
  - \int \psi_{\mu}^*(\mathbf{r}_i)\psi_{\nu}^*(\mathbf{r}_j)
  V(r_{ij})\psi_{\nu}(\mathbf{r}_i)\psi_{\mu}(\mathbf{r}_j)
  d\mathbf{r}_id\mathbf{r}_j
  \right]. \label{H2Expectation}
\end{split}
\end{equation}
The first term is the so-called direct term. It is frequently also called the  Hartree term, 
while the second is due to the Pauli principle and is called
the exchange term or just the Fock term.
The factor  $1/2$ is introduced because we now run over
all pairs twice. 
}
\end{small}
}
\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
The last equation allows us to  introduce some further definitions.  
The single-particle wave functions $\psi_{\mu}({\bf r})$, defined by the quantum numbers $\mu$ and ${\bf r}$
(recall that ${\bf r}$ also includes spin degree)   are defined as the overlap 
\[
   \psi_{\alpha}({\bf r})  = \langle {\bf r} | \alpha \rangle .
\]

}
\end{small}
}
\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
We introduce the following shorthands for the above two integrals
\[
\langle \mu\nu|V|\mu\nu\rangle =  \int \psi_{\mu}^*(\mathbf{r}_i)\psi_{\nu}^*(\mathbf{r}_j)V(r_{ij})\psi_{\mu}(\mathbf{r}_i)\psi_{\nu}(\mathbf{r}_j)
    d\mathbf{r}_id\mathbf{r}_j,
\]
and 
\[
\langle \mu\nu|V|\nu\mu\rangle = \int \psi_{\mu}^*(\mathbf{r}_i)\psi_{\nu}^*(\mathbf{r}_j)
  V(r_{ij})\psi_{\nu}(\mathbf{r}_j)\psi_{\mu}(\mathbf{r}_i)
  d\mathbf{r}_id\mathbf{r}_j.  
\]
}
\end{small}
}
\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
The direct and exchange matrix elements can be  brought together if we define the antisymmetrized matrix element
\[
\langle \mu\nu|V|\mu\nu\rangle_{AS}= \langle \mu\nu|V|\mu\nu\rangle-\langle \mu\nu|V|\nu\mu\rangle,
\]
or for a general matrix element  
\[
\langle \mu\nu|V|\sigma\tau\rangle_{AS}= \langle \mu\nu|V|\sigma\tau\rangle-\langle \mu\nu|V|\tau\sigma\rangle.
\]
It has the symmetry property
\[
\langle \mu\nu|V|\sigma\tau\rangle_{AS}= -\langle \mu\nu|V|\tau\sigma\rangle_{AS}=-\langle \nu\mu|V|\sigma\tau\rangle_{AS}.
\]
}
\end{small}
}
\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
The antisymmetric matrix element is also hermitian, implying 
\[
\langle \mu\nu|V|\sigma\tau\rangle_{AS}= \langle \sigma\tau|V|\mu\nu\rangle_{AS}.
\]

With these notations we rewrite Eq.~(\ref{H2Expectation}) as 
\begin{equation}
  \int \Phi^*\hat{H_1}\Phi d\mathbf{\tau} 
  = \frac{1}{2}\sum_{\mu=1}^N\sum_{\nu=1}^N \langle \mu\nu|V|\mu\nu\rangle_{AS}.
\label{H2Expectation2}
\end{equation}

}
\end{small}
}
\frame
{
  \frametitle{Definitions and notations}
\begin{small}
{\scriptsize
Combining Eqs.~(\ref{H1Expectation1}) and
(\ref{H2Expectation2}) we obtain the energy functional 
\begin{equation}
  E[\Phi] 
  = \sum_{\mu=1}^N \langle \mu | h | \mu \rangle +
  \frac{1}{2}\sum_{{\mu}=1}^N\sum_{{\nu}=1}^N \langle \mu\nu|V|\mu\nu\rangle_{AS}.
\label{FunctionalEPhi}
\end{equation}
which we will use as our starting point for the Hartree-Fock calculations 
and density functional calculations (DFT) based on solving the Kohn-Sham equations later in this course. 
}
\end{small}
}


\frame
{ 
  \frametitle{Quantum dots, the case of our project}
  \begin{small}
    {\scriptsize
We consider a system of electrons confined in a pure isotropic harmonic oscillator potential $V(\vec{r})=m^* \omega_0^2 r^2/2$, where $m^*$ is the effective mass of the electrons in the host semiconductor, $\omega_0$ is the oscillator frequency of the confining potential, and $\vec{r}=(x,y,z)$ denotes the position of the particle.

The Hamiltonian of a single particle trapped in this harmonic oscillator potential simply reads
\[
  \hat{H}= \frac{\textbf{p}^2}{2m^*}  + \frac{1}{2} m^* \omega_0^2 {\textbf{r}}^2
\]
where $\textbf{p}$ is the canonical momentum of the particle.
    }
  \end{small}
}


\frame
{ 
  \frametitle{Quantum dots}
  \begin{small}
    {\scriptsize
When considering several particles trapped in the same quantum dot, the Coulomb repulsion between those electrons has to be added to the single particle Hamiltonian which gives
\[
\hat{H}=\sum_{i=1}^{N_e} \left( \frac{\mathbf{p_i}^2}{2m^*}+ \frac{1}{2} m^* \omega_0^2 {\mathbf{r_i}}^2 \right) + \frac{e^2}{4 \pi \epsilon_0 \epsilon_r} \sum_{i<j} \frac{1}{{\mathbf{r_i}-\mathbf{r_j}}},
\]
where $N_e$ is the number of electrons, $-e \;  (e>0)$ is the charge of the electron, $\epsilon_0$ and $\epsilon_r$ are respectively the free space permitivity and the relative permitivity of the host material (also called dielectric constant), and the index $i$ labels the electrons.

    }
  \end{small}
}


\frame
{ 
  \frametitle{Quantum dots}
  \begin{small}
    {\scriptsize
We assume that the magnetic field $\overrightarrow{B}$ is static and along the $z$ axis.
At first we ignore the spin-dependent terms. The Hamiltonian of these electrons in a magnetic field now reads
\begin{align}
  \hat{H}&=&\sum_{i=1}^{N_e}\left(  \frac{(\mathbf{p_i}+e\mathbf{A})^2}{2m^*}  + \frac{1}{2} m^* \omega_0^2 {\mathbf{r_i}}^2  \right) + \frac{e^2}{4 \pi \epsilon_0 \epsilon_r} \sum_{i<j}\frac{1}{{\mathbf{r_i}-\mathbf{r_j}}}, \\
&=&\sum_{i=1}^{N_e}\left(  \frac{\mathbf{p_i}^2}{2m^*} + \frac{e}{2m^*}(\mathbf{A}\cdot \mathbf{p_i}+\mathbf{p_i}\cdot \mathbf{A}) + \frac{e^2}{2m^*}\mathbf{A}^2  + \frac{1}{2} m^* \omega_0^2 {\mathbf{r_i}}^2  \right) \\
& &+ \frac{e^2}{4 \pi \epsilon_0 \epsilon_r} \sum_{i<j}\frac{1}{{\mathbf{r_i}-\mathbf{r_j}}},
\end{align}
where $\mathbf{A}$ is the vector potential defined by $\mathbf{B}=\nabla \times \mathbf{A}$.

    }
  \end{small}
}


\frame
{ 
  \frametitle{Quantum dots}
  \begin{small}
    {\scriptsize
In coordinate space, $\mathbf{p_i}$ is the operator $-i \hbar \nabla_i$ and by applying the Hamiltonian on the total wave function $\Psi(\mathbf{r})$ in the Schr\"odinger equation, we obtain the following operator acting on $\Psi(\mathbf{r})$
\begin{align}
\mathbf{A}\cdot \mathbf{p_i}+\mathbf{p_i}\cdot \mathbf{A} &= - i \hbar \left( \mathbf{A}\cdot \nabla_i+\nabla_i\cdot \mathbf{A} \right) \Psi \\
&=  - i \hbar \left( \mathbf{A}\cdot  ( \nabla_i \Psi)+\nabla_i\cdot (\mathbf{A} \Psi) \right) 
\end{align}

    }
  \end{small}
}


\frame
{ 
  \frametitle{Quantum dots}
  \begin{small}
    {\scriptsize
We note that if we use the product rule and the Coulomb gauge $\nabla \cdot \mathbf{A} = 0$ (by choosing the vector potential as $\mathbf{A} = \frac{1}{2} \mathbf{B} \times \mathbf{r}$), $\mathbf{p_i}$ and $\nabla_i$ commute and we obtain
\[
 \nabla_i \cdot (\mathbf{A}\Psi) = \mathbf{A} \cdot (\nabla_i\Psi) + (\underbrace{\nabla_i \cdot \mathbf{A})}_0 \Psi=\mathbf{A} \cdot (\nabla_i\Psi) 
\]
    }
  \end{small}
}


\frame
{ 
  \frametitle{Quantum dots}
  \begin{small}
    {\scriptsize
This leads us to the following Hamiltonian:
\[
  \hat{H}=\sum_{i=1}^{N_e}\left(  -\frac{ \hbar^2}{2m^*} \nabla_i^2- i \hbar \frac{e}{m^*} \mathbf{A}\cdot \nabla_i + \frac{e^2}{2m^*}\mathbf{A}^2  + \frac{1}{2} m^* \omega_0^2 {\mathbf{r_i}}^2  \right)
+ \frac{e^2}{4 \pi \epsilon_0 \epsilon_r} \sum_{i<j}\frac{1}{{\mathbf{r_i}-\mathbf{r_j}}},
\]
    }
  \end{small}
}


\frame
{ 
  \frametitle{Quantum dots}
  \begin{small}
    {\scriptsize
The linear term in $\mathbf{A}$ becomes, in terms of $\mathbf{B}$:
\begin{align}
\label{eq:linearTermA}
\frac{-i \hbar e}{m^*} \mathbf{A} \cdot \nabla_i &= -\frac{i \hbar e}{2m^*} (\mathbf{B} \times \mathbf{r_i}) \cdot \nabla_i \\
&= \frac{-i \hbar e}{2m^*} \mathbf{B} \cdot( \mathbf{r_i} \times \nabla_i)  \\
&= \frac{ e}{2m^*} \mathbf{B} \cdot \mathbf{L} 
\end{align}
where $\mathbf{L}=-i \hbar (\mathbf{r_i} \times \nabla_i)$ is the orbital angular momentum operator of the electron $i$.
    }
  \end{small}
}


\frame
{ 
  \frametitle{Quantum dots}
  \begin{small}
    {\scriptsize
If we assume that the electrons are confined in the $xy$-plane, the quadratic term in $\mathbf{A}$ can be written as
\[
\frac{e^2}{2m^*} \mathbf{A}^2 = \frac{e^2}{8m^*} (\mathbf{B} \times \mathbf{r})^2
= \frac{e^2}{8m^*} B^2 r_i^2
\]

    }
  \end{small}
}


\frame
{ 
  \frametitle{Quantum dots}
  \begin{small}
    {\scriptsize
Until this point we have neglected the intrinsic magnetic moment of the electrons which is due to the electron spin in the host material. We will now add its effect to the Hamiltonian. This intrinsic magnetic moment is given by $\mathcal{M}_s=-g^*_s (e \mathbf{S})/(2m^*)$, where $\mathbf{S}$ is the spin operator of the electron and $g^*_s$ its effective spin gyromagnetic ratio (or effective \textit{g-factor} in the host material).% Dirac's relativistic theory predicts for $g_s$, the value $g_s=2$ which is in very good agreement with experiment~\cite{Bransden2003}.
We see that the spin magnetic moment $\mathcal{M}_s$ gives rise to an additional interaction energy linear in the magnetic field,
\[
  \hat{H_s}= - \mathcal{M}_s \cdot \mathbf{B} = g^*_s \frac{e }{2 m^*} B \hat{S_z}= g^*_s \frac{\omega_c}{2} \hat{S_z}
\]
where $\omega_c=e B/m^*$ is known as the cyclotron frequency.

    }
  \end{small}
}


\frame
{ 
  \frametitle{Quantum dots}
  \begin{small}
    {\scriptsize
The final Hamiltonian reads
\begin{align}
  \hat{H}&=\sum_{i=1}^{N_e} \bigg(  \frac{- \hbar^2}{2m^*} \nabla_i^2 + \overbrace{\frac{1}{2} m^* \omega_0^2 {\mathbf{r_i}}^2}^{\begin{smallmatrix}
  \text{Harmonic ocscillator} \\
  \text{potential}
\end{smallmatrix}} \bigg) + \overbrace{\frac{e^2}{4 \pi \epsilon_0 \epsilon_r} \sum_{i<j}\frac{1}{\abs{\mathbf{r_i}-\mathbf{r_j}}}}^{\begin{smallmatrix}
  \text{Coulomb} \\
  \text{interactions}
\end{smallmatrix}}  \nonumber \\
&+  \underbrace{\sum_{i=1}^{N_e} \left( \frac{1}{2} m^* \left( \frac{\omega_c}{2} \right)^2 {\mathbf{r_i}}^2 + \frac{1}{2}  \omega_c \hat{L}_z^{(i)}+ \frac{1  }{2} g_s^*  \omega_c \hat{S}_z^{(i)}\right)}_{\begin{smallmatrix}
  \text{single particle interactions} \\
  \text{with the magnetic field}
\end{smallmatrix}},
\end{align}
    }
  \end{small}
}


\frame
{ 
  \frametitle{Quantum dots}
  \begin{small}
    {\scriptsize
In order to simplify the computation, the Hamiltonian can be rewritten on dimensionless form.
For this purpose, we introduce the following constants:
\begin{itemize}
 \item the oscillator frequency $\omega = \omega_0\sqrt{1+\omega_c^2/ (4\omega_0^2)}$,
 \item a new energy unit $\hbar \omega$,
\item 	a new length unit, the oscillator length defined by $l=\sqrt{\hbar /(m^* \omega)}$, also called the characteristic length unit.
\end{itemize}

We rewrite the Hamiltonian in dimensionless units using:\\
$$\mathbf{r} \longrightarrow \frac{\mathbf{r}}{l}, \quad \nabla \longrightarrow l \;\nabla \quad \text{and} \quad \hat{L}_z \longrightarrow \hat{L}_z$$ 

    }
  \end{small}
}


\frame
{ 
  \frametitle{Quantum dots}
  \begin{small}
    {\scriptsize
It leads to the following Hamiltonian:
\begin{align}
\hat{H}&=\sum_{i=1}^{N_e} \left(  -\frac{1}{2} \nabla_i^2 + \frac{1}{2} r_i^2 \right)  + \overbrace{\frac{e^2}{4 \pi \epsilon_0 \epsilon_r} \frac{1}{\hbar \omega l}}^{\begin{smallmatrix}
  \text{Dimensionless} \\
 \text{confinement } \\
  \text{strength ($\lambda$)}
\end{smallmatrix}}
\sum_{i<j}\frac{1}{r_{ij}}  \nonumber \\
&+  \sum_{i=1}^{N_e} \left(  \frac{1}{2}  \frac{\omega_c}{\hbar \omega} \hat{L}_z^{(i)}+ \frac{1  }{2} g_s^* \frac{\omega_c}{\hbar \omega} \hat{S}_z^{(i)}\right),
\end{align}
Lengths are now measured in units of $l=\sqrt{\hbar/(m^*\omega)}$, and energies in units of $\hbar \omega$.

    }
  \end{small}
}
\frame
{ 
  \frametitle{Quantum dots}
  \begin{small}
    {\scriptsize
A new dimensionless parameter $\lambda=l / a_0^*$ (where $a_0^*= 4 \pi \epsilon_0 \epsilon_r \hbar^2 / (e^2 m^*)$ is the effective Bohr radius) describes the strength of the electron-electron interaction.
Large $\lambda$ implies strong interaction and/or large quantum dot.

Since both $\hat{L_z}$ and $\hat{S_z}$ commute with the Hamiltonian we can perform the calculations separately in subspaces of given quantum numbers $L_z$ and $S_z$.
    }
  \end{small}
}
\frame
{ 
  \frametitle{Quantum dots}
  \begin{small}
    {\scriptsize
The simplified dimensionless Hamiltonian becomes
\[
  \hat{H}=\sum_{i=1}^{N_e} \left[  -\frac{1}{2} \nabla_i^2 + \frac{1}{2} r_i^2  \right]+ \lambda \sum_{i<j}\frac{1}{r_{ij}} +  \sum_{i=1}^{N_e} \left(  \frac{1}{2}  \frac{\omega_c}{\hbar \omega} L_z^{(i)}+ \frac{1  }{2} g_s^* \frac{\omega_c}{\hbar \omega} S_z^{(i)}\right),
\]
    }
  \end{small}
}
\frame
{ 
  \frametitle{Quantum dots}
  \begin{small}
    {\scriptsize
The last sum which is proportional to the magnetic field involves only the quantum numbers $L_z$ and $S_z$ and not the operators themselves. Therefore these terms can be put aside during the resolution, the squizzing effect of the magnetic field being included simply in the parameter $\lambda$. The contribution of these terms will be added when the other part has been solved. This brings us to the simple and general form of the Hamiltonian:
\[
\hat{H}=\sum_{i=1}^{N_e} \left(  -\frac{1}{2} \nabla_i^2 + \frac{1}{2} r_i^2  \right)+ \lambda \sum_{i<j}\frac{1}{r_{ij}}.
\]
    }
  \end{small}
}


\frame
{ 
  \frametitle{Quantum dots}
  \begin{small}
    {\scriptsize
The form
\[
\hat{H}=\sum_{i=1}^{N_e} \left(  -\frac{1}{2} \nabla_i^2 + \frac{1}{2} r_i^2  \right)+ \lambda \sum_{i<j}\frac{1}{r_{ij}},
\]
is however not so practical since the interaction carries a strength $\lambda$. Why?
    }
  \end{small}
}



\frame
{ 
  \frametitle{Quantum dots}
  \begin{small}
    {\scriptsize
We rewrite it as a one-body part 
\[
\hat{H}_0=\sum_{i=1}^{N_e}\left(-{\frac{1}{2}}\nabla^2_{i}+\frac{ \omega^2}{2}r^2_{i} \right),
\]
and interacting part
\[
\hat{V}=\sum_{i<j}^{N_e}\frac{1}{|{\bf r}_i-{\bf r}_j|}.
\]
Your task till next week is to show this.
The unperturbed part of the Hamiltonian yields the  single-particle energies
\begin{equation}
\epsilon_i = \omega\left(2n+|m| + 1\right),
\end{equation}
where $n = 0,1,2,3,..$ and $m = 0, \pm 1, \pm 2,..$. The index $i$ runs from $0,1,2,\dots$.
    }
  \end{small}
}


\frame
{ 
  \frametitle{Tasks for next week}
  \begin{small}
    {\scriptsize
\begin{itemize}
\item Solve the exercise on the previous slide
\item Set up the harmonic oscillator wave function in cartesian coordinates 
for an electron with $n_x=n_y=0$ and find the oscillator energy.
\item Use this result to find the unperturbed energy 
\[
  \int \Phi^*\hat{H_0}\Phi  d\mathbf{\tau}
  = \sum_{\mu=1}^N \langle \mu | h | \mu \rangle.
\]
for two electrons with the same quantum numbers. Is that possible?
\item Repeat for six electrons (find the relevant harmonic oscillator quantum numbers)
\item Read chapter 12 of Thijssen or alternatively chapter 14 of lecture notes (Variational Monte Carlo)
\item Read chapter 5 of Lars Eivind Lerv\aa g's thesis, it deals with quantum dots and gives a good introduction to the physics of quantum dots. 
\end{itemize}
    }
  \end{small}
}


\section[Week 5]{Weeks 5}
\frame
{
  \frametitle{Topics for Week 5, January 31-February 4}
  \begin{block}{Quantum Monte Carlo and start of project 1}
\begin{itemize}
\item Repetition from the last two weeks
\item Quantum Monte Carlo
\item How to compute the local energy, numerically versus closed form expressions
%\item Overview of Parallelization with MPI
\end{itemize}
Project work this week: Start  1a and 1b. 
  \end{block}
} 



\frame
{
  \frametitle{Quantum Monte Carlo Motivation}
\begin{small}
{\scriptsize
Most quantum mechanical  
problems of interest in e.g., atomic, molecular, nuclear and solid state 
physics consist of a large number of 
interacting electrons and ions or nucleons. 
The total number of particles $N$ is usually sufficiently large
that an exact solution cannot be found. 
Typically, 
the expectation value for a chosen hamiltonian for a system of 
$N$ particles is
\[
   \langle H \rangle =
\]
\[
   \frac{\int d{\bf R}_1d{\bf R}_2\dots d{\bf R}_N
         \Psi^{\ast}({\bf R_1},{\bf R}_2,\dots,{\bf R}_N)
          H({\bf R_1},{\bf R}_2,\dots,{\bf R}_N)
          \Psi({\bf R_1},{\bf R}_2,\dots,{\bf R}_N)}
        {\int d{\bf R}_1d{\bf R}_2\dots d{\bf R}_N
        \Psi^{\ast}({\bf R_1},{\bf R}_2,\dots,{\bf R}_N)
        \Psi({\bf R_1},{\bf R}_2,\dots,{\bf R}_N)},
\]
an in general intractable problem.
an in general intractable problem.

 This integral is actually the starting point in a Variational Monte Carlo calculation.\newline
 {\bf Gaussian quadrature: Forget it!} given 10 particles and 10 mesh points for each degree of freedom
and an
 ideal 1 Tflops machine (all operations take the same time), how long will it ta
ke to compute the above integral? Lifetime of the universe $T\approx 4.7 \times
10^{17}$s.
}
\end{small}
}


\frame
{
  \frametitle{Quantum Monte Carlo}
\begin{small}
{\scriptsize
As an example from the nuclear many-body problem, we have Schr\"odinger's
equation as 
a differential equation
\[
  \hat{H}\Psi({\bf r}_1,..,{\bf r}_A,\alpha_1,..,\alpha_A)=E\Psi({\bf r}_1,..,{\bf r}_A,\alpha_1,..,\alpha_A)
\]
where
\[
  {\bf r}_1,..,{\bf r}_A,
\]
are the coordinates and 
\[
  \alpha_1,..,\alpha_A,
\]
are sets of relevant quantum numbers such as spin and isospin for a system of 
$A$ nucleons ($A=N+Z$, $N$ being the number of neutrons and $Z$ the number of protons).
}
\end{small}
}


\frame
{
  \frametitle{Quantum Monte Carlo}
\begin{small}
{\scriptsize
There are
\[
 2^A\times \left(\begin{array}{c} A\\ Z\end{array}\right)
\]
coupled second-order differential equations in $3A$ dimensions.

For a nucleus like $^{10}$Be this number is
{\bf 215040}.
This is a truely challenging many-body problem.

Methods like partial differential equations can at most be used for 2-3 particles.
}
\end{small}
}

\frame
{
  \frametitle{Quantum Many-particle(body) Methods}
\begin{small}
{\scriptsize
\begin{enumerate}
\item Monte-Carlo methods
\item Renormalization group (RG) methods, in particular density matrix RG
\item Large-scale diagonalization (Iterative methods, Lanczo's method, dimensionalities 
$10^{10}$ states)
\item Coupled cluster theory, favoured method in quantum chemistry, 
molecular and atomic physics. Applications to ab initio calculations in 
nuclear physics as well for large nuclei.
\item Perturbative many-body methods 
\item Green's function methods
\item Density functional theory/Mean-field theory and Hartree-Fock theory
\end{enumerate}
The physics of the system hints at which many-body methods to use. For systems with strong correlations
among the constituents, item 5 and 7 are ruled out.
}
\end{small}
}


\frame
{
  \frametitle{Pros and Cons of Monte Carlo}
\begin{small}
{\scriptsize
\begin{itemize} 
\item Is physically intuitive.
\item Allows one to study systems with many degrees of freedom. Diffusion Monte Carlo (DMC) and Green's function Monte Carlo (GFMC) yield
in principle the exact solution to Schr\"odinger's equation.
\item Variational Monte Carlo (VMC) is easy  to implement but needs 
a reliable trial wave function, can be difficult to obtain.  This is where we will use Hartree-Fock theory to construct
an optimal basis.
\item DMC/GFMC for fermions (spin with half-integer values, electrons, baryons, neutrinos, quarks) 
has a sign problem. Nature prefers an anti-symmetric wave function. PDF in this case given
distribution of random walkers ($p\ge 0$).
\item The solution has a statistical error, which can be large. 
\item There is a limit for how large systems one can study, DMC needs a huge number of random walkers
in order to achieve stable results. 
\item Obtain only the lowest-lying states with a given symmetry. Can get excited states.
\end{itemize}
}
\end{small}
}




\frame
{
  \frametitle{Where and why do we use Monte Carlo Methods in Quantum Physics}
\begin{small}
{\scriptsize
\begin{itemize} 
\item Quantum systems with many particles at finite temperature: Path Integral 
Monte Carlo with applications to dense matter and quantum liquids (phase transitions from
normal fluid to superfluid). Strong correlations.
\item Bose-Einstein condensation of dilute gases, method transition from 
non-linear PDE to Diffusion Monte Carlo as density increases.
\item Light atoms, molecules, solids and nuclei. 
\item Lattice Quantum-Chromo Dynamics. Impossible to solve without MC calculations. 
\item Simulations of systems in solid state physics, from semiconductors to 
spin systems. Many electrons active and possibly strong correlations.
\end{itemize}
}
\end{small}
}


\frame
 {
  \frametitle{Bose-Einstein Condensation of atoms, thousands of Atoms in one State, Project 2 in 2007}
 \begin{center}
\includegraphics[scale=0.3]{BEC_three_peaks}
 \end{center}
 }



\frame
{
  \frametitle{Quantum Monte Carlo}
\begin{small}
{\scriptsize
Given a hamiltonian $H$ and a trial
wave function $\Psi_T$, the variational principle states that
the expectation value of $\langle H \rangle$, defined through 
\[
   E[H]= \langle H \rangle =
   \frac{\int d{\bf R}\Psi^{\ast}_T({\bf R})H({\bf R})\Psi_T({\bf R})}
        {\int d{\bf R}\Psi^{\ast}_T({\bf R})\Psi_T({\bf R})},
\]
is an upper bound to the ground state energy $E_0$ of the hamiltonian $H$, that
is 
\[
    E_0 \le \langle H \rangle .
\]
In general, the integrals involved in the calculation of various  expectation
values  are multi-dimensional ones. Traditional integration methods
such as the Gauss-Legendre will not be adequate for say the 
computation of the energy of a many-body system.
}
\end{small}
}


\frame
{
  \frametitle{Quantum Monte Carlo}
\begin{small}
{\scriptsize
The trial wave function can be expanded
in the eigenstates of the hamiltonian since they form a complete set, viz.,
\[
   \Psi_T({\bf R})=\sum_i a_i\Psi_i({\bf R}),
\]
and assuming the set of eigenfunctions to be normalized one obtains 
\[
     \frac{\sum_{nm}a^*_ma_n \int d{\bf R}\Psi^{\ast}_m({\bf R})H({\bf R})\Psi_n({\bf R})}
        {\sum_{nm}a^*_ma_n \int d{\bf R}\Psi^{\ast}_m({\bf R})\Psi_n({\bf R})} =\frac{\sum_{n}a^2_n E_n}
        {\sum_{n}a^2_n} \ge E_0,
\]
where we used that $H({\bf R})\Psi_n({\bf R})=E_n\Psi_n({\bf R})$.
In general, the integrals involved in the calculation of various  expectation
values  are multi-dimensional ones. 
The variational principle yields the lowest state of a given symmetry.
}
\end{small}
}


\frame
{
  \frametitle{Quantum Monte Carlo}
\begin{small}
{\scriptsize
In most cases, a wave function has only small values in large parts of 
configuration space, and a straightforward procedure which uses
homogenously distributed random points in configuration space 
will most likely lead to poor results. This may suggest that some kind
of importance sampling combined with e.g., the Metropolis algorithm 
may be  a more efficient way of obtaining the ground state energy.
The hope is then that those regions of configurations space where
the wave function assumes appreciable values are sampled more 
efficiently. 

The tedious part in a VMC calculation is the search for the variational
minimum. A good knowledge of the system is required in order to carry out
reasonable VMC calculations. This is not always the case, 
and often VMC calculations 
serve rather as the starting
point for so-called diffusion Monte Carlo calculations (DMC). DMC is a way of
solving exactly the many-body Schr\"odinger equation by means of 
a stochastic procedure. A good guess on the binding energy
and its wave function is however necessary. 
A carefully performed VMC calculation can aid in this context. 
}
\end{small}
}


\frame
{
  \frametitle{Quantum Monte Carlo}
\begin{small}
{\scriptsize
\begin{itemize}
\item Construct first a trial wave function $\psi_T^{\alpha}({\bf R})$, 
for a many-body
system consisting of $N$ particles located at positions
${\bf R=(R_1,\dots ,R_N)}$. The trial wave function depends
on $\alpha$ variational parameters 
${\bf \alpha}=(\alpha_1,\dots ,\alpha_N)$.
\item Then we evaluate the expectation value of the hamiltonian $H$ 
\[
   E[H]=\langle H \rangle =
   \frac{\int d{\bf R}\Psi^{\ast}_{T_{\alpha}}({\bf R})H({\bf R})
         \Psi_{T_{\alpha}}({\bf R})}
        {\int d{\bf R}\Psi^{\ast}_{T_{\alpha}}({\bf R})\Psi_{T_{\alpha}}({\bf R})}.
\]
\item Thereafter we vary $\alpha$ according to some minimization
algorithm and return to the first step.
\end{itemize}
}
\end{small}
}


\frame
{
  \frametitle{Quantum Monte Carlo}
\begin{small}
{\scriptsize
Choose a trial wave function
$\psi_T({\bf R})$.
\[
   P({\bf R})= \frac{\left|\psi_T({\bf R})\right|^2}{\int \left|\psi_T({\bf R})\right|^2d{\bf R}}.
\]
This is our new probability distribution function  (PDF).
The approximation to the expectation value of the Hamiltonian is now 
\[
   E[H]\approx 
   \frac{\int d{\bf R}\Psi^{\ast}_T({\bf R})H({\bf R})\Psi_T({\bf R})}
        {\int d{\bf R}\Psi^{\ast}_T({\bf R})\Psi_T({\bf R})}.
\]
Define a new quantity
\[
   E_L({\bf R})=\frac{1}{\psi_T({\bf R})}H\psi_T({\bf R}),
   \label{eq:locale1}
\]
called the local energy, which, together with our trial PDF yields
\[
  E[H]=\langle H \rangle \approx \int P({\bf R})E_L({\bf R}) d{\bf R}\approx \frac{1}{N}\sum_{i=1}^NP({\bf R_i})E_L({\bf R_i})
  \label{eq:vmc1}
\]
with $N$ being the number of Monte Carlo samples.
}
\end{small}
}


\frame
{
  \frametitle{Quantum Monte Carlo}
\begin{small}
{\scriptsize
Algo:
       \begin{itemize}
          \item Initialisation: Fix the number of Monte Carlo steps. Choose an initial ${\bf R}$ and
                variational parameters $\alpha$ and 
                calculate
                $\left|\psi_T^{\alpha}({\bf R})\right|^2$. 
          \item Initialise the energy and the variance and start the Monte Carlo calculation (thermalize)
                \begin{enumerate}
                  \item Calculate  a trial position  ${\bf R}_p={\bf R}+r*step$
                        where $r$ is a random variable $r \in [0,1]$.
                  \item Metropolis algorithm to accept
                        or reject this move                         \[
                           w = P({\bf R}_p)/P({\bf R}).
                        \]
                  \item If the step is accepted, then we set 
                        ${\bf R}={\bf R}_p$. Update averages
                 \end{enumerate}
          \item Finish and
compute final averages.
      \end{itemize}
Observe that the jumping in space is governed by the variable $step$. Called brute-force sampling.
Need importance sampling to get more relevant sampling.
}
\end{small}
}

\frame
{
  \frametitle{Quantum Monte Carlo}
\begin{small}
{\scriptsize
The radial Schr\"odinger equation for the hydrogen atom can be
written as
\[
-\frac{\hbar^2}{2m}\frac{\partial^2 u(r)}{\partial r^2}-
\left(\frac{ke^2}{r}-\frac{\hbar^2l(l+1)}{2mr^2}\right)u(r)=Eu(r),
\]
or with dimensionless variables
\[
-\frac{1}{2}\frac{\partial^2 u(\rho)}{\partial \rho^2}-
\frac{u(\rho)}{\rho}+\frac{l(l+1)}{2\rho^2}u(\rho)-\lambda u(\rho)=0,
\label{eq:hydrodimless1}
\]
with the hamiltonian
\[
H=-\frac{1}{2}\frac{\partial^2 }{\partial \rho^2}-
\frac{1}{\rho}+\frac{l(l+1)}{2\rho^2}.
\]
Use variational parameter $\alpha$ in the trial
wave function 
\[
   u_T^{\alpha}(\rho)=\alpha\rho e^{-\alpha\rho}. 
   \label{eq:trialhydrogen}
\]

}
\end{small}
}


\frame
{
  \frametitle{Quantum Monte Carlo}
\begin{small}
{\scriptsize
Inserting this wave function into the expression for the
local energy $E_L$ gives
\[
   E_L(\rho)=-\frac{1}{\rho}-
              \frac{\alpha}{2}\left(\alpha-\frac{2}{\rho}\right).
\]

\begin{tabular}{rrrr}\hline
$\alpha$&$\langle H \rangle $&$\sigma^2$&$\sigma/\sqrt{N}$ \\\hline
 7.00000E-01 & -4.57759E-01 &  4.51201E-02 &  6.71715E-04 \\ 
 8.00000E-01 & -4.81461E-01 &  3.05736E-02 &  5.52934E-04 \\ 
 9.00000E-01 & -4.95899E-01 &  8.20497E-03 &  2.86443E-04 \\ 
 1.00000E-00 & -5.00000E-01 &  0.00000E+00 &  0.00000E+00 \\ 
 1.10000E+00 & -4.93738E-01 &  1.16989E-02 &  3.42036E-04 \\ 
 1.20000E+00 & -4.75563E-01 &  8.85899E-02 &  9.41222E-04 \\ 
 1.30000E+00 & -4.54341E-01 &  1.45171E-01 &  1.20487E-03 \\ 
\end{tabular}
}
\end{small}
}


\frame
{
  \frametitle{Quantum Monte Carlo}
\begin{small}
{\scriptsize
We note that at $\alpha=1$ we obtain the exact
result, and the variance is zero, as it should. The reason is that 
we then have the exact wave function, and the action of the hamiltionan
on the wave function
\[
   H\psi = \mathrm{constant}\times \psi,
\]
yields just a constant. The integral which defines various 
expectation values involving moments of the hamiltonian becomes then
\[
   \langle H^n \rangle =
   \frac{\int d{\bf R}\Psi^{\ast}_T({\bf R})H^n({\bf R})\Psi_T({\bf R})}
        {\int d{\bf R}\Psi^{\ast}_T({\bf R})\Psi_T({\bf R})}=
\mathrm{constant}\times\frac{\int d{\bf R}\Psi^{\ast}_T({\bf R})\Psi_T({\bf R})}
        {\int d{\bf R}\Psi^{\ast}_T({\bf R})\Psi_T({\bf R})}=
\mathrm{constant}.
\]
{\bf This gives an important information: the exact wave function leads to zero variance!}
Variation is then performed by minimizing both the energy and the variance.
}
\end{small}
}


\frame
{
  \frametitle{Quantum Monte Carlo}
\begin{small}
{\scriptsize
The helium atom consists of two electrons and a nucleus with
charge $Z=2$. 
The contribution  
to the potential energy due to the attraction from the nucleus is
\[
   -\frac{2ke^2}{r_1}-\frac{2ke^2}{r_2},
\] 
and if we add the repulsion arising from the two 
interacting electrons, we obtain the potential energy
\[
 V(r_1, r_2)=-\frac{2ke^2}{r_1}-\frac{2ke^2}{r_2}+
               \frac{ke^2}{r_{12}},
\]
with the electrons separated at a distance 
$r_{12}=|{\bf r}_1-{\bf r}_2|$.
}
\end{small}
}


\frame
{
  \frametitle{Quantum Monte Carlo}
\begin{small}
{\scriptsize
The hamiltonian becomes then
\[
   \OP{H}=-\frac{\hbar^2\nabla_1^2}{2m}-\frac{\hbar^2\nabla_2^2}{2m}
          -\frac{2ke^2}{r_1}-\frac{2ke^2}{r_2}+
               \frac{ke^2}{r_{12}},
\]
and  Schr\"odingers equation reads
\[
   \OP{H}\psi=E\psi.
\]
All observables are evaluated with respect to the probability distribution
\[
   P({\bf R})= \frac{\left|\psi_T({\bf R})\right|^2}{\int \left|\psi_T({\bf R})\right|^2d{\bf R}}.
\]
generated by the trial wave function.   
The trial wave function must approximate an exact 
eigenstate in order that accurate results are to be obtained. 
Improved trial
wave functions also improve the importance sampling, 
reducing the cost of obtaining a certain statistical accuracy. 
}
\end{small}
}


\frame
{
  \frametitle{Quantum Monte Carlo}
\begin{small}
{\scriptsize
Choice of trial wave function for Helium:
Assume $r_1 \rightarrow 0$.
\[
   E_L({\bf R})=\frac{1}{\psi_T({\bf R})}H\psi_T({\bf R})=
     \frac{1}{\psi_T({\bf R})}\left(-\frac{1}{2}\nabla^2_1
     -\frac{Z}{r_1}\right)\psi_T({\bf R}) + \mathrm{finite \hspace{0.1cm}terms}.
\]
\[ 
    E_L(R)=
    \frac{1}{{\cal R}_T(r_1)}\left(-\frac{1}{2}\frac{d^2}{dr_1^2}-
     \frac{1}{r_1}\frac{d}{dr_1}
     -\frac{Z}{r_1}\right){\cal R}_T(r_1) + \mathrm{finite\hspace{0.1cm} terms}
\]
For small values of $r_1$, the terms which dominate are
\[ 
    \lim_{r_1 \rightarrow 0}E_L(R)=
    \frac{1}{{\cal R}_T(r_1)}\left(-
     \frac{1}{r_1}\frac{d}{dr_1}
     -\frac{Z}{r_1}\right){\cal R}_T(r_1),
\]
since the second derivative does not diverge due to the finiteness of 
$\Psi$ at the origin.
}
\end{small}
}


\frame
{
\frametitle{Quantum Monte Carlo}
\begin{small}
{\scriptsize
This results in
\[
     \frac{1}{{\cal R}_T(r_1)}\frac{d {\cal R}_T(r_1)}{dr_1}=-Z,
\]
and
\[
   {\cal R}_T(r_1)\propto e^{-Zr_1}.
\]
A similar condition applies to electron 2 as well. 
For orbital momenta $l > 0$ we have 
\[
     \frac{1}{{\cal R}_T(r)}\frac{d {\cal R}_T(r)}{dr}=-\frac{Z}{l+1}.
\]
Similarry, studying the case $r_{12}\rightarrow 0$ we can write 
a possible trial wave function as
\[
   \psi_T({\bf R})=e^{-\alpha(r_1+r_2)}e^{\beta r_{12}}.
    \label{eq:wavehelium2}
\]
The last equation can be generalized to
\[
   \psi_T({\bf R})=\phi({\bf r}_1)\phi({\bf r}_2)\dots\phi({\bf r}_N)
                   \prod_{i< j}f(r_{ij}),
\]
for a system with $N$ electrons or particles. 
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{VMC code for helium, vmc\_para.cpp}
\begin{small}
{\scriptsize
\begin{lstlisting}
//  Here we define global variables  used in various functions
//  These can be changed by reading from file the different parameters
int dimension = 3; // three-dimensional system
int charge = 2;  //  we fix the charge to be that of the helium atom
int my_rank, numprocs;  //  these are the parameters used by MPI  to 
                        //    define which node and how many
double step_length = 1.0;  //  we fix the brute force jump to 1 Bohr radius
int number_particles  = 2;  //  we fix also the number of electrons to be 2
\end{lstlisting}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{VMC code for helium, vmc\_para.cpp, main part}
\begin{small}
{\scriptsize
\begin{lstlisting}
  //  MPI initializations, discuss properly next week 
  MPI_Init (&argc, &argv);
  MPI_Comm_size (MPI_COMM_WORLD, &numprocs);
  MPI_Comm_rank (MPI_COMM_WORLD, &my_rank);
  time_start = MPI_Wtime();

  if (my_rank == 0 && argc <= 2) {
    cout << "Bad Usage: " << argv[0] << 
      " read also output file on same line" << endl;
    exit(1);
  }
  if (my_rank == 0 && argc > 2) {
    outfilename=argv[1];
    ofile.open(outfilename); 
  }
\end{lstlisting}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{VMC code for helium, vmc\_para.cpp, main part}
\begin{small}
{\scriptsize
\begin{lstlisting}
// Setting output file name for this rank:
ostringstream ost;
ost << "blocks_rank" << my_rank << ".dat";
// Open file for writing:
blockofile.open(ost.str().c_str(), ios::out | ios::binary);

total_cumulative_e = new double[max_variations+1];
total_cumulative_e2 = new double[max_variations+1];
cumulative_e = new double[max_variations+1];
cumulative_e2 = new double[max_variations+1];

//  initialize the arrays  by zeroing them
for( i=1; i <= max_variations; i++){
   cumulative_e[i] = cumulative_e2[i]  = 0.0; 
   total_cumulative_e[i] = total_cumulative_e2[i]  = 0.0;
}
\end{lstlisting}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{VMC code for helium, vmc\_para.cpp, main part}
\begin{small}
{\scriptsize
\begin{lstlisting}
// broadcast the total number of  variations
MPI_Bcast (&max_variations, 1, MPI_INT, 0, MPI_COMM_WORLD);
MPI_Bcast (&number_cycles, 1, MPI_INT, 0, MPI_COMM_WORLD);
total_number_cycles = number_cycles*numprocs; 
// array to store all energies for last variation of alpha
all_energies = new double[number_cycles+1];
//  Do the mc sampling  and accumulate data with MPI_Reduce
mc_sampling(max_variations, number_cycles, cumulative_e, 
              cumulative_e2, all_energies);
//  Collect data in total averages
for( i=1; i <= max_variations; i++){
  MPI_Reduce(&cumulative_e[i], &total_cumulative_e[i], 1, MPI_DOUBLE, 
                                        MPI_SUM, 0, MPI_COMM_WORLD);
  MPI_Reduce(&cumulative_e2[i], &total_cumulative_e2[i], 1, MPI_DOUBLE, 
                               MPI_SUM, 0, MPI_COMM_WORLD);
}
\end{lstlisting}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{VMC code for helium, vmc\_para.cpp, main part}
\begin{small}
{\scriptsize
\begin{lstlisting}
blockofile.write((char*)(all_energies+1), 
		   number_cycles*sizeof(double));
blockofile.close();
delete [] total_cumulative_e; delete [] total_cumulative_e2; 
delete [] cumulative_e; delete [] cumulative_e2; delete [] all_energies; 
// End MPI
MPI_Finalize ();  
return 0;
}  //  end of main function
\end{lstlisting}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{VMC code for helium, vmc\_para.cpp, sampling}
\begin{small}
{\scriptsize
\begin{lstlisting}
 alpha = 0.5*charge;
 // every node has its own seed for the random numbers
 idum = -1-my_rank;
 // allocate matrices which contain the position of the particles  
 r_old =(double **)matrix(number_particles,dimension,sizeof(double));
 r_new =(double **)matrix(number_particles,dimension,sizeof(double));
 for (i = 0; i < number_particles; i++) { 
    for ( j=0; j < dimension; j++) {
      r_old[i][j] = r_new[i][j] = 0;
    }
  }
 // loop over variational parameters  

\end{lstlisting}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{VMC code for helium, vmc\_para.cpp, sampling}
\begin{small}
{\scriptsize
\begin{lstlisting}
  for (variate=1; variate <= max_variations; variate++){
    // initialisations of variational parameters and energies 
    alpha += 0.1;  
    energy = energy2 = 0; accept =0; delta_e=0;
    //  initial trial position, note calling with alpha 
    for (i = 0; i < number_particles; i++) { 
      for ( j=0; j < dimension; j++) {
	  r_old[i][j] = step_length*(ran2(&idum)-0.5);
      }
    }
    wfold = wave_function(r_old, alpha);
\end{lstlisting}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{VMC code for helium, vmc\_para.cpp, sampling}
\begin{small}
{\scriptsize
\begin{lstlisting}
// loop over monte carlo cycles 
for (cycles = 1; cycles <= number_cycles; cycles++){ 
   // new position 
   for (i = 0; i < number_particles; i++) { 
       for ( j=0; j < dimension; j++) {
  	   r_new[i][j] = r_old[i][j]+step_length*(ran2(&idum)-0.5);
     }  
//  for the other particles we need to set the position to the old position since
//  we move only one particle at the time
     for (k = 0; k < number_particles; k++) {
	 if ( k != i) {
	    for ( j=0; j < dimension; j++) {
	      r_new[k][j] = r_old[k][j];
	    }
	  } 
        }
\end{lstlisting}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{VMC code for helium, vmc\_para.cpp, sampling}
\begin{small}
{\scriptsize
\begin{lstlisting}
      wfnew = wave_function(r_new, alpha); 
// The Metropolis test is performed by moving one particle at the time
      if(ran2(&idum) <= wfnew*wfnew/wfold/wfold ) { 
	  for ( j=0; j < dimension; j++) {
	    r_old[i][j]=r_new[i][j];
	  }
	  wfold = wfnew;
	}
      }  //  end of loop over particles
\end{lstlisting}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{VMC code for helium, vmc\_para.cpp, sampling}
\begin{small}
{\scriptsize
\begin{lstlisting}
      // compute local energy  
      delta_e = local_energy(r_old, alpha, wfold);
      // save all energies on last variate
      if(variate==max_variations){
	   all_energies[cycles] = delta_e;
      }
      // update energies
      energy += delta_e;
      energy2 += delta_e*delta_e;
    }   // end of loop over MC trials   
    // update the energy average and its squared 
    cumulative_e[variate] = energy;
    cumulative_e2[variate] = energy2;
  }    // end of loop over variational  steps 
\end{lstlisting}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{VMC code for helium, vmc\_para.cpp, wave function}
\begin{small}
{\scriptsize
\begin{lstlisting}
// Function to compute the squared wave function, simplest form 

double  wave_function(double **r, double alpha)
{
  int i, j, k;
  double wf, argument, r_single_particle, r_12;
  
  argument = wf = 0;
  for (i = 0; i < number_particles; i++) { 
    r_single_particle = 0;
    for (j = 0; j < dimension; j++) { 
      r_single_particle  += r[i][j]*r[i][j];
    }
    argument += sqrt(r_single_particle);
  }
  wf = exp(-argument*alpha) ;
  return wf;
}
\end{lstlisting}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{VMC code for helium, vmc\_para.cpp, local energy}
\begin{small}
{\scriptsize
\begin{lstlisting}
// Function to calculate the local energy with num derivative

double  local_energy(double **r, double alpha, double wfold)
{
  int i, j , k;
  double e_local, wfminus, wfplus, e_kinetic, e_potential, r_12, 
    r_single_particle;
  double **r_plus, **r_minus;
  
\end{lstlisting}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{VMC code for helium, vmc\_para.cpp, local energy}
\begin{small}
{\scriptsize
\begin{lstlisting}
  // allocate matrices which contain the position of the particles  
  // the function matrix is defined in the progam library 
  r_plus =(double **)matrix(number_particles,dimension,sizeof(double));
  r_minus =(double **)matrix(number_particles,dimension,sizeof(double));
  for (i = 0; i < number_particles; i++) { 
    for ( j=0; j < dimension; j++) {
      r_plus[i][j] = r_minus[i][j] = r[i][j];
    }
  }
\end{lstlisting}
}
\end{small}
}





\frame[containsverbatim]
{
  \frametitle{VMC code for helium, vmc\_para.cpp, local energy}
\begin{small}
{\scriptsize
\begin{lstlisting}
  // compute the kinetic energy  
  e_kinetic = 0;
  for (i = 0; i < number_particles; i++) {
    for (j = 0; j < dimension; j++) { 
      r_plus[i][j] = r[i][j]+h;
      r_minus[i][j] = r[i][j]-h;
      wfminus = wave_function(r_minus, alpha); 
      wfplus  = wave_function(r_plus, alpha); 
      e_kinetic -= (wfminus+wfplus-2*wfold);
      r_plus[i][j] = r[i][j];
      r_minus[i][j] = r[i][j];
    }
  }
// include electron mass and hbar squared and divide by wave function 
  e_kinetic = 0.5*h2*e_kinetic/wfold;
\end{lstlisting}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{VMC code for helium, vmc\_para.cpp, local energy}
\begin{small}
{\scriptsize
\begin{lstlisting}
  // compute the potential energy 
  e_potential = 0;
  // contribution from electron-proton potential  
  for (i = 0; i < number_particles; i++) { 
    r_single_particle = 0;
    for (j = 0; j < dimension; j++) { 
      r_single_particle += r[i][j]*r[i][j];
    }
    e_potential -= charge/sqrt(r_single_particle);
  }
\end{lstlisting}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{VMC code for helium, vmc\_para.cpp, local energy}
\begin{small}
{\scriptsize
\begin{lstlisting}
  // contribution from electron-electron potential  
  for (i = 0; i < number_particles-1; i++) { 
    for (j = i+1; j < number_particles; j++) {
      r_12 = 0;  
      for (k = 0; k < dimension; k++) { 
	r_12 += (r[i][k]-r[j][k])*(r[i][k]-r[j][k]);
      }
      e_potential += 1/sqrt(r_12);          
    }
  }
\end{lstlisting}
}
\end{small}
}




\frame
{
  \frametitle{Structuring the code}
\begin{small}
{\scriptsize
During the development of our code we need to make several checks. It is also very instructive to compute a closed form expression for the local energy. Since our wave function is rather simple  it is straightforward
to find an analytic expressions.  Consider first the case of the simple helium function 
\[
   \Psi_T(\mathbf{r}_1,\mathbf{r}_2) = e^{-\alpha(r_1+r_2)}
\]
The local energy is for this case 
\[ 
E_{L1} = \left(\alpha-Z\right)\left(\frac{1}{r_1}+\frac{1}{r_2}\right)+\frac{1}{r_{12}}-\alpha^2
\]
which gives an expectation value for the local energy given by
\[
\langle E_{L1} \rangle = \alpha^2-2\alpha\left(Z-\frac{5}{16}\right)
\]
In our project, the simple form is
\[
   \Psi_T(\mathbf{r}_1,\mathbf{r}_2) = e^{-\alpha\omega(r_1^2+r_2^2)/2}
\]
Find the contribution to the local energy!
}
\end{small}
}

\frame
{
  \frametitle{Structuring the code}
\begin{small}
{\scriptsize
With closed form formulae we  can speed up the computation of the correlation. In our case
we write it as 
\[
\Psi_C= \exp{\left\{\sum_{i<j}\frac{ar_{ij}}{1+\beta r_{ij}}\right\}},
\]
which means that the gradient needed for the so-called quantum force and local energy 
can be calculated analytically.
This will speed up your code since the computation of the correlation part and the Slater determinant are the most 
time consuming parts in your code.  

We will refer to this correlation function as $\Psi_C$ or the \emph{linear Pad\'e-Jastrow}.

}
\end{small}
}

\frame
{
  \frametitle{Structuring the code}
\begin{small}
{\scriptsize
We can test this by computing the local energy for our helium wave function

\[
   \psi_{T}({\bf r}_1,{\bf r}_2) = 
   \exp{\left(-\alpha(r_1+r_2)\right)}
   \exp{\left(\frac{r_{12}}{2(1+\beta r_{12})}\right)}, 
\]

with $\alpha$ and $\beta$ as variational parameters.

The local energy is for this case 
\[ 
E_{L2} = E_{L1}+\frac{1}{2(1+\beta r_{12})^2}\left\{\frac{\alpha(r_1+r_2)}{r_{12}}(1-\frac{\mathbf{r}_1\mathbf{r}_2}{r_1r_2})-\frac{1}{2(1+\beta r_{12})^2}-\frac{2}{r_{12}}+\frac{2\beta}{1+\beta r_{12}}\right\}
\]
It is very useful to test your code against these expressions. It means also that you don't need to
compute a derivative numerically as discussed last week.  This week you should find the corresponding expression 
for a quantum dot system with two electrons.
}
\end{small}
}



\frame
{
  \frametitle{Your tasks for today and till next week}
\begin{small}
{\scriptsize
\begin{itemize}
\item Implement the closed form expression for the local energy
\item Convince yourself that the closed form expressions are correct. See also the slides below. Background: Lars Eivind Lerv\aa g's thesis, chapter 7.1 and 7.2
\item Implement the above expressions for systems with more than two electrons.
\item Finish part 1a and 1b. 
\end{itemize}
}
\end{small}
}

\frame
{
  \frametitle{Structuring the code, simple task}
\begin{small}
{\scriptsize
\begin{itemize}
\item Make another copy of your code.
\item Implement the closed form expression for the local energy
\item Compile the new and old codes with the -pg option for profiling.
\item Run both codes and profile them afterwards using $\mathrm{gprof} \{\mathrm{name executable}\} > \mathrm{outprofile}$
\item Study the time usage in the file {\bf outprofile}
\end{itemize}
}
\end{small}
}


\frame
 {
   \frametitle{Efficient calculations of wave function ratios}
 \begin{small}
 {\scriptsize

In the Metropolis/Hasting algorithm, the \emph{acceptance ratio} determines the probability for a particle to be accepted at a new position. The ratio of the trial wave functions evaluated at the new and current positions is given by

\begin{equation}\label{acceptanceRatio}
\boxed{R \equiv \frac{\Psi_{T}^{new}}{\Psi_{T}^{cur}} = \underbrace{\frac{\Psi_{D}^{new}}{\Psi_{D}^{cur}}}_{R_{SD}}\, \underbrace{\frac{\Psi_{C}^{new}}{\Psi_{C}^{cur}}}_{R_{C}}.}
\end{equation}
Here $\Psi_{D}$ is our Slater determinant while $\Psi_{C}$ is our correlation function. 
We need to optimize $\nabla \Psi_T / \Psi_T$ ratio and the second derivative as well, that is
the $\nabla^2 \Psi_T/\Psi_T$ ratio. The first is needed when we compute the so-called quantum force in importance sampling.
The second is needed when we compute the kinetic energy term of the local energy.
\[
\frac{\Grad \Psi}{\Psi}  = \frac{\Grad (\Psi_{D} \, \Psi_{C})}{\Psi_{D} \, \Psi_{C}}  =  \frac{ \Psi_C \Grad \Psi_{D} + \Psi_{D} \Grad \Psi_{C}}{\Psi_{D} \Psi_{C}} = \frac{\Grad \Psi_{D}}{\Psi_{D}} + \frac{\Grad  \Psi_C}{ \Psi_C}
\]
 }
 \end{small}
 }


\frame
 {
   \frametitle{Efficient calculations of wave function ratios}
 \begin{small}
 {\scriptsize

The expectation value of the kinetic energy expressed in atomic units for electron $i$ is 
\begin{equation}
 \langle \Op{K}_i \rangle = -\frac{1}{2}\frac{\langle\Psi|\nabla_{i}^2|\Psi \rangle}{\langle\Psi|\Psi \rangle},
\end{equation}

\begin{equation}\label{kineticE}
K_i = -\frac{1}{2}\frac{\nabla_{i}^{2} \Psi}{\Psi}.
\end{equation}
\begin{eqnarray}
\frac{\nabla^2 \Psi}{\Psi} & = & \frac{\nabla^2 ({\Psi_{D} \,  \Psi_C})}{\Psi_{D} \,  \Psi_C} = \frac{\Grad \cdot [\Grad {(\Psi_{D} \,  \Psi_C)}]}{\Psi_{D} \,  \Psi_C} = \frac{\Grad \cdot [ \Psi_C \Grad \Psi_{D} + \Psi_{D} \Grad  \Psi_C]}{\Psi_{D} \,  \Psi_C}\nonumber\\
&  = & \frac{\Grad  \Psi_C \cdot \Grad \Psi_{D} +  \Psi_C \nabla^2 \Psi_{D} + \Grad \Psi_{D} \cdot \Grad  \Psi_C + \Psi_{D} \nabla^2  \Psi_C}{\Psi_{D} \,  \Psi_C}\nonumber\\
\end{eqnarray}
\begin{eqnarray}
\frac{\nabla^2 \Psi}{\Psi}
& = & \frac{\nabla^2 \Psi_{D}}{\Psi_{D}} + \frac{\nabla^2  \Psi_C}{ \Psi_C} + 2 \frac{\Grad \Psi_{D}}{\Psi_{D}}\cdot\frac{\Grad  \Psi_C}{ \Psi_C}
\end{eqnarray}
 }
 \end{small}
 }


\frame
 {
   \frametitle{Definitions}
 \begin{small}
 {\scriptsize
We define the correlated function as
\[
\Psi_C=\prod_{i< j}g(r_{ij})=\prod_{i< j}^Ng(r_{ij})= \prod_{i=1}^N\prod_{j=i+1}^Ng(r_{ij}),
\]
with 
$r_{ij}=|{\bf r}_i-{\bf r}_j|=\sqrt{(x_i-x_j)^2+(y_i-y_j)^2+(z_i-z_j)^2}$ for three dimensions and
$r_{ij}=|{\bf r}_i-{\bf r}_j|=\sqrt{(x_i-x_j)^2+(y_i-y_j)^2}$ for two dimensions.

In our particular case we have
\[
\Psi_C=\prod_{i< j}g(r_{ij})=\exp{\left\{\sum_{i<j}f(r_{ij})\right\}}=
\exp{\left\{\sum_{i<j}\frac{ar_{ij}}{1+\beta r_{ij}}\right\}},
\]
 }
 \end{small}
 }





\frame
 {
   \frametitle{Efficient calculations of wave function ratios}
 \begin{small}
 {\scriptsize

The total number of different relative distances $r_{ij}$ is $N(N-1)/2$. In a matrix storage format, the set forms a strictly upper triangular matrix
\begin{equation}\label{utrij}
 \bfv{r} \equiv \begin{pmatrix}
  0 & r_{1,2} & r_{1,3} & \cdots & r_{1,N} \\
  \vdots & 0       & r_{2,3} & \cdots & r_{2,N} \\
  \vdots & \vdots  & 0  & \ddots & \vdots  \\
  \vdots & \vdots  & \vdots  & \ddots  & r_{N-1,N} \\
  0 & 0  & 0  & \cdots  & 0
 \end{pmatrix}.
\end{equation}
This applies to  $\bfv{g} = \bfv{g}(r_{ij})$ as well. 

In our algorithm we will move one particle at the time, say the $kth$-particle. Keep this in mind in the discussion to come.
 }
 \end{small}
 }


\frame
 {
   \frametitle{Efficient calculations of wave function ratios}
 \begin{small}
 {\scriptsize
\begin{equation}\label{RjfRatio}
 \boxed{R_{C} = \frac{\Psi_{C}^\mathrm{new}}{\Psi_{C}^\mathrm{cur}} =
\prod_{i=1}^{k-1}\frac{g_{ik}^\mathrm{new}}{g_{ik}^\mathrm{cur}}\;
\prod_{i=k+1}^{N}\frac{g_{ki}^\mathrm{new}}{g_{ki}^\mathrm{cur}}}.
\end{equation}\label{padepadeRatio}
For the Pad\'e-Jastrow form
\begin{equation}
 \boxed{R_{C} = \frac{\Psi_{C}^\mathrm{new}}{\Psi_{C}^\mathrm{cur}} = \frac{e^{U_{new}}}{e^{U_{cur}}} = e^{\Delta U},}
\end{equation}
where
\begin{equation}
\Delta U =
\sum_{i=1}^{k-1}\big(f_{ik}^\mathrm{new}-f_{ik}^\mathrm{cur}\big)
+
\sum_{i=k+1}^{N}\big(f_{ki}^\mathrm{new}-f_{ki}^\mathrm{cur}\big)
\end{equation}

One needs to develop a special algorithm 
that runs only through the elements of the upper triangular
matrix $\bfv{g}$ and have $k$ as an index. 

 }
 \end{small}
 }


\frame
 {
   \frametitle{Efficient calculations of wave function ratios}
 \begin{small}
 {\scriptsize
The expression to be derived in the following is of interest when computing the quantum force and the kinetic energy. It has the form
$$
\frac{\bfv{{\nabla_i}}\Psi_C}{\Psi_C} = \frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_i},
$$
for all dimensions and with $i$ running over all particles.
For the first derivative only $N-1$ terms survive the ratio because the $g$-terms that are not differentiated cancel with their corresponding ones in the denominator. Then,
\begin{equation}\label{1jgradG}
\frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_k} =
\sum_{i=1}^{k-1}\frac{1}{g_{ik}}\frac{\partial g_{ik}}{\partial x_k}
+
\sum_{i=k+1}^{N}\frac{1}{g_{ki}}\frac{\partial g_{ki}}{\partial x_k}.
\end{equation}
An equivalent equation is obtained for the exponential form after replacing $g_{ij}$ by $\exp(f_{ij})$, yielding:
\begin{equation}\label{1jgradEG}
\frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_k} =
\sum_{i=1}^{k-1}\frac{\partial g_{ik}}{\partial x_k}
+
\sum_{i=k+1}^{N}\frac{\partial g_{ki}}{\partial x_k},
\end{equation}
with both expressions scaling as $\mathcal{O}(N)$.
 }
 \end{small}
 }


\frame
 {
   \frametitle{Efficient calculations of wave function ratios}
 \begin{small}
 {\scriptsize
Using the identity 
\begin{equation}\label{firstDerIdentity}
\frac{\partial}{\partial x_i}g_{ij} = -\frac{\partial}{\partial x_j}g_{ij} 
\end{equation}
on the right hand side terms of Eq.~(\ref{1jgradG}) and Eq.~(\ref{1jgradEG}), we get expressions where all the derivatives acting on the particle are represented by the
\emph{second} index of $g$:
\begin{equation}\label{gradJasGen}
\boxed{
\frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_k} =
\sum_{i=1}^{k-1}\frac{1}{g_{ik}}\frac{\partial g_{ik}}{\partial x_k}
-
\sum_{i=k+1}^{N}\frac{1}{g_{ki}}\frac{\partial g_{ki}}{\partial x_i},
}
\end{equation}
and for the exponential case:
\begin{equation}\label{gradJasGenExp}
\boxed{
\frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_k} =
\sum_{i=1}^{k-1}\frac{\partial g_{ik}}{\partial x_k}
-
\sum_{i=k+1}^{N}\frac{\partial g_{ki}}{\partial x_i}.
}
\end{equation}
 }
 \end{small}
 }
\frame
 {
   \frametitle{Efficient calculations of wave function ratios}
 \begin{small}
 {\scriptsize

For correlation forms depending only on the scalar distances $r_{ij}$ we can use the chain rule. Noting that 
\begin{equation}\label{chainRule}
\frac{\partial g_{ij}}{\partial x_j} = \frac{\partial g_{ij}}{\partial r_{ij}} \frac{\partial r_{ij}}{\partial x_j} = \frac{x_j - x_i}{r_{ij}} \frac{\partial g_{ij}}{\partial r_{ij}},
\end{equation}
after substitution in Eq.~(\ref{gradJasGen}) and Eq.~(\ref{gradJasGenExp}) we arrive at
\begin{equation}\label{generalCorrelation}
\boxed{
\frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_k} = 
\sum_{i=1}^{k-1}\frac{1}{g_{ik}} \frac{\bfv{r_{ik}}}{r_{ik}} \frac{\partial g_{ik}}{\partial r_{ik}}
-
\sum_{i=k+1}^{N}\frac{1}{g_{ki}}\frac{\bfv{r_{ki}}}{r_{ki}}\frac{\partial g_{ki}}{\partial r_{ki}}.
}
\end{equation}
 }
 \end{small}
 }
\frame
 {
   \frametitle{Efficient calculations of wave function ratios}
 \begin{small}
 {\scriptsize

Note that for the Pad\'e-Jastrow form we can set $g_{ij} \equiv g(r_{ij}) = e^{f(r_{ij})} = e^{f_{ij}}$ and 
\begin{equation}
\frac{\partial g_{ij}}{\partial r_{ij}} = g_{ij} \frac{\partial f_{ij}}{\partial r_{ij}}.
\end{equation}
Therefore, 
\begin{equation}\label{padeJastrowGradJasRatio}
\boxed{
\frac{1}{\Psi_{C}}\frac{\partial \Psi_{C}}{\partial x_k} =
\sum_{i=1}^{k-1}\frac{\bfv{r_{ik}}}{r_{ik}}\frac{\partial f_{ik}}{\partial r_{ik}}
-
\sum_{i=k+1}^{N}\frac{\bfv{r_{ki}}}{r_{ki}}\frac{\partial f_{ki}}{\partial r_{ki}},
}
\end{equation}
where 
\begin{equation}\label{distanceVector}
 \bfv{r}_{ij} = |\bfv{r}_j - \bfv{r}_i| = (x_j - x_i)\vec{e}_1 + (y_j - y_i)\vec{e}_2 + (z_j - z_i)\vec{e}_3
\end{equation}
is the vectorial distance. When the correlation function is the \emph{linear Pad\'e-Jastrow} we set \begin{equation}
f_{ij} = \frac{a r_{ij}}{(1 + \beta r_{ij})},
\end{equation}
which yields the analytical expression
\begin{equation}\label{analyticalPJGrad}
 \boxed{\frac{\partial f_{ij}}{\partial r_{ij}} = \frac{a}{(1 + \beta r_{ij})^2}}.
\end{equation}
 }
 \end{small}
 }



\frame
 {
   \frametitle{Efficient calculations of wave function ratios}
 \begin{small}
 {\scriptsize

{Computing the $\nabla^2 \Psi_C/\Psi_C$ ratio}

\[\bfv{\nabla}_k \Psi_C = 
\sum_{i=1}^{k-1}\frac{1}{g_{ik}} \bfv{\nabla}_k g_{ik}
+
\sum_{i=k+1}^{N}\frac{1}{g_{ki}}\bfv{\nabla}_k g_{ki}.\]
After multiplying by $\Psi_C$ and taking the gradient on both sides we get,
\begin{align}\label{gradLap}
\nabla_{k}^2 \Psi_C & = \bfv{\nabla}_k \Psi_C \cdot 
\left(\sum_{i=1}^{k-1}\frac{1}{g_{ik}} \bfv{\nabla}_k g_{ik}
+
\sum_{i=k+1}^{N}\frac{1}{g_{ki}}\bfv{\nabla}_k g_{ki}\right)\nonumber\\
&+
\Psi_C \nabla_k \cdot \left(\sum_{i=k+1}^{N}\frac{1}{g_{ki}}\bfv{\nabla}_k g_{ki}
+
\sum_{i=k+1}^{N}\frac{1}{g_{ki}}\bfv{\nabla}_k g_{ki}\right)\nonumber\\
& = \Psi_C \left(\frac{\bfv{\nabla}_k \Psi_C}{\Psi_C}\right)^2 +
\Psi_C \nabla_k \cdot \left(\sum_{i=k+1}^{N}\frac{1}{g_{ki}}\bfv{\nabla}_k g_{ki}
+
\sum_{i=k+1}^{N}\frac{1}{g_{ki}}\bfv{\nabla}_k g_{ki}\right).
\end{align}
 }
 \end{small}
 }
\frame
 {
   \frametitle{Efficient calculations of wave function ratios}
 \begin{small}
 {\scriptsize

Now,
\begin{align}
 \bfv{\nabla}_k \cdot \left(\frac{1}{g_{ik}}\bfv{\nabla}_k g_{ik}\right) &= \bfv{\nabla}_k \left(\frac{1}{g_{ik}}\right)\cdot \bfv{\nabla}_k g_{ik} + \frac{1}{g_{ik}}\bfv{\nabla}_k \cdot \bfv{\nabla}_k g_{ik}\nonumber\\
 & = -\frac{1}{g_{ik}^2} \bfv{\nabla}_k g_{ik} \cdot \bfv{\nabla}_k g_{ik} + \frac{1}{g_{ik}} \bfv{\nabla}_k \cdot \left(\frac{\bfv{r}_{ik}}{r_{ik}}\frac{\partial g_{ik}}{\partial r_{ik}}\right)\nonumber\\
 & = -\frac{1}{g_{ik}^2} (\bfv{\nabla}_k g_{ik})^2 \nonumber\\&+ \frac{1}{g_{ik}}\left[\bfv{\nabla}_k \left(\frac{1}{r_{ik}}\frac{\partial g_{ik}}{\partial r_{ik}}\right)\cdot \bfv{r}_{ik} + \left(\frac{1}{r_{ik}}\frac{\partial g_{ik}}{\partial r_{ik}}\right) \bfv{\nabla}_k \cdot \bfv{r}_{ik}  \right] \nonumber\\
 &= -\frac{1}{g_{ik}^2} \left(\frac{\bfv{r}_{ik}}{r_{ik}}\frac{\partial g_{ik}}{\partial r_{ik}}\right)^2\nonumber\\ &+ \frac{1}{g_{ik}}\left[\bfv{\nabla}_k \left(\frac{1}{r_{ik}}\frac{\partial g_{ik}}{\partial r_{ik}}\right)\cdot \bfv{r}_{ik} + \left(\frac{1}{r_{ik}}\frac{\partial g_{ik}}{\partial r_{ik}}\right) d  \right]\nonumber\\
 &= -\frac{1}{g_{ik}^2} \left(\frac{\partial g_{ik}}{\partial r_{ik}}\right)^2\nonumber\\ &+ \frac{1}{g_{ik}}\left[\bfv{\nabla}_k \left(\frac{1}{r_{ik}}\frac{\partial g_{ik}}{\partial r_{ik}}\right)\cdot \bfv{r}_{ik} + \left(\frac{1}{r_{ik}}\frac{\partial g_{ik}}{\partial r_{ik}}\right) d  \right], \label{subs0}
 \end{align}
with $d$ being the number of spatial dimensions.
 }
 \end{small}
 }
\frame
 {
   \frametitle{Efficient calculations of wave function ratios}
 \begin{small}
 {\scriptsize

Moreover, 
\begin{align*}
\bfv{\nabla}_k \left(\frac{1}{r_{ik}}\frac{\partial g_{ik}}{\partial r_{ik}}\right) &= \frac{\bfv{r}_{ik}}{r_{ik}} \frac{\partial }{\partial r_{ik}} \left(\frac{1}{r_{ik}}\frac{\partial g_{ik}}{\partial r_{ik}}\right)\nonumber\\
&=\frac{\bfv{r}_{ik}}{r_{ik}}\left(-\frac{1}{r_{ik}^2}\frac{\partial g_{ik}}{\partial r_{ik}} + \frac{1}{r_{ik}}\frac{\partial^2 g_{ik}}{\partial r_{ik}^2}\right).\label{subs1}
\end{align*}

We finally get
\begin{align*}
  \bfv{\nabla}_k \cdot \left(\frac{1}{g_{ik}}\bfv{\nabla}_k g_{ik}\right) &= -\frac{1}{g_{ik}^2}\left(\frac{\partial g_{ik}}{\partial r_{ik}}\right)^2 + \frac{1}{g_{ik}}\left[\left(\frac{d-1}{r_{ik}}\right)\frac{\partial g_{ik}}{\partial r_{ik}} + \frac{\partial^2 g_{ik}}{\partial r_{ik}^2} \right].
\end{align*}
 }
 \end{small}
 }
\frame
 {
   \frametitle{Efficient calculations of wave function ratios}
 \begin{small}
 {\scriptsize

Inserting the last expression in Eq.~(\ref{gradLap}) and after division by $\Psi_C$ we get,

\begin{align}
 \frac{\nabla_{k}^2 \Psi_C}{\Psi_C} & =  \left(\frac{\bfv{\nabla}_k \Psi_C}{\Psi_C}\right)^2 \nonumber\\
 & + \sum_{i=1}^{k-1} -\frac{1}{g_{ik}^2}\left(\frac{\partial g_{ik}}{\partial r_{ik}}\right)^2 + \frac{1}{g_{ik}}\left[\left(\frac{d-1}{r_{ik}}\right)\frac{\partial g_{ik}}{\partial r_{ik}} + \frac{\partial^2 g_{ik}}{\partial r_{ik}^2} \right]\nonumber\\
 & + \sum_{i=k+1}^{N} -\frac{1}{g_{ki}^2}\left(\frac{\partial g_{ki}}{\partial r_{ki}}\right)^2 + \frac{1}{g_{ki}}\left[\left(\frac{d-1}{r_{ki}}\right)\frac{\partial g_{ki}}{\partial r_{ki}} + \frac{\partial^2 g_{ki}}{\partial r_{ki}^2} \right].
\end{align}
 }
 \end{small}
 }
\frame
 {
   \frametitle{Efficient calculations of wave function ratios}
 \begin{small}
 {\scriptsize

For the exponential case we have
\begin{align*}
 \frac{\nabla_{k}^2 \Psi_{C}}{\Psi_{C}} & =  \left(\frac{\bfv{\nabla}_k \Psi_{C}}{\Psi_{C}}\right)^2 \nonumber\\
 & + \sum_{i=1}^{k-1} -\frac{1}{g_{ik}^2}\left(g_{ik}\frac{\partial f_{ik}}{\partial r_{ik}}\right)^2 + \frac{1}{g_{ik}}\left[\left(\frac{d-1}{r_{ik}}\right)g_{ik}\frac{\partial f_{ik}}{\partial r_{ik}} + \frac{\partial }{\partial r_{ik}}\left(g_{ik}\frac{\partial f_{ik}}{\partial r_{ik}}\right) \right]\nonumber\\
 & + \sum_{i=k+1}^{N} -\frac{1}{g_{ki}^2}\left(g_{ik}\frac{\partial f_{ki}}{\partial r_{ki}}\right)^2 + \frac{1}{g_{ki}}\left[\left(\frac{d-1}{r_{ki}}\right)g_{ki}\frac{\partial f_{ki}}{\partial r_{ki}} + \frac{\partial }{\partial r_{ki}}\left(g_{ki}\frac{\partial f_{ki}}{\partial r_{ki}}\right) \right].
 \end{align*}
 }
 \end{small}
 }\frame
 {
   \frametitle{Efficient calculations of wave function ratios}
 \begin{small}
 {\scriptsize

Using
\begin{align*}
 \frac{\partial }{\partial r_{ik}}\left(g_{ik}\frac{\partial f_{ik}}{\partial r_{ik}}\right) & = \frac{\partial g_{ik}}{\partial r_{ik}}\frac{\partial f_{ik}}{\partial r_{ik}} + g_{ik}\frac{\partial^2 f_{ik}}{\partial r_{ik}^2}\\
 & = g_{ik}\frac{\partial f_{ik}}{\partial r_{ik}}\frac{\partial f_{ik}}{\partial r_{ik}} + g_{ik}\frac{\partial^2 f_{ik}}{\partial r_{ik}^2}\\
 & = g_{ik}\left(\frac{\partial f_{ik}}{\partial r_{ik}}\right)^2 + g_{ik}\frac{\partial^2 f_{ik}}{\partial r_{ik}^2}
\end{align*}
and substituting this result into the equation above gives rise to the final expression,
\begin{align}\label{lapJasRatio}
\frac{\nabla_{k}^2 \Psi_{PJ}}{\Psi_{PJ}}  &=  \left(\frac{\bfv{\nabla}_k \Psi_{PJ}}{\Psi_{PJ}}\right)^2\nonumber\\
  &+ \sum_{i=1}^{k-1} \left[\left(\frac{d-1}{r_{ik}}\right)\frac{\partial f_{ik}}{\partial r_{ik}} + \frac{\partial^2  f_{ik}}{\partial r_{ik}^2} \right]
  + \sum_{i=k+1}^{N} \left[\left(\frac{d-1}{r_{ki}}\right)\frac{\partial f_{ki}}{\partial r_{ki}} + \frac{\partial^2 f_{ki}}{\partial r_{ki}^2} \right].
 \end{align}
 }
 \end{small}
 }


\frame
{
  \frametitle{Summing up: Bringing it all together, Local energy}
\begin{small}
{\scriptsize

The second derivative of the Jastrow factor divided by the Jastrow factor (the way it enters the kinetic energy) is
\[
\left[\frac{\nabla^2 \Psi_C}{\Psi_C}\right]_x =\  
2\sum_{k=1}^{N}
\sum_{i=1}^{k-1}\frac{\partial^2 g_{ik}}{\partial x_k^2}\ +\ 
\sum_{k=1}^N
\left(
\sum_{i=1}^{k-1}\frac{\partial g_{ik}}{\partial x_k} -
\sum_{i=k+1}^{N}\frac{\partial g_{ki}}{\partial x_i}
\right)^2
\]
But we have a simple form for the function, namely
\[
\Psi_{C}=\prod_{i< j}\exp{f(r_{ij})}= \exp{\left\{\sum_{i<j}\frac{ar_{ij}}{1+\beta r_{ij}}\right\}},
\]
and it is easy to see that for particle $k$
we have
\[
  \frac{\nabla^2_k \Psi_C}{\Psi_C }=
\sum_{ij\ne k}\frac{({\bf r}_k-{\bf r}_i)({\bf r}_k-{\bf r}_j)}{r_{ki}r_{kj}}f'(r_{ki})f'(r_{kj})+
\sum_{j\ne k}\left( f''(r_{kj})+\frac{2}{r_{kj}}f'(r_{kj})\right)
\]
}
\end{small}
}



\frame
{
  \frametitle{Bringing it all together, Local energy}
\begin{small}
{\scriptsize
Using 
\[
f(r_{ij})= \frac{ar_{ij}}{1+\beta r_{ij}},
\]
and $g'(r_{kj})=dg(r_{kj})/dr_{kj}$ and 
$g''(r_{kj})=d^2g(r_{kj})/dr_{kj}^2$  we find that for particle $k$
we have
\[
  \frac{\nabla^2_k \Psi_C}{\Psi_C }=
\sum_{ij\ne k}\frac{({\bf r}_k-{\bf r}_i)({\bf r}_k-{\bf r}_j)}{r_{ki}r_{kj}}\frac{a}{(1+\beta r_{ki})^2}
\frac{a}{(1+\beta r_{kj})^2}+
\sum_{j\ne k}\left(\frac{2a}{r_{kj}(1+\beta r_{kj})^2}-\frac{2a\beta}{(1+\beta r_{kj})^3}\right)
\]
}
\end{small}
}

\frame
{
  \frametitle{Important feature}
\begin{small}
{\scriptsize
For the correlation part 
\[
\Psi_C=\prod_{i< j}g(r_{ij})= \exp{\left\{\sum_{i<j}\frac{ar_{ij}}{1+\beta r_{ij}}\right\}},
\]
we need to take into account whether electrons have equal or opposite spins since we have to obey the
electron-electron cusp condition as well.  
When the electrons have  equal spins 
\[
a= 1/3,
\]
while for opposite spins (as for the ground state of a quantum dot with two electrons)
\[
a= 1.
\] 
}
\end{small}
}




\section[Week 6]{Weeks 6}
\frame
{
  \frametitle{Topics for Week 6, February 7-11}
  \begin{block}{Importance sampling, Fokker-Planck and Langevin equations and parallelization}
\begin{itemize}
\item Repetition from last week
\item Importance sampling, discussion of codes, crash introduction to get you started
\item MPI programming and access to titan.uio.no
\item Derivation of the Fokker-Planck and the Langevin equations (Background material) if we get time, else this is postponed till next week.
\end{itemize}
Project work this week: finalize 1a and 1b.  Start implementing importance sampling and exercise 1c.
Next week we discuss blocking as a tool to perform statistical analysis of MonteCarlo data.  We will also continue the discussion on importance sampling.
  \end{block}
} 



\frame[containsverbatim]
{
  \frametitle{Importance sampling, what we want to do}
\begin{small}
{\scriptsize
We need to replace the brute force
Metropolis algorithm with a walk in coordinate space biased by the trial wave function.
This approach is based on the Fokker-Planck equation and the Langevin equation for generating a trajectory in coordinate space.  This is explained later.

For a diffusion process characterized by a time-dependent probability density $P(x,t)$ in one dimension the Fokker-Planck
equation reads (for one particle/walker) 
\[
   \frac{\partial P}{\partial t} = D\frac{\partial }{\partial x}\left(\frac{\partial }{\partial x} -F\right)P(x,t),
\]
where $F$ is a drift term and $D$ is the diffusion coefficient. 

The new positions in coordinate space are given as the solutions of the Langevin equation using Euler's method, namely,
we go from the Langevin equation
\[ 
   \frac{\partial x(t)}{\partial t} = DF(x(t)) +\eta,
\]
with $\eta$ a random variable,
yielding a new position 
\[
   y = x+DF(x)\Delta t +\xi,
\]
where $\xi$ is gaussian random variable and $\Delta t$ is a chosen time step. 
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, what we want to do}
\begin{small}
{\scriptsize
The process of isotropic diffusion characterized by a time-dependent probability density $P(\bfv{x},t)$ obeys (as an approximation) 
the so-called Fokker-Planck equation 
$$
   \frac{\partial P}{\partial t} = \sum_i D\frac{\partial }{\partial \bfv{x_i}}\left(\frac{\partial }{\partial \bfv{x_i}} -\bfv{F_i}\right)P(\bfv{x},t),
$$
where $\bfv{F_i}$ is the $i^{th}$ component of the drift term (drift velocity) caused by an external potential, and $D$ is the diffusion coefficient. The convergence to a stationary probability density can be obtained by setting the left hand side to zero. The resulting equation will be satisfied if and only if all the terms of the sum are equal zero,
$$
\frac{\partial^2 P}{\partial {\bfv{x_i}^2}} = P\frac{\partial}{\partial {\bfv{x_i}}}\bfv{F_i} + \bfv{F_i}\frac{\partial}{\partial {\bfv{x_i}}}P.
$$
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, what we want to do}
\begin{small}
{\scriptsize
The drift vector should be of the form $\bfv{F} = g(\bfv{x}) \frac{\partial P}{\partial \bfv{x}}$. Then,
$$
\frac{\partial^2 P}{\partial {\bfv{x_i}^2}} = P\frac{\partial g}{\partial P}\left( \frac{\partial P}{\partial {\bfv{x_i}}}  \right)^2 + P g \frac{\partial ^2 P}{\partial {\bfv{x_i} ^2}}  + g \left( \frac{\partial P}{\partial {\bfv{x_i}}}  \right)^2.
$$
The condition of stationary density means that the left hand side equals zero. In other words, the terms containing first and second derivatives have to cancel each other. It is possible only if $g = \frac{1}{P}$, which yields
\begin{equation}\label{quantumForceEQ}
\boxed{\bfv{F} = 2\frac{1}{\Psi_T}\nabla\Psi_T,}
\end{equation}
which is known as the so-called \emph{quantum force}. This term is responsible for pushing the walker towards regions of configuration space where the trial wave function is large, increasing the efficiency of the simulation in contrast to the Metropolis algorithm where the walker has the same probability of moving in every direction.
}
\end{small}
}





\frame
{
  \frametitle{Importance Sampling}
\begin{small}
{\scriptsize
The Fokker-Planck equation yields a (the solution to the equation) transition probability given by the Green's function
\[
  G(y,x,\Delta t) = \frac{1}{(4\pi D\Delta t)^{3N/2}} \exp{\left(-(y-x-D\Delta t F(x))^2/4D\Delta t\right)}
\]
which in turn means that our brute force Metropolis algorithm
\[ 
    A(y,x) = \mathrm{min}(1,q(y,x))),
\]
with $q(y,x) = |\Psi_T(y)|^2/|\Psi_T(x)|^2$ is now replaced by
\[
q(y,x) = \frac{G(x,y,\Delta t)|\Psi_T(y)|^2}{G(y,x,\Delta t)|\Psi_T(x)|^2}
\]
See program vmc\_importance.cpp for example. 
Read more in Thijssen's text chapters 8.8 and 12.2.

}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, new positions, see code vmc\_importance.cpp under the programs link}
%  \begin{block}{}
\begin{small}
{\scriptsize
\lstset{language=c++}  
\begin{lstlisting}
 for (variate=1; variate <= max_variations; variate++){
    // initialisations of variational parameters and energies 
    beta += 0.1;  
    energy = energy2 = delta_e = 0.0;
    //  initial trial position, note calling with beta 
    for (i = 0; i < number_particles; i++) { 
      for ( j=0; j < dimension; j++) {
	r_old[i][j] = gaussian_deviate(&idum)*sqrt(timestep);
      }
    }
    wfold = wave_function(r_old, beta);
    quantum_force(r_old, qforce_old, beta, wfold);
\end{lstlisting} 
}
\end{small}
%  \end{block}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, new positions in function vmc\_importance.cpp}
%  \begin{block}{}
\begin{small}
{\scriptsize
\lstset{language=c++}  
\begin{lstlisting}
    // loop over monte carlo cycles 
    for (cycles = 1; cycles <= number_cycles; cycles++){ 
      // new position 
      for (i = 0; i < number_particles; i++) { 
	for ( j=0; j < dimension; j++) {
	  // gaussian deviate to compute new positions using a given timestep
	  r_new[i][j] = r_old[i][j] + gaussian_deviate(&idum)*sqrt(timestep)+qforce_old[i][j]*timestep*D;
\end{lstlisting} 
}
\end{small}
%  \end{block}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, new positions in function vmc\_importance.cpp}
%  \begin{block}{}
\begin{small}
{\scriptsize
\lstset{language=c++}  
\begin{lstlisting}
//  we move only one particle at the time
        for (k = 0; k < number_particles; k++) {
	  if ( k != i) {
	    for ( j=0; j < dimension; j++) {
	      r_new[k][j] = r_old[k][j];
	    }
	  } 
        }
	//        wave_function_onemove(r_new, qforce_new, &wfnew, beta); 
        wfnew = wave_function(r_new, beta); 
        quantum_force(r_new, qforce_new, beta, wfnew);
\end{lstlisting} 
}
\end{small}
%  \end{block}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, new positions in function vmc\_importance.cpp}
%  \begin{block}{}
\begin{small}
{\scriptsize
\lstset{language=c++}  
\begin{lstlisting}
	//  we compute the log of the ratio of the greens functions to be used in the 
	//  Metropolis-Hastings algorithm
        greensfunction = 0.0;            
	for ( j=0; j < dimension; j++) {
	  greensfunction += 0.5*(qforce_old[i][j]+qforce_new[i][j])*
	    (D*timestep*0.5*(qforce_old[i][j]-qforce_new[i][j])-r_new[i][j]+r_old[i][j]);
        }
        greensfunction = exp(greensfunction);
\end{lstlisting} 
}
\end{small}
%  \end{block}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, new positions in function vmc\_importance.cpp}
%  \begin{block}{}
\begin{small}
{\scriptsize
\lstset{language=c++}  
\begin{lstlisting}
 	// The Metropolis test is performed by moving one particle at the time
	if(ran2(&idum) <= greensfunction*wfnew*wfnew/wfold/wfold ) { 
	  for ( j=0; j < dimension; j++) {
	    r_old[i][j] = r_new[i][j];
	    qforce_old[i][j] = qforce_new[i][j];
	  }
	  wfold = wfnew;
          .....
\end{lstlisting} 
}
\end{small}
%  \end{block}
}



\frame[containsverbatim]
{
  \frametitle{Importance sampling, Quantum force in function vmc\_importance.cpp}
%  \begin{block}{}
\begin{small}
{\scriptsize
\lstset{language=c++}  
\begin{lstlisting}
void  quantum_force(double **r, double **qforce, double beta, double wf)
{
  int i, j;
  double wfminus, wfplus; 
  double **r_plus, **r_minus;

  r_plus = (double **) matrix( number_particles, dimension, sizeof(double));
  r_minus = (double **) matrix( number_particles, dimension, sizeof(double));
  for (i = 0; i < number_particles; i++) { 
    for ( j=0; j < dimension; j++) {
      r_plus[i][j] = r_minus[i][j] = r[i][j];
    }
  }
...
\end{lstlisting} 
}
\end{small}
%  \end{block}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, Quantum force in function vmc\_importance.cpp, brute force derivative}
%  \begin{block}{}
\begin{small}
{\scriptsize
\lstset{language=c++}  
\begin{lstlisting}

  // compute the first derivative
  for (i = 0; i < number_particles; i++) {
    for (j = 0; j < dimension; j++) { 
      r_plus[i][j] = r[i][j]+h;
      r_minus[i][j] = r[i][j]-h;
      wfminus = wave_function(r_minus, beta); 
      wfplus  = wave_function(r_plus, beta); 
      qforce[i][j] = (wfplus-wfminus)*2.0/wf/(2*h);
      r_plus[i][j] = r[i][j];
      r_minus[i][j] = r[i][j];
    }
  }

} // end of quantum_force function
\end{lstlisting} 
}
\end{small}
%  \end{block}
}

\frame
{
  \frametitle{Closed form expressions for quantum force}
\begin{small}
{\scriptsize
The general derivative formula of the Jastrow factor is
\[
\frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_k} =
\sum_{i=1}^{k-1}\frac{\partial g_{ik}}{\partial x_k}
+
\sum_{i=k+1}^{N}\frac{\partial g_{ki}}{\partial x_k}
\]
However, 
with our
\[
\Psi_C=\prod_{i< j}g(r_{ij})= \exp{\left\{\sum_{i<j}\frac{ar_{ij}}{1+\beta r_{ij}}\right\}},
\]
the gradient needed for the quantum force and local energy is easy to compute.  
We get for particle $k$
\[
\frac{ \nabla_k \Psi_C}{ \Psi_C }= \sum_{j\ne k}\frac{{\bf r}_{kj}}{r_{kj}}\frac{a}{(1+\beta r_{kj})^2},
\]
which is rather easy to code.  Remember to sum over all particles  when you compute the local energy.
}
\end{small}
}

\frame
{
  \frametitle{Your tasks from the previous  week plus new tasks}
\begin{small}
{\scriptsize
\begin{itemize}
\item Implement the closed form expression for the local energy and the so-called quantum force
\item Convince yourself that the closed form expressions are correct, see slides from last week.
\item Implement the closed form expressions for systems with more than two electrons.
\item Start implementing importance sampling, part 1c, see code vmc\_importance.cpp.
\item Finish part 1a and begin part 1b.  
\item You need to produce random numbers with a Gaussian distribution.
\item Reading task: Thijssen's text chapters 8.8 and 12.2. To be discussed today. 
\item Task to next week: Finish coding importance sampling in 1c. 
\end{itemize}
}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{Going Parallel with MPI}
You will need  to parallelize the codes you develop. 

{\bf Task parallelism}: the work of a global problem can be divided
into a number of independent tasks, which rarely need to synchronize. 
Monte Carlo simulation or integrations are examples of this. It is almost embarrassingly trivial to parallelize
Monte Carlo codes.

MPI is a message-passing library where all the routines
have corresponding C/C++-binding
\begin{lstlisting}
   MPI_Command_name
\end{lstlisting}
and Fortran-binding (routine names are in uppercase, but can also be in lower case)
\begin{lstlisting}
   MPI_COMMAND_NAME
\end{lstlisting}
} 


\frame[containsverbatim]
{
  \frametitle{What is Message Passing Interface (MPI)? Yet another library!}

MPI is a library, not a language. It specifies the names, calling sequences and results of functions
or subroutines to be called from C or Fortran programs, and the classes and methods that make up the MPI C++
library. The programs that users write in Fortran, C or C++ are compiled with ordinary compilers and linked
with the MPI library.

MPI is a specification, not a particular implementation. MPI programs should be able to run
on all possible machines and run all MPI implementetations without change.

An MPI computation is a collection of processes communicating with messages.

See chapter 4.7 of lecture notes for more details.
} 

\frame[containsverbatim]
{
  \frametitle{MPI}

MPI is a library specification for the message passing interface,
proposed as a standard.
\begin{itemize}
\item independent of hardware;
\item not a language or compiler specification;
\item not a specific implementation or product.
\end{itemize}


A message passing standard for portability and ease-of-use. 
Designed for high performance.

Insert communication and synchronization functions where necessary.
} 



\frame[containsverbatim]
{
  \frametitle{Demands from the HPC community}
In the field of scientific computing, there is an ever-lasting wish
to do larger simulations using shorter computer time.

Development of the capacity for single-processor computers can
hardly keep up with the pace of scientific computing:
\begin{itemize}
\item processor speed
\item memory size/speed
\end{itemize}

Solution: parallel computing!
} 


\frame[containsverbatim]
{
  \frametitle{The basic ideas of parallel computing}
\begin{itemize}
\item Pursuit of shorter computation time and larger simulation size
gives rise to parallel computing.
\item Multiple processors are involved to solve a global problem.
\item The essence is to divide the entire computation evenly among
collaborative processors.  Divide and conquer.
\end{itemize}
} 


\frame[containsverbatim]
{
  \frametitle{A rough classification of hardware models}
\begin{itemize}
\item
Conventional single-processor computers can be called SISD
(single-instruction-single-data) machines.
\item SIMD (single-instruction-multiple-data) machines incorporate the
idea of parallel processing, which use a large number of process-
ing units to execute the same instruction on different data.
\item Modern parallel computers are so-called MIMD (multiple-instruction-
multiple-data) machines and can execute different instruction
streams in parallel on different data.
\end{itemize}
} 


\frame[containsverbatim]
{
  \frametitle{Shared memory and distributed memory}
\begin{itemize}
\item One way of categorizing modern parallel computers is to look at
the memory configuration.
\item In shared memory systems the CPUs share the same address
space. Any CPU can access any data in the global memory.
\item In distributed memory systems each CPU has its own memory.
The CPUs are connected by some network and may exchange
messages.
\end{itemize}
} 



\frame[containsverbatim]
{
  \frametitle{Different parallel programming paradigms}
\begin{itemize}
\item {\bf Task parallelism}  the work of a global problem can be divided
into a number of independent tasks, which rarely need to synchronize. 
Monte Carlo simulation is one example. Integration is another. However this
paradigm is of limited use.
\item {\bf Data parallelism}  use of multiple threads (e.g. one thread per
processor) to dissect loops over arrays etc. 
This paradigm requires a single memory address space. 
Communication and synchronization between processors are often hidden, thus easy to
program. However, the user surrenders much control to a specialized compiler.
Examples of data parallelism are compiler-based parallelization
and OpenMP directives.
\end{itemize}
} 



\frame[containsverbatim]
{
  \frametitle{Today's situation of parallel computing}
\begin{itemize}
\item Distributed memory is the dominant hardware configuration. There
is a large diversity in these machines, from 
MPP (massively parallel processing) systems to clusters of off-the-shelf PCs, which
are very cost-effective.
\item Message-passing is a mature programming paradigm and widely
accepted. It often provides an efficient match to the hardware.
It is primarily used for the distributed memory systems, but can also
be used on shared memory systems.
\end{itemize}
In these lectures we consider only message-passing for 
writing parallel programs.
} 



\frame[containsverbatim]
{
  \frametitle{Overhead present in parallel computing}
\begin{itemize}
\item {\bf Uneven load balance}:  not all the processors can perform useful
work at all time.
\item {\bf Overhead of synchronization.}
\item {\bf Overhead of communication}.
\item {Extra computation due to parallelization}.
\end{itemize}
Due to the above overhead and that certain part of a sequential
algorithm cannot be parallelized we may not achieve an optimal parallelization.
} 


\frame[containsverbatim]
{
  \frametitle{Parallelizing a sequential algorithm}
\begin{itemize}
\item Identify the part(s) of a sequential algorithm that can be 
executed in parallel. This is the difficult part,
\item Distribute the global work and data among $P$ processors.
\end{itemize}
} 




\frame[containsverbatim]
{
  \frametitle{Process and processor}
\begin{itemize}
\item We refer to process as a logical unit which executes its own code,
in an MIMD style.
\item The processor is a physical device on which one or several processes
are executed.
\item The MPI standard uses the concept process consistently throughout 
its documentation.
\end{itemize}
} 



\section[Week 7]{Week 7}
\frame
{
  \frametitle{Topics for Week 7, February 14-18}
  \begin{block}{Importance sampling, Fokker-Planck and Langevin equations and parallelization}
\begin{itemize}
\item Repetition from last week
\item MPI programming.
\item Structuring the code and performing benchmarks
\item Derivation of the Fokker-Planck and the Langevin equations (Background material).
\end{itemize}
Project work this week: Try to finalize importance sampling in exercise 1c.
We start with blocking next week.
  \end{block}
} 



\frame[containsverbatim]
{
  \frametitle{Bindings to MPI routines}

MPI is a message-passing library where all the routines
have corresponding C/C++-binding
\begin{lstlisting}
   MPI_Command_name
\end{lstlisting}
and Fortran-binding (routine names are in uppercase, but can also be in lower case)
\begin{lstlisting}
   MPI_COMMAND_NAME
\end{lstlisting}
The discussion in these slides focuses on the C++ binding.
} 


\frame[containsverbatim]
{
  \frametitle{Communicator}
\begin{itemize}
\item A group of MPI processes with a name (context).
\item Any process is identified by its rank. The rank is only meaningful
within a particular communicator.
\item By default communicator MPI\_COMM\_WORLD contains all the MPI
processes.
\item Mechanism to identify subset of processes.
\item Promotes modular design of parallel libraries.
\end{itemize}
} 






\frame[containsverbatim]
{
  \frametitle{Some of the  most important MPI routines}
\begin{itemize}
\item MPI\_ Init - initiate an MPI computation
\item MPI\_Finalize - terminate the MPI computation and clean up
\item MPI\_Comm\_size - how many processes participate in a given MPI
communicator?
\item MPI\_Comm\_rank - which one am I? (A number between 0 and size-1.)
\item MPI\_Send - send a message to a particular process within an MPI
communicator
\item MPI\_Recv - receive a message from a particular process within an
MPI communicator
\end{itemize}
} 


\frame[containsverbatim]
{
  \frametitle{The first MPI C/C++ program}
Let every process write "Hello world" on the standard output.  This is program2.cpp of chapter 4.
\begin{lstlisting}
using namespace std;
#include <mpi.h>
#include <iostream>
int main (int nargs, char* args[])
{
int numprocs, my_rank;
//   MPI initializations
MPI_Init (&nargs, &args);
MPI_Comm_size (MPI_COMM_WORLD, &numprocs);
MPI_Comm_rank (MPI_COMM_WORLD, &my_rank);
cout << "Hello world, I have  rank " << my_rank << " out of " 
     << numprocs << endl;
//  End MPI
MPI_Finalize ();
\end{lstlisting}
} 





\frame[containsverbatim]
{
  \frametitle{The Fortran program}
\begin{lstlisting}
PROGRAM hello
INCLUDE "mpif.h"
INTEGER:: size, my_rank, ierr

CALL  MPI_INIT(ierr)
CALL MPI_COMM_SIZE(MPI_COMM_WORLD, size, ierr)
CALL MPI_COMM_RANK(MPI_COMM_WORLD, my_rank, ierr)
WRITE(*,*)"Hello world, I've rank ",my_rank," out of ",size
CALL MPI_FINALIZE(ierr)

END PROGRAM hello
\end{lstlisting}
} 

\frame[containsverbatim]
{
  \frametitle{Note 1}
The output to screen is not ordered since all processes are trying to write  to screen simultaneously.
It is then the operating system which opts for an ordering.  
If we wish to have an organized output, starting from the first process, we may rewrite our program as in the next example
(program3.cpp), see again chapter 4.7 of lecture notes.
} 



\frame[containsverbatim]
{
  \frametitle{Ordered output with MPI\_Barrier}
\begin{lstlisting}
int main (int nargs, char* args[])
{
 int numprocs, my_rank, i;
 MPI_Init (&nargs, &args);
 MPI_Comm_size (MPI_COMM_WORLD, &numprocs);
 MPI_Comm_rank (MPI_COMM_WORLD, &my_rank);
 for (i = 0; i < numprocs; i++) {}
 MPI_Barrier (MPI_COMM_WORLD);
 if (i == my_rank) {
 cout << "Hello world, I have  rank " << my_rank << 
        " out of " << numprocs << endl;}
      MPI_Finalize ();
\end{lstlisting}
} 


\frame[containsverbatim]
{
  \frametitle{Note 2}
Here we have used the $MPI\_Barrier$ function to ensure that
that every process has completed  its set of instructions in  a particular order.
A barrier is a special collective operation that does not allow the processes to continue
until all processes in the communicator (here $MPI\_COMM\_WORLD$ have called 
$MPI\_Barrier$. 
The barriers make sure that all processes have reached the same point in the code. Many of the collective operations
like $MPI\_ALLREDUCE$ to be discussed later, have the same property; viz.~no process can exit the operation
until all processes have started. 
However, this is slightly more time-consuming since the processes synchronize between themselves as many times as there
are processes.  In the next Hello world example we use the send and receive functions in order to a have a synchronized
action.
} 


\frame
{
  \frametitle{Strategies}
\begin{itemize}
\item Develop codes locally, run with some few processes and test your codes.  Do benchmarking, timing
and so forth on local nodes, for example your laptop.  You can install MPICH2 on your laptop
(most new laptos come with dual cores).  You can test with one node at the lab. 
\item When you are convinced that your codes run correctly, you start your production runs on 
available supercomputers, in our case titan.uio.no.
\end{itemize}
} 


\frame[containsverbatim]
{
  \frametitle{How do I run MPI on the machines at the lab (MPICH2)}
The machines at the lab are all quad-cores
\begin{itemize}
\item Compile with mpicxx or mpic++
\item Set up collaboration between processes and run 
\begin{lstlisting}
mpd --ncpus=4 &
#  run code with
mpiexec -n 4 ./nameofprog
\end{lstlisting}
Here we declare that we will use 4 processes via the $-ncpus$ option
and via $-n 4 $ when running.
\item End with
\begin{lstlisting}
mpdallexit
\end{lstlisting} 
\end{itemize}
} 



\frame[containsverbatim]
{
  \frametitle{Can I do it on my own PC/laptop?}
Of course:
\begin{itemize}
\item go to \url{http://www.mcs.anl.gov/research/projects/mpich2/}
\item follow the instructions and install it on your own PC/laptop
\end{itemize}
I don't have windows as operating system and need dearly your feedback. 

} 





\frame[containsverbatim]
{
  \frametitle{Ordered output with MPI\_Recv and MPI\_Send}
\begin{lstlisting}
.....
int numprocs, my_rank, flag;
MPI_Status status;
MPI_Init (&nargs, &args);
MPI_Comm_size (MPI_COMM_WORLD, &numprocs);
MPI_Comm_rank (MPI_COMM_WORLD, &my_rank);
if (my_rank > 0)
MPI_Recv (&flag, 1, MPI_INT, my_rank-1, 100, 
           MPI_COMM_WORLD, &status);
cout << "Hello world, I have  rank " << my_rank << " out of " 
<< numprocs << endl;
if (my_rank < numprocs-1)
MPI_Send (&my_rank, 1, MPI_INT, my_rank+1, 
          100, MPI_COMM_WORLD);
MPI_Finalize ();
\end{lstlisting}
} 

\frame[containsverbatim]
{
  \frametitle{Note 3}
The basic sending of messages is given by the function $MPI\_SEND$, which in C/C++
is defined as 
\begin{lstlisting}
int MPI_Send(void *buf, int count, 
             MPI_Datatype datatype, 
             int dest, int tag, MPI_Comm comm)}
\end{lstlisting}
This single command allows the passing of any kind of variable, even a large array, to any group of tasks. 
The variable {\bf buf} is the variable we wish to send while {\bf count} 
is the  number of variables we are passing. If we are passing only a single value, this should be 1. 
If we transfer an array, it is  the overall size of the array. 
For example, if we want to send a 10 by 10 array, count would be $10\times 10=100$ 
since we are  actually passing 100 values.  
}

\frame[containsverbatim]
{
  \frametitle{Note 4}
Once you have  sent a message, you must receive it on another task. The function {\bf MPI\_RECV}
is similar to the send call.
\begin{lstlisting}
int MPI_Recv( void *buf, int count, MPI_Datatype datatype, 
            int source, 
            int tag, MPI_Comm comm, MPI_Status *status )
\end{lstlisting}

The arguments that are different from those in $MPI\_SEND$ are
{\bf buf} which  is the name of the variable where you will  be storing the received data, 
{\bf source} which  replaces the destination in the send command. This is the return ID of the sender.

Finally,  we have used  {\bf MPI\_Status\ status;} 
where one can check if the receive was completed.

The output of this code is the same as the previous example, but now
process 0 sends a message to process 1, which forwards it further
to process 2, and so forth.

Armed with this wisdom, performed all hello world greetings, we are now ready for serious work. 
}


\frame
{
  \frametitle{Integrating $\pi$}
\begin{columns}
\column{5.5cm}
%\begin{center}
\begin{pgfpicture}{-2.25cm}{0.5cm}{5cm}{0.5cm}
   {\pgfbox[center,center]{\pgfuseimage{pi}}}
\end{pgfpicture}
\column{4.5cm}
  \begin{block}{Examples}
\begin{itemize}
\item Go to the webpage and click on the programs link 
\item Go to MPI and then chapter 4
\item Look at program5.ccp and program6.cpp. (Fortran version also available).
\item These codes compute $\pi$ using the rectangular and trapezoidal rules.
\end{itemize}
  \end{block}
\end{columns}
} 



\frame
{
  \frametitle{Integration algos}
 \begin{small}
 {\scriptsize
The trapezoidal rule (example6.cpp)
\[
   I=\int_a^bf(x) dx=h\left(f(a)/2 + f(a+h) +f(a+2h)+
                          \dots +f(b-h)+ f_{b}/2\right).
\]

Another very simple approach is the so-called midpoint or rectangle method.
In this case the integration area is split in a given number of rectangles with length $h$ and
heigth given by the mid-point value of the function.  This gives the following simple rule for
approximating an integral
\[
   I=\int_a^bf(x) dx \approx  h\sum_{i=1}^N f(x_{i-1/2}), 
\]
where $f(x_{i-1/2})$ is the midpoint value of $f$ for a given rectangle.  This is used in program5.cpp.
}
\end{small}
} 


\frame[containsverbatim]
{
  \frametitle{Dissection of example program5.cpp}
 \begin{small}
 {\scriptsize
\begin{lstlisting}
1   //    Reactangle rule and numerical integration
2   using namespace std;
3   #include <mpi.h>
4   #include <iostream>

5   int main (int nargs, char* args[])
6   {
7      int numprocs, my_rank, i, n = 1000;
8      double local_sum, rectangle_sum, x, h;
9      //   MPI initializations
10     MPI_Init (&nargs, &args);
11     MPI_Comm_size (MPI_COMM_WORLD, &numprocs);
12     MPI_Comm_rank (MPI_COMM_WORLD, &my_rank);
\end{lstlisting}
 }
 \end{small}
} 








\frame[containsverbatim]
{
  \frametitle{Dissection of example program5.cpp}
 \begin{small}
 {\scriptsize
\begin{lstlisting}
13     //   Read from screen a possible new vaue of n
14     if (my_rank == 0 && nargs > 1) {
15        n = atoi(args[1]);
16     }
17     h = 1.0/n;
18     //  Broadcast n and h to all processes
19     MPI_Bcast (&n, 1, MPI_INT, 0, MPI_COMM_WORLD);
20     MPI_Bcast (&h, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);
21     //  Every process sets up its contribution to the integral
22     local_sum = 0.;
\end{lstlisting}
 }
 \end{small}
} 

\frame[containsverbatim]
{
  \frametitle{Dissection of example program5.cpp}
 \begin{small}
 {\scriptsize
After the standard initializations with MPI such as MPI\_Init, MPI\_Comm\_size
and MPI\_Comm\_rank, MPI\_COMM\_WORLD contains now the number of processes
defined  by using for example 
\begin{lstlisting}
mpiexec -np 10 ./prog.x
\end{lstlisting}
In line 4 we check if
we have read in from screen the number of mesh points  $n$. Note that in line 7 we fix $n=1000$, however
we have the possibility to run the code with a different number of mesh points as well.
If \lstinline{my_rank} equals zero, which correponds to the master node, then we read a new value of
$n$  if the number of arguments is larger than two. This can be done as follows when we run the code
\begin{lstlisting}
mpiexec -np 10 ./prog.x  10000
\end{lstlisting}
 }
 \end{small}
} 


\frame[containsverbatim]
{
  \frametitle{Dissection of example program5.cpp}
 \begin{small}
 {\scriptsize
\begin{lstlisting}
23     for (i = my_rank; i < n; i += numprocs) {
24       x = (i+0.5)*h;
25       local_sum += 4.0/(1.0+x*x);
26    }
27     local_sum *= h;
\end{lstlisting}
In line 17 we define also the step length $h$.
In lines 19 and 20 we use the broadcast function \lstinline{MPI_Bcast}.
We use this particular function because we want data on one processor (our master node) to be shared
with all other processors. The broadcast function sends data to a group of processes. 
 }
 \end{small}
} 





\frame[containsverbatim]
{
  \frametitle{Dissection of example program5.cpp}
 \begin{small}
 {\scriptsize
The MPI routine MPI\_Bcast transfers data from one task to a group of others. 
The format for the call
is in C++ given by the parameters of 
\begin{lstlisting}
MPI_Bcast (&n, 1, MPI_INT, 0, MPI_COMM_WORLD);.
MPI_Bcast (&h, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);
\end{lstlisting}
 in a case of a double.
The general structure of this function is 
\begin{lstlisting}
MPI_Bcast( void *buf, int count, MPI_Datatype datatype, int root, MPI_Comm comm).
\end{lstlisting}
All processes call this function, both the process sending the data (with rank zero) and all the other
processes in MPI\_COMM\_WORLD.  
Every process has now  copies of $n$ and $h$, the number of mesh points and the step length, respectively.

We transfer the addresses of $n$ and $h$.  The second argument represents the number of data sent. In case of 
a one-dimensional array, one needs to transfer the number of array elements. 
If you have an $n\times m$ matrix, you must transfer $n\times m$. We need also to specify whether the variable
type we transfer is a non-numerical such as a logical or character variable or numerical of the integer,
real or complex type. 
 }
 \end{small}
} 



\frame[containsverbatim]
{
  \frametitle{Dissection of example program5.cpp}
 \begin{small}
 {\scriptsize
\begin{lstlisting}
28     if (my_rank == 0) {
29       MPI_Status status;
30       rectangle_sum = local_sum;
31       for (i=1; i < numprocs; i++) {
32         MPI_Recv(&local_sum,1,MPI_DOUBLE,MPI_ANY_SOURCE,500,
                    MPI_COMM_WORLD,&status);
33         rectangle_sum += local_sum;
34       }
35       cout << "Result: " << rectangle_sum  << endl;
36     }  else
37       MPI_Send(&local_sum,1,MPI_DOUBLE,0,500,MPI_COMM_WORLD);
38     // End MPI
39     MPI_Finalize ();
40     return 0;
41   }
\end{lstlisting}

 }
 \end{small}
} 







\frame[containsverbatim]
{
  \frametitle{Dissection of example program5.cpp}
 \begin{small}
 {\scriptsize
In lines  23-27, every process sums its own part of the final sum used by the rectangle rule. The receive statement collects
the sums from all other processes in case \lstinline{my_rank == 0}, else an MPI send is performed.
If we are not the master node, we send the results, else they are received and the local results are added 
to final sum.   The above can be rewritten using the MPI\_allreduce, as discussed in the next example. 


The above function is not very elegant. Furthermore, the MPI instructions can be simplified by using the
functions MPI\_Reduce or MPI\_Allreduce.
The first function takes information from all processes and sends the result of the MPI operation to one process only,
typically the master node.  If we use MPI\_Allreduce, the result is sent back to all processes, a feature which is
useful when all nodes need the value of a joint operation.  We limit ourselves to MPI\_Reduce since it is only one 
process which will print out the final number of our calculation, The arguments to MPI\_Allreduce are the same.  
 }
 \end{small}
} 



\frame[containsverbatim]
{
  \frametitle{MPI\_reduce}
 \begin{small}
 {\scriptsize
Call as
\begin{lstlisting}
MPI_reduce( void *senddata, void* resultdata, int count, 
     MPI_Datatype datatype, MPI_Op, int root, MPI_Comm comm)
\end{lstlisting}

The two variables $senddata$ and $resultdata$ are obvious, besides the fact that one sends the address
of the variable or the first element of an array.  If they are arrays they need to have the same size. 
The variable $count$ represents the total dimensionality, 1 in case of just one variable, 
while MPI\_Datatype 
defines the type of variable which is sent and received.  

The new feature is MPI\_Op. It defines the type
of operation we want to do. 
In our case, since we are summing
the rectangle  contributions from every process we define  MPI\_Op = MPI\_SUM.
If we have an array or matrix we can search for the largest og smallest element by sending either MPI\_MAX or 
MPI\_MIN.  If we want the location as well (which array element) we simply transfer 
MPI\_MAXLOC or MPI\_MINOC. If we want the product we write MPI\_PROD. 

MPI\_Allreduce is defined as
\begin{lstlisting}     
MPI_Alreduce( void *senddata, void* resultdata, int count, 
          MPI_Datatype datatype, MPI_Op, MPI_Comm comm)}.        
\end{lstlisting} 
}
 \end{small}
} 





\frame[containsverbatim]
{
  \frametitle{Dissection of example program6.cpp}
 \begin{small}
 {\scriptsize
\begin{lstlisting}
//    Trapezoidal rule and numerical integration usign MPI, example program6.cpp
using namespace std;
#include <mpi.h>
#include <iostream>

//     Here we define various functions called by the main program

double int_function(double );
double trapezoidal_rule(double , double , int , double (*)(double));

//   Main function begins here
int main (int nargs, char* args[])
{
  int n, local_n, numprocs, my_rank; 
  double a, b, h, local_a, local_b, total_sum, local_sum;   
  double  time_start, time_end, total_time;
\end{lstlisting}
 }
 \end{small}
} 



\frame[containsverbatim]
{
  \frametitle{Dissection of example program6.cpp}
 \begin{small}
 {\scriptsize
\begin{lstlisting}
  //  MPI initializations
  MPI_Init (&nargs, &args);
  MPI_Comm_size (MPI_COMM_WORLD, &numprocs);
  MPI_Comm_rank (MPI_COMM_WORLD, &my_rank);
  time_start = MPI_Wtime();
  //  Fixed values for a, b and n 
  a = 0.0 ; b = 1.0;  n = 1000;
  h = (b-a)/n;    // h is the same for all processes 
  local_n = n/numprocs;  
  // make sure n > numprocs, else integer division gives zero
  // Length of each process' interval of
  // integration = local_n*h.  
  local_a = a + my_rank*local_n*h;
  local_b = local_a + local_n*h;
\end{lstlisting}
 }
 \end{small}
} 



\frame[containsverbatim]
{
  \frametitle{Dissection of example program6.cpp}
 \begin{small}
 {\scriptsize
\begin{lstlisting}
  total_sum = 0.0;
  local_sum = trapezoidal_rule(local_a, local_b, local_n, 
                               &int_function); 
  MPI_Reduce(&local_sum, &total_sum, 1, MPI_DOUBLE, 
              MPI_SUM, 0, MPI_COMM_WORLD);
  time_end = MPI_Wtime();
  total_time = time_end-time_start;
  if ( my_rank == 0) {
    cout << "Trapezoidal rule = " <<  total_sum << endl;
    cout << "Time = " <<  total_time  
         << " on number of processors: "  << numprocs  << endl;
  }
  // End MPI
  MPI_Finalize ();  
  return 0;
}  // end of main program
\end{lstlisting}

 }
 \end{small}
} 



\frame[containsverbatim]
{
  \frametitle{Dissection of example program6.cpp}
 \begin{small}
 {\scriptsize
We use MPI\_reduce to collect data from each process. Note also the use of the function 
MPI\_Wtime. The final functions are
\begin{lstlisting}
//  this function defines the function to integrate
double int_function(double x)
{
  double value = 4./(1.+x*x);
  return value;
} // end of function to evaluate

\end{lstlisting}
 }
 \end{small}
} 


\frame[containsverbatim]
{
  \frametitle{Dissection of example program6.cpp}
 \begin{small}
 {\scriptsize
Implementation of the trapezoidal rule.
\begin{lstlisting}
//  this function defines the trapezoidal rule
double trapezoidal_rule(double a, double b, int n, 
                         double (*func)(double))
{
  double trapez_sum;
  double fa, fb, x, step;
  int    j;
  step=(b-a)/((double) n);
  fa=(*func)(a)/2. ;
  fb=(*func)(b)/2. ;
  trapez_sum=0.;
  for (j=1; j <= n-1; j++){
    x=j*step+a;
    trapez_sum+=(*func)(x);
  }
  trapez_sum=(trapez_sum+fb+fa)*step;
  return trapez_sum;
}  // end trapezoidal_rule 
\end{lstlisting}
 }
 \end{small}
} 






\frame{
  \frametitle{How do I use the titan.uio.no cluster?}
  \begin{block}{hpc@usit.uio.no}
    \begin{itemize}
    \item Computational Physics requires High Performance Computing
      (HPC) resources
    \item USIT and the Research Computing Services (RCS) provides
      HPC resources and HPC support
    \item Resources: \url{titan.uio.no}
    \item Support: 14 people
    \item Contact: \url{hpc@usit.uio.no}
    \end{itemize}
  \end{block}
}


\frame{
  \frametitle{Titan}
  \begin{block}{Hardware}
    \begin{itemize}
    \item 304 dual-cpu quad-core SUN X2200 Opteron nodes (total 2432 cores), 2.2 Ghz, and 8 - 16 GB RAM and 250 - 1000 GB disk on each node
    \item 3 eight-cpu quad-core Sun X4600 AMD Opteron nodes (total 96 cores), 2.5 Ghz, and 128, 128 and 256 GB memory, respectively
    \item Infiniband interconnect
    \item Heterogenous cluster!
    \end{itemize}
  \end{block}
}

\frame{
  \frametitle{Titan}
  \begin{block}{Software}
    \begin{itemize}
      \item Batch system: SLURM and MAUI
      \item Message Passing Interface (MPI):
        \begin{itemize}
        \item OpenMPI
        \item Scampi
        \item MPICH2
        \end{itemize}
      \item Compilers: GCC, Intel, Portland and Pathscale
      \item Optimized math libraries and scientific applications
      \item All you need may be found under \texttt{/site}
      \item Available software: \url{http://www.hpc.uio.no/index.php/Titan_software}
    \end{itemize}
  \end{block}
}

\frame{
  \frametitle{Getting started}
  \begin{block}{Batch systems}
    \begin{itemize}
    \item A batch system controls the use of the cluster resources
    \item Submits the job to the right resource
    \item Monitors the job while executing
    \item Restarts the job in case of failure
    \item Takes care of priorities and queues to control execution
      order of unrelated jobs
    \end{itemize}
  \end{block}
  \begin{block}{Sun Grid Engine}
    \begin{itemize}
    \item SGE is the batch system used on Titan
    \item Jobs are executed either interactively or through job scripts
    \item Useful commands: \texttt{showq}, \texttt{qlogin}, \texttt{sbatch}
    \item \url{http://hpc.uio.no/index.php/Titan_User_Guide}
    \end{itemize}
  \end{block}
}

\frame{
  \frametitle{Getting started}
  \begin{block}{Modules}
    \begin{itemize}
    \item Different compilers, MPI-versions and applications need
      different sets of user environment variables
    \item The \texttt{modules} package lets you load and remove the
      different variable sets
    \item Useful commands:
      \begin{itemize}
      \item List available modules: \texttt{module avail}
      \item Load module: \texttt{module load <environment>}
      \item Unload module: \texttt{module unload <environment>}
      \item Currently loaded: \texttt{module list}
      \end{itemize}
    \item \url{http://hpc.uio.no/index.php/Titan_User_Guide}
    \end{itemize}
  \end{block}
}

\lstset{basicstyle=\tiny}
\frame{
  \frametitle{Example}
  \begin{block}{Interactively}
    \lstinputlisting{interactive.sh}
  \end{block}
}
\frame{
  \frametitle{The job script}
  \begin{block}{job.sge}
    \lstinputlisting{job.sge}
  \end{block}
}

\lstset{basicstyle=\small}

\frame{
  \frametitle{Example}
  \begin{block}{Submitting}
    \lstinputlisting{submitting.sh}
  \end{block}
}

\lstset{basicstyle=\tiny}
\frame{
  \frametitle{Example}
  \begin{block}{Checking execution}
    \lstinputlisting{checking.sh}
  \end{block}
}

\lstset{basicstyle=\small}

\frame{
  \frametitle{Tips and admonitions}
  \begin{block}{Tips}
    \begin{itemize}
    \item Titan FAQ:
      \url{http://www.hpc.uio.no/index.php/FAQ}
    \item man-pages, e.g. \texttt{man sbatch}
    \item Ask us
    \end{itemize}
  \end{block}
  \begin{block}{Admonitions}
    \begin{itemize}
    \item Remember to exit from qlogin-sessions; the resource is
      reserved for you untill you exit
    \item Don't run jobs on login-nodes; these are only for compiling
      and editing files
    \end{itemize}
  \end{block}
}





\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equation, discussed only if time}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
A stochastic process is simply a function of two variables, one is the time,
the other is a stochastic variable $X$, defined by specifying
\begin{itemize}
   \item the set $\left\{x\right\}$ of possible values for $X$;
   \item  the probability distribution, $w_X(x)$, 
over this set, or briefly $w(x)$
\end{itemize}
The set of values $\left\{x\right\}$ for $X$ 
may be discrete, or continuous. If the set of
values is continuous, then $w_X (x)$ is a probability density so that 
$w_X (x)dx$
is the probability that one finds the stochastic variable $X$ to have values
in the range $[x, x + dx]$ .
}
\end{small}
%\end{block}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equation}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
     An arbitrary number of other stochastic variables may be derived from
$X$. For example, any $Y$ given by a mapping of $X$, is also a stochastic
variable. The mapping may also be time-dependent, that is, the mapping
depends on an additional variable $t$
\[
                              Y_X (t) = f (X, t) .
\]
The quantity $Y_X (t)$ is called a random function, or, since $t$ often is time,
a stochastic process. A stochastic process is a function of two variables,
one is the time, the other is a stochastic variable $X$. Let $x$ be one of the
possible values of $X$ then\[
                               y(t) = f (x, t),\]
is a function of $t$, called a sample function or realization of the process.
In physics one considers the stochastic process to be an ensemble of such
sample functions.
}
\end{small}
  %\end{block}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equation}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
     For many physical systems initial distributions of a stochastic 
variable $y$ tend to equilibrium distributions: $w(y, t)\rightarrow w_0(y)$ 
as $t\rightarrow\infty$. In
equilibrium detailed balance constrains the transition rates
\[
     W(y\rightarrow y')w(y ) = W(y'\rightarrow y)w_0 (y),
\]
where $W(y'\rightarrow y)$ 
is the probability, per unit time, that the system changes
from a state $|y\rangle$ , characterized by the value $y$ 
for the stochastic variable $Y$ , to a state $|y'\rangle$.

Note that for a system in equilibrium the transition rate 
$W(y'\rightarrow y)$ and
the reverse $W(y\rightarrow y')$ may be very different. 
}
\end{small}
  %\end{block}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equation}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
Consider, for instance, a simple
system that has only two energy levels $\epsilon_0 = 0$ and 
$\epsilon_1 = \Delta E$. 

For a system governed by the Boltzmann distribution we find (the partition function has been taken out)
\[
     W(0\rightarrow 1)\exp{-\epsilon_0/kT} = W(1\rightarrow 0)\exp{-\epsilon_1/kT}
\]
We get then
\[
     \frac{W(1\rightarrow 0)}{W(0 \rightarrow 1)}=\exp{-\Delta E/kT},
\]
which goes to zero when $T$ tends to zero.
}
\end{small}
  %\end{block}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equation}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
If we assume a discrete set events,
our initial probability
distribution function can be  given by 
\[
   w_i(0) = \delta_{i,0},
\]
and its time-development after a given time step $\Delta t=\epsilon$ is
\[ 
   w_i(t) = \sum_{j}W(j\rightarrow i)w_j(t=0).
\]   
The continuous analog to $w_i(0)$ is
\be
   w({\bf x})\rightarrow \delta({\bf x}),
\ee
where we now have generalized the one-dimensional position $x$ to a generic-dimensional  
vector ${\bf x}$. The Kroenecker $\delta$ function is replaced by the $\delta$ distribution
function $\delta({\bf x})$ at  $t=0$.  
}
\end{small}
  %\end{block}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equation}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
The transition from a state $j$ to a state $i$ is now replaced by a transition
to a state with position ${\bf y}$ from a state with position ${\bf x}$. 
The discrete sum of transition probabilities can then be replaced by an integral
and we obtain the new distribution at a time $t+\Delta t$ as 
\be
   w({\bf y},t+\Delta t)= \int W({\bf y},t+\Delta t| {\bf x},t)w({\bf x},t)d{\bf x},
\ee
and after $m$ time steps we have
\be
   w({\bf y},t+m\Delta t)= \int W({\bf y},t+m\Delta t| {\bf x},t)w({\bf x},t)d{\bf x}.
\ee
When equilibrium is reached we have
\be
   w({\bf y})= \int W({\bf y}|{\bf x}, t)w({\bf x})d{\bf x},
\ee
that is no time-dependence. Note our change of notation for $W$
}
\end{small}
  %\end{block}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equation}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
We can solve the equation for $w({\bf y},t)$ by making a Fourier transform to
momentum space. 
The PDF $w({\bf x},t)$ is related to its Fourier transform
$\tilde{w}({\bf k},t)$ through
\be\label{eq:fouriertransform}
   w({\bf x},t) = \int_{-\infty}^{\infty}d{\bf k} \exp{(i{\bf kx})}\tilde{w}({\bf k},t),
\ee
and using the definition of the 
$\delta$-function 
\be
   \delta({\bf x}) = \frac{1}{2\pi} \int_{-\infty}^{\infty}d{\bf k} \exp{(i{\bf kx})},
\ee
 we see that
\be
   \tilde{w}({\bf k},0)=1/2\pi.
\ee
}
\end{small}
  %\end{block}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equation}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
We can then use the Fourier-transformed diffusion equation 
\begin{equation}
    \frac{\partial \tilde{w}({\bf k},t)}{\partial t} = -D{\bf k}^2\tilde{w}({\bf k},t),
\end{equation}
with the obvious solution
\begin{equation}
   \tilde{w}({\bf k},t)=\tilde{w}({\bf k},0)\exp{\left[-(D{\bf k}^2t)\right)}=
    \frac{1}{2\pi}\exp{\left[-(D{\bf k}^2t)\right]}. 
\end{equation}
}
\end{small}
  %\end{block}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equation}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
Using Eq.~(\ref{eq:fouriertransform}) we obtain 
\begin{equation}\label{eq:finalw}
   w({\bf x},t)=\int_{-\infty}^{\infty}d{\bf k} \exp{\left[i{\bf kx}\right]}\frac{1}{2\pi}\exp{\left[-(D{\bf k}^2t)\right]}=
    \frac{1}{\sqrt{4\pi Dt}}\exp{\left[-({\bf x}^2/4Dt)\right]}, 
\end{equation}
with the normalization condition
\be
   \int_{-\infty}^{\infty}w({\bf x},t)d{\bf x}=1.
\ee
}
\end{small}
  %\end{block}
}
\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equation}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
It is rather easy to verify by insertion that Eq.~(\ref{eq:finalw}) is a solution
of the diffusion equation. The solution represents the probability of finding
our random walker at position ${\bf x}$ at time $t$ if the initial distribution 
was placed at ${\bf x}=0$ at $t=0$. 

There is another interesting feature worth observing. The discrete transition probability $W$
itself is given by a binomial distribution.
The results from the central limit theorem state that 
transition probability in the limit $n\rightarrow \infty$ converges to the normal 
distribution. It is then possible to show that
\be 
    W(il-jl,n\epsilon)\rightarrow W({\bf y},t+\Delta t|{\bf x},t)=
    \frac{1}{\sqrt{4\pi D\Delta t}}\exp{\left[-(({\bf y}-{\bf x})^2/4D\Delta t)\right]},
\ee
and that it satisfies the normalization condition and is itself a solution
to the diffusion equation.
}
\end{small}
  %\end{block}
}
\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equation}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
Let us now assume that we have three PDFs for times $t_0 < t' < t$, that is
$w({\bf x}_0,t_0)$, $w({\bf x}',t')$ and $w({\bf x},t)$.
We have then  
\[
   w({\bf x},t)= \int_{-\infty}^{\infty} W({\bf x}.t|{\bf x}'.t')w({\bf x}',t')d{\bf x}',
\]
and
\[
   w({\bf x},t)= \int_{-\infty}^{\infty} W({\bf x}.t|{\bf x}_0.t_0)w({\bf x}_0,t_0)d{\bf x}_0,
\]
and
\[
   w({\bf x}',t')= \int_{-\infty}^{\infty} W({\bf x}'.t'|{\bf x}_0,t_0)w({\bf x}_0,t_0)d{\bf x}_0.
\]
}
\end{small}
  %\end{block}
}
\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equation}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
We can combine these equations and arrive at the famous Einstein-Smoluchenski-Kolmogorov-Chapman (ESKC) relation
\[
 W({\bf x}t|{\bf x}_0t_0)  = \int_{-\infty}^{\infty} W({\bf x},t|{\bf x}',t')W({\bf x}',t'|{\bf x}_0,t_0)d{\bf x}'.
\]
We can replace the spatial dependence with a dependence upon say the velocity
(or momentum), that is we have
\[
 W({\bf v},t|{\bf v}_0,t_0)  = \int_{-\infty}^{\infty} W({\bf v},t|{\bf v}',t')W({\bf v}',t'|{\bf v}_0,t_0)d{\bf x}'.
\]
}
\end{small}
  %\end{block}
}

\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equation}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
We will now derive the Fokker-Planck equation. 
We start from the ESKC equation
\[
 W({\bf x},t|{\bf x}_0,t_0)  = \int_{-\infty}^{\infty} W({\bf x},t|{\bf x}',t')W({\bf x}',t'|{\bf x}_0,t_0)d{\bf x}'.
\]
Define $s=t'-t_0$, $\tau=t-t'$ and $t-t_0=s+\tau$. We have then
\[
 W({\bf x},s+\tau|{\bf x}_0)  = \int_{-\infty}^{\infty} W({\bf x},\tau|{\bf x}')W({\bf x}',s|{\bf x}_0)d{\bf x}'.
\]
}
\end{small}
  %\end{block}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equation}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
Assume now that $\tau$ is very small so that we can make an expansion in terms of a small step $xi$, with ${\bf x}'={\bf x}-\xi$, that is
\[
 W({\bf x},s|{\bf x}_0)+\frac{\partial W}{\partial s}\tau +O(\tau^2) = \int_{-\infty}^{\infty} W({\bf x},\tau|{\bf x}-\xi)W({\bf x}-\xi,s|{\bf x}_0)d{\bf x}'.
\]
We assume that $W({\bf x},\tau|{\bf x}-\xi)$ takes non-negligible values only when $\xi$ is small. This is just another way of stating the Master equation!!
}
\end{small}
  %\end{block}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equation}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
We say thus that ${\bf x}$ changes only by a small amount in the time interval $\tau$. 
This means that we can make a Taylor expansion in terms of $\xi$, that is we
expand
\[
W({\bf x},\tau|{\bf x}-\xi)W({\bf x}-\xi,s|{\bf x}_0) =
\sum_{n=0}^{\infty}\frac{(-\xi)^n}{n!}\frac{\partial^n}{\partial x^n}\left[W({\bf x}+\xi,\tau|{\bf x})W({\bf x},s|{\bf x}_0)
\right].
\]
We can then rewrite the ESKC equation as 
\[
\frac{\partial W}{\partial s}\tau=-W({\bf x},s|{\bf x}_0)+
\sum_{n=0}^{\infty}\frac{(-\xi)^n}{n!}\frac{\partial^n}{\partial x^n}
\left[W({\bf x},s|{\bf x}_0)\int_{-\infty}^{\infty} \xi^nW({\bf x}+\xi,\tau|{\bf x})d\xi\right].
\]
We have neglected higher powers of $\tau$ and have used that for $n=0$ 
we get simply $W({\bf x},s|{\bf x}_0)$ due to normalization.
}
\end{small}
  %\end{block}
}

\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equation}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
We say thus that ${\bf x}$ changes only by a small amount in the time interval $\tau$. 
This means that we can make a Taylor expansion in terms of $\xi$, that is we
expand
\[
W({\bf x},\tau|{\bf x}-\xi)W({\bf x}-\xi,s|{\bf x}_0) =
\sum_{n=0}^{\infty}\frac{(-\xi)^n}{n!}\frac{\partial^n}{\partial x^n}\left[W({\bf x}+\xi,\tau|{\bf x})W({\bf x},s|{\bf x}_0)
\right].
\]
We can then rewrite the ESKC equation as 
\[
\frac{\partial W({\bf x},s|{\bf x}_0)}{\partial s}\tau=-W({\bf x},s|{\bf x}_0)+
\sum_{n=0}^{\infty}\frac{(-\xi)^n}{n!}\frac{\partial^n}{\partial x^n}
\left[W({\bf x},s|{\bf x}_0)\int_{-\infty}^{\infty} \xi^nW({\bf x}+\xi,\tau|{\bf x})d\xi\right].
\]
We have neglected higher powers of $\tau$ and have used that for $n=0$ 
we get simply $W({\bf x},s|{\bf x}_0)$ due to normalization.
}
\end{small}
  %\end{block}
}




\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equation}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
We simplify the above by introducing the moments 
\[
M_n=\frac{1}{\tau}\int_{-\infty}^{\infty} \xi^nW({\bf x}+\xi,\tau|{\bf x})d\xi=
\frac{\langle [\Delta x(\tau)]^n\rangle}{\tau},
\]
resulting in
\[
\frac{\partial W({\bf x},s|{\bf x}_0)}{\partial s}=
\sum_{n=1}^{\infty}\frac{(-\xi)^n}{n!}\frac{\partial^n}{\partial x^n}
\left[W({\bf x},s|{\bf x}_0)M_n\right].
\]
}
\end{small}
  %\end{block}
}


\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equation}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
When $\tau \rightarrow 0$ we assume that $\langle [\Delta x(\tau)]^n\rangle \rightarrow 0$ more rapidly than $\tau$ itself if $n > 2$. 
When $\tau$ is much larger than the standard correlation time of 
system then $M_n$ for $n > 2$ can normally be neglected.
This means that fluctuations become negligible at large time scales.

If we neglect such terms we can rewrite the ESKC equation as 
\[
\frac{\partial W({\bf x},s|{\bf x}_0)}{\partial s}=
-\frac{\partial M_1W({\bf x},s|{\bf x}_0)}{\partial x}+
\frac{1}{2}\frac{\partial^2 M_2W({\bf x},s|{\bf x}_0)}{\partial x^2}.
\]
}
\end{small}
  %\end{block}
}
\frame[containsverbatim]
{
  \frametitle{Importance sampling, Fokker-Planck and Langevin equation}
%\begin{block}{Fokker-Planck and detailed balance}
\begin{small}
{\scriptsize
In a more compact form we have
\[
\frac{\partial W}{\partial s}=
-\frac{\partial M_1W}{\partial x}+
\frac{1}{2}\frac{\partial^2 M_2W}{\partial x^2},
\]
which is the Fokker-Planck equation!  It is trivial to replace 
position with velocity (momentum).
}
\end{small}
  %\end{block}
}

\frame
{
  \frametitle{Langevin equation}
\begin{small}
{\scriptsize
Consider a particle suspended in a liquid. On its path through the liquid it will continuously collide with the liquid molecules. Because on average the particle will collide more often on the front side than on the back side, it will experience a systematic force proportional with its velocity, and directed opposite to its velocity. Besides this systematic force the particle will experience a stochastic force  $ \vec{F}(t)$. 
The equations of motion then read 
\[ 
 \frac{d\vec{r}}{dt} 	=  \vec{v},
\] 	
\[
\frac{d\vec{v}}{dt} 	=  -\xi \vec{v}+\vec{F}.
\]


}
\end{small}
}
\frame
{
  \frametitle{Langevin equation}
\begin{small}
{\scriptsize
From hydrodynamics  we know that the friction constant  $\xi$ is given by

\begin{displaymath}\xi =6\pi \eta a/m \end{displaymath}

where $\eta$ is the viscosity  of the solvent and a is the radius of the particle.

Solving the second equation in the previous slide we get 
\[
\vec{v}(t)=\vec{v}_{0}e^{-\xi t}+\int_{0}^{t}d\tau e^{-\xi (t-\tau )}\vec{F }(\tau ). 
\]
}
\end{small}
}
\frame
{
  \frametitle{Langevin equation}
\begin{small}
{\scriptsize
If we want to get some useful information out of this, we have to average over all possible realizations of 
$ \vec{F}(t)$, with the initial velocity as a condition. A useful quantity for example is
 \[ 
\langle \vec{v}(t)\cdot \vec{v}(t)\rangle_{\vec{v}_{0}}=v_{0}^{-\xi 2t}
+2\int_{0}^{t}d\tau e^{-\xi (2t-\tau)}\vec{v}_{0}\cdot \langle \vec{F}(\tau )\rangle_{\vec{v}_{0}}
\]
\[  	  	
 +\int_{0}^{t}d\tau ^{\prime }\int_{0}^{t}d\tau e^{-\xi (2t-\tau -\tau ^{\prime })}
\langle \vec{F}(\tau )\cdot \vec{F}(\tau ^{\prime })\rangle_{ \vec{v}_{0}}.
\]
}
\end{small}
}
\frame
{
  \frametitle{Langevin equation}
\begin{small}
{\scriptsize
In order to continue we have to make some assumptions about the conditional averages of the stochastic forces. 
In view of the chaotic character of the stochastic forces the following assumptions seem to be appropriate
  
\[ \langle \vec{F}(t)\rangle 	= 	0, \]
\[\langle \vec{F}(t)\cdot \vec{F}(t^{\prime })\rangle_{\vec{v}_{0}}=  C_{\vec{v}_{0}}\delta (t-t^{\prime }).
\] 	

}
\end{small}
}
\frame
{
  \frametitle{Langevin equation}
\begin{small}
{\scriptsize
We omit the subscript $\vec{v}_{0}$, when the quantity of interest turns out to be independent of $\vec{v}_{0}$. Using the last three equations we get

 \[
\langle \vec{v}(t)\cdot \vec{v}(t)\rangle_{\vec{v}_{0}}=v_{0}^{2}e^{-2\xi t}+\frac{C_{\vec{v}_{0}}}{2\xi }(1-e^{-2\xi t}).\]

For large t this should be equal to 3kT/m, from which it follows that

\[
\langle \vec{F}(t)\cdot \vec{F}(t^{\prime })\rangle =6\frac{kT}{m}\xi \delta (t-t^{\prime }). \]

This result is called the fluctuation-dissipation theorem .
}
\end{small}
}
\frame
{
  \frametitle{Langevin equation}
\begin{small}
{\scriptsize
Integrating 
 \[ 
\vec{v}(t)=\vec{v}_{0}e^{-\xi t}+\int_{0}^{t}d\tau e^{-\xi (t-\tau )}\vec{F }(\tau ), \] 
we get
\[
\vec{r}(t)=\vec{r}_{0}+\vec{v}_{0}\frac{1}{\xi }(1-e^{-\xi t})+
\int_0^td\tau \int_0^{\tau}\tau ^{\prime } e^{-\xi (\tau -\tau ^{\prime })}\vec{F}(\tau ^{\prime }), \]
from which we calculate the mean square displacement 
\[
\langle ( \vec{r}(t)-\vec{r}_{0})^{2}\rangle _{\vec{v}_{0}}=\frac{v_0^2}{\xi}(1-e^{-\xi t})^{2}+\frac{3kT}{m\xi ^{2}}(2\xi t-3+4e^{-\xi t}-e^{-2\xi t}). \]
}
\end{small}
}
\frame
{
  \frametitle{Langevin equation}
\begin{small}
{\scriptsize
For very large $t$ this becomes

\[
\langle (\vec{r}(t)-\vec{r}_{0})^{2}\rangle =\frac{6kT}{m\xi }t \] 

from which we get the Einstein relation  

 \[ D= \frac{kT}{m\xi } \] 	

where we have used $\langle (\vec{r}(t)-\vec{r}_{0})^{2}\rangle =6Dt$.
}
\end{small}
}

\section[Week 8]{Week 8}
\frame
{
  \frametitle{Topics for Week 8, February 21-25}
  \begin{block}{Blocking and statistical analysis}
\begin{itemize}
\item Repetition from last week
\item Importance sampling, further discussion of codes
\item Begin discussion of blocking and statistical analysis of data
\item Definition  of onebody densities. 
\end{itemize}
Your tasks this week:
\begin{itemize}
\item Finalize importance sampling
\item Start implementing blocking.
\item Start computing one-body densities.
\item Read about the conjugate gradient method in Thijssen's text till next week. Alternatively, chapter 10 of Numerical Recipes gives a very good overview. The code dfpmin is taken from chapter 10.7 of Numerical Recipes. 
\end{itemize}


  \end{block}
} 


\frame[containsverbatim]
{
  \frametitle{Definition of onebody density, needed in 1d}
\begin{small}
{\scriptsize
The harmonic oscillator-like functions for so-called $n_x=n_y=0$ waves are rather simple. 

This means that if we use just the harmonic oscillator-like wave functions, our ground state
for the two electron dot is 
\[
\Phi({\bf r_1},{\bf r_2}) = C\exp{\left(-\omega(r_1^2+r_2^2)/2\right)}.
\]
and the onebody density is defined as 
\[
\rho({\bf r}_1) = \int d {\bf r}_2\left|C\exp{\left(-\omega(r_1^2+r_2^2)/2\right)}\right|^2,
\]
if we use just the Harmonic oscillator wave functions. Remember that these 
are eigenfunctions of the unperturbed problem.
 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Definition of onebody density, needed in 1d}
\begin{small}
{\scriptsize
With the onebody density defined as  
\[
\rho({\bf r}_1) = \int d {\bf r}_2\left|C\exp{\left(-\omega(r_1^2+r_2^2)/2\right)}\right|^2,
\]
your tasks are to find the constant $C$ and then calculate the density for only a harmonic oscillator state. Plot it as a function of $x$ and $y$ for the ground state. 
 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Definition of onebody density, needed in 1d}
\begin{small}
{\scriptsize
In the next step the pure harmonic oscillator wave function is replaced
by the optimal trial  wave function from our Monte Carlo calculations, namely
$\Psi_T$. This gives a new density given by 
\[
\rho({\bf r}_1) = \int d {\bf r}_2\left|\Psi_T\left({\bf r}_1,{\bf r}_2)\right)}\right|^2.
\]
Your task then is to compute the density for the ground state with the correlations baked in and compare the result with the one obtained with the pure harmonic oscillator. You have to compare this for different values of $\omega$ in order to study the role of correlations. 
 }
 \end{small}
 }




\frame{
  \frametitle{Why blocking?}
  \begin{block}{Statistical analysis, see chapter 11.2 of lecture notes}
    \begin{itemize}
    \item Monte Carlo simulations have to be treated as {\em computer
      experiments}
    \item The results can be analysed with the same statistical tools as
      we would use when analyzing experimental data.
    \item As in all experiments, we are looking for expectation
      values and an estimate of how accurate they are, i.e., possible sources for errors.
    \end{itemize}
  \end{block}
}

\frame{
  \frametitle{Why blocking?}
  \begin{block}{Statistical analysis}
    \begin{itemize}
    \item As in other experiments, Monte Carlo experiments have two
      classes of errors:
      \begin{itemize}
      \item Statistical errors
      \item Systematical errors
      \end{itemize}
    \item Statistical errors can be estimated using standard tools
      from statistics
    \item Systematical errors are method specific and must be treated
      differently from case to case. (In VMC a common source is the
      step length or time step in importance sampling)
    \end{itemize}
  \end{block}
}


\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
The \emph{probability distribution function (PDF)} is a function
$p(x)$ on the domain which, in the discrete case, gives us the
probability or relative frequency with which these values of $X$
occur:
\bdm
p(x) = \prob(X=x)
\edm
In the continuous case, the PDF does not directly depict the
actual probability. Instead we define the probability for the
stochastic variable to assume any value on an infinitesimal interval
around $x$ to be $p(x)dx$. The continuous function $p(x)$ then gives us
the \emph{density} of the probability rather than the probability
itself. The probability for a stochastic variable to assume any value
on a non-infinitesimal interval $[a,\,b]$ is then just the integral:
\bdm
\prob(a\leq X\leq b) = \int_a^b p(x)dx
\edm
Qualitatively speaking, a stochastic variable represents the values of
numbers chosen as if by chance from some specified PDF so that the
selection of a large set of these numbers reproduces this PDF.
}
\end{small}
}


\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
Also of interest to us is the \emph{cumulative probability
distribution function (CDF)}, $P(x)$, which is just the probability
for a stochastic variable $X$ to assume any value less than $x$:
\bdm
P(x)=\mathrm{Prob(}X\leq x\mathrm{)} =
\int_{-\infty}^x p(x^{\prime})dx^{\prime}
\edm
The relation between a CDF and its corresponding PDF is then:
\bdm
p(x) = \frac{d}{dx}P(x)
\edm
}
\end{small}
}


\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
A particularly useful class of special expectation values are the
\emph{moments}. The $n$-th moment of the PDF $p$ is defined as
follows:
\bdm
\mean{x^n} \equiv \int\! x^n p(x)\,dx
\edm
The zero-th moment $\mean{1}$ is just the normalization condition of
$p$. The first moment, $\mean{x}$, is called the \emph{mean} of $p$
and often denoted by the letter $\mu$:
\bdm
\mean{x} = \mu \equiv \int\! x p(x)\,dx
\edm
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
A special version of the moments is the set of \emph{central moments},
the n-th central moment defined as:
\bdm
\mean{(x-\mean{x})^n} \equiv \int\! (x-\mean{x})^n p(x)\,dx
\edm
The zero-th and first central moments are both trivial, equal $1$ and
$0$, respectively. But the second central moment, known as the
\emph{variance} of $p$, is of particular interest. For the stochastic
variable $X$, the variance is denoted as $\sigma^2_X$ or $\var(X)$:
\beaN
\sigma^2_X\ \ =\ \ \var(X) & = & \mean{(x-\mean{x})^2} =
\int\! (x-\mean{x})^2 p(x)\,dx\\
& = & \int\! \left(x^2 - 2 x \mean{x}^{\phantom{2}} +
  \mean{x}^2\right)p(x)\,dx\\
& = & \mean{x^2} - 2 \mean{x}\mean{x} + \mean{x}^2\\
& = & \mean{x^2} - \mean{x}^2
\eeaN
The square root of the variance, $\sigma =
\sqrt{\mean{(x-\mean{x})^2}}$ is called the \emph{standard
  deviation} of $p$. It is clearly just the RMS (root-mean-square)
value of the deviation of the PDF from its mean value, interpreted
qualitatively as the ``spread'' of $p$ around its mean.
}
\end{small}
}


\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
Another important quantity is the so called covariance, a variant of
the above defined variance. Consider again the set $\{X_i\}$ of $n$
stochastic variables (not necessarily uncorrelated) with the
multivariate PDF $P(x_1,\dots,x_n)$. The \emph{covariance} of two
of the stochastic variables, $X_i$ and $X_j$, is defined as follows:
\bea
\cov(X_i,\,X_j) &\equiv& \meanb{(x_i-\mean{x_i})(x_j-\mean{x_j})}
\nonumber\\
&=&
\int\!\cdots\!\int\!(x_i-\mean{x_i})(x_j-\mean{x_j})\,
P(x_1,\dots,x_n)\,dx_1\dots dx_n
\label{eq:def_covariance}
\eea
with
\bdm
\mean{x_i} =
\int\!\cdots\!\int\!x_i\,P(x_1,\dots,x_n)\,dx_1\dots dx_n
\edm
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
If we consider the above covariance as a matrix $C_{ij} =
\cov(X_i,\,X_j)$, then the diagonal elements are just the familiar
variances, $C_{ii} = \cov(X_i,\,X_i) = \var(X_i)$. It turns out that
all the off-diagonal elements are zero if the stochastic variables are
uncorrelated. This is easy to show, keeping in mind the linearity of
the expectation value. Consider the stochastic variables $X_i$ and
$X_j$, ($i\neq j$):
\beaN
\cov(X_i,\,X_j) &=& \meanb{(x_i-\mean{x_i})(x_j-\mean{x_j})}\\
&=&\mean{x_i x_j - x_i\mean{x_j} - \mean{x_i}x_j + \mean{x_i}\mean{x_j}}\\
&=&\mean{x_i x_j} - \mean{x_i\mean{x_j}} - \mean{\mean{x_i}x_j} +
\mean{\mean{x_i}\mean{x_j}}\\
&=&\mean{x_i x_j} - \mean{x_i}\mean{x_j} - \mean{x_i}\mean{x_j} +
\mean{x_i}\mean{x_j}\\
&=&\mean{x_i x_j} - \mean{x_i}\mean{x_j}
\eeaN
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
If $X_i$ and $X_j$ are independent, we get $\mean{x_i x_j} =
\mean{x_i}\mean{x_j}$, resulting in $\cov(X_i, X_j) = 0\ \ (i\neq j)$.

Also useful for us is the covariance of linear combinations of
stochastic variables. Let $\{X_i\}$ and $\{Y_i\}$ be two sets of
stochastic variables. Let also $\{a_i\}$ and $\{b_i\}$ be two sets of
scalars. Consider the linear combination:
\bdm
U = \sum_i a_i X_i \qquad V = \sum_j b_j Y_j
\edm
By the linearity of the expectation value
\bdm
\cov(U, V) = \sum_{i,j}a_i b_j \cov(X_i, Y_j)
\edm
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
Now, since the variance is just $\var(X_i) = \cov(X_i, X_i)$, we get
the variance of the linear combination $U = \sum_i a_i X_i$:
\be
\var(U) = \sum_{i,j}a_i a_j \cov(X_i, X_j)
\label{eq:variance_linear_combination}
\ee
And in the special case when the stochastic variables are
uncorrelated, the off-diagonal elements of the covariance are as we
know zero, resulting in:
\bdm
\var(U) = \sum_i a_i^2 \cov(X_i, X_i) = \sum_i a_i^2 \var(X_i)
\edm
\bdm
\var(\sum_i a_i X_i) = \sum_i a_i^2 \var(X_i)
\edm
which will become very useful in our study of the error in the mean
value of a set of measurements.
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
A \emph{stochastic process} is a process that produces sequentially a
chain of values:
\bdm
\{x_1, x_2,\dots\,x_k,\dots\}.
\edm
We will call these
values our \emph{measurements} and the entire set as our measured
\emph{sample}.  The action of measuring all the elements of a sample
we will call a stochastic \emph{experiment} (since, operationally,
they are often associated with results of empirical observation of
some physical or mathematical phenomena; precisely an experiment). We
assume that these values are distributed according to some 
PDF $p_X^{\phantom X}(x)$, where $X$ is just the formal symbol for the
stochastic variable whose PDF is $p_X^{\phantom X}(x)$. Instead of
trying to determine the full distribution $p$ we are often only
interested in finding the few lowest moments, like the mean
$\mu_X^{\phantom X}$ and the variance $\sigma_X^{\phantom X}$.
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
In practical situations a sample is always of finite size. Let that
size be $n$. The expectation value of a sample, the \emph{sample
mean}, is then defined as follows:
\bdm
\bar x_n \equiv \frac{1}{n}\sum_{k=1}^n x_k
\edm
The \emph{sample variance} is:
\bdm
\var(x) \equiv \frac{1}{n}\sum_{k=1}^n (x_k - \bar x_n)^2
\edm
its square root being the \emph{standard deviation of the sample}. The
\emph{sample covariance} is:
\bdm
\cov(x)\equiv\frac{1}{n}\sum_{kl}(x_k - \bar x_n)(x_l - \bar x_n)
\edm
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
Note that the sample variance is the sample covariance without the
cross terms. In a similar manner as the covariance in
eq.~(\ref{eq:def_covariance}) is a measure of the correlation between
two stochastic variables, the above defined sample covariance is a
measure of the sequential correlation between succeeding measurements
of a sample.

These quantities, being known experimental values, differ
significantly from and must not be confused with the similarly named
quantities for stochastic variables, mean $\mu_X$, variance $\var(X)$
and covariance $\cov(X,Y)$.
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
The law of large numbers
states that as the size of our sample grows to infinity, the sample
mean approaches the true mean $\mu_X^{\phantom X}$ of the chosen PDF:
\bdm
\lim_{n\to\infty}\bar x_n = \mu_X^{\phantom X}
\edm
The sample mean $\bar x_n$ works therefore as an estimate of the true
mean $\mu_X^{\phantom X}$.

What we need to find out is how good an approximation $\bar x_n$ is to
$\mu_X^{\phantom X}$. In any stochastic measurement, an estimated
mean is of no use to us without a measure of its error. A quantity
that tells us how well we can reproduce it in another experiment. We
are therefore interested in the PDF of the sample mean itself. Its
standard deviation will be a measure of the spread of sample means,
and we will simply call it the \emph{error} of the sample mean, or
just sample error, and denote it by $\mathrm{err}_X^{\phantom X}$. In
practice, we will only be able to produce an \emph{estimate} of the
sample error since the exact value would require the knowledge of the
true PDFs behind, which we usually do not have.
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
The straight forward brute force way of estimating the sample error is
simply by producing a number of samples, and treating the mean of each
as a measurement. The standard deviation of these means will then be
an estimate of the original sample error. If we are unable to produce
more than one sample, we can split it up sequentially into smaller
ones, treating each in the same way as above. This procedure is known
as \emph{blocking} and will be given more attention shortly. At this
point it is worth while exploring more indirect methods of estimation
that will help us understand some important underlying principles of
correlational effects.
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
Let us first take a look at what happens to the sample error as the
size of the sample grows. In a sample, each of the measurements $x_i$
can be associated with its own stochastic variable $X_i$. The
stochastic variable $\overline X_n$ for the sample mean $\bar x_n$ is
then just a linear combination, already familiar to us:
\bdm
\overline X_n = \frac{1}{n}\sum_{i=1}^n X_i
\edm
All the coefficients are just equal $1/n$. The PDF of $\overline X_n$,
denoted by $p_{\overline X_n}(x)$ is the desired PDF of the sample
means. 
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
The probability density of obtaining a sample mean $\bar x_n$
is the product of probabilities of obtaining arbitrary values $x_1,
x_2,\dots,x_n$ with the constraint that the mean of the set $\{x_i\}$
is $\bar x_n$:
\bdm
p_{\overline X_n}(x) = \int p_X^{\phantom X}(x_1)\cdots
\int p_X^{\phantom X}(x_n)\ 
\delta\!\left(x - \frac{x_1+x_2+\dots+x_n}{n}\right)dx_n \cdots dx_1
\edm
And in particular we are interested in its variance $\var(\overline
X_n)$.
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
It is generally not possible to express $p_{\overline X_n}(x)$ in a
closed form given an arbitrary PDF $p_X^{\phantom X}$ and a number
$n$. But for the limit $n\to\infty$ it is possible to make an
approximation. The very important result is called \emph{the central
  limit theorem}. It tells us that as $n$ goes to infinity,
$p_{\overline X_n}(x)$ approaches a Gaussian distribution whose mean
and variance equal the true mean and variance, $\mu_{X}^{\phantom X}$
and $\sigma_{X}^{2}$, respectively:
\be
\lim_{n\to\infty} p_{\overline X_n}(x) =
\left(\frac{n}{2\pi\var(X)}\right)^{1/2}
e^{-\frac{n(x-\bar x_n)^2}{2\var(X)}}
\label{eq:central_limit_gaussian}
\ee
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
The desired variance
$\var(\overline X_n)$, i.e. the sample error squared
$\mathrm{err}_X^2$, is given by:
\be
\mathrm{err}_X^2 = \var(\overline X_n) = \frac{1}{n^2}
\sum_{ij} \cov(X_i, X_j)
\label{eq:error_exact}
\ee
We see now that in order to calculate the exact error of the sample
with the above expression, we would need the true means
$\mu_{X_i}^{\phantom X}$ of the stochastic variables $X_i$. To
calculate these requires that we know the true multivariate PDF of all
the $X_i$. But this PDF is unknown to us, we have only got the measurements of
one sample. The best we can do is to let the sample itself be an
estimate of the PDF of each of the $X_i$, estimating all properties of
$X_i$ through the measurements of the sample.
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
Our estimate of $\mu_{X_i}^{\phantom X}$ is then the sample mean $\bar x$
itself, in accordance with the the central limit theorem:
\bdm
\mu_{X_i}^{\phantom X} = \mean{x_i} \approx \frac{1}{n}\sum_{k=1}^n
x_k = \bar x
\edm
Using $\bar x$ in place of $\mu_{X_i}^{\phantom X}$ we can give an
\emph{estimate} of the covariance in eq.~(\ref{eq:error_exact}):
\beaN
\cov(X_i, X_j) &=& \mean{(x_i-\mean{x_i})(x_j-\mean{x_j})}
\approx\mean{(x_i - \bar x)(x_j - \bar{x})}\\
&\approx&\frac{1}{n} \sum_{l}^n \left(\frac{1}{n}\sum_{k}^n (x_k -
\bar x_n)(x_l - \bar x_n)\right)
=\frac{1}{n}\frac{1}{n} \sum_{kl} (x_k -
\bar x_n)(x_l - \bar x_n)\\
&=&\frac{1}{n}\cov(x)
\eeaN
}
\end{small}
}



\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
By the same procedure we can use the sample variance as an
estimate of the variance of any of the stochastic variables $X_i$:
\bea
\var(X_i)
&=&\mean{x_i - \mean{x_i}} \approx \mean{x_i - \bar x_n}\nonumber\\
&\approx&\frac{1}{n}\sum_{k=1}^n (x_k - \bar x_n)\nonumber\\
&=&\var(x)
\label{eq:var_estimate_i_think}
\eea
Now we can calculate an estimate of the error
$\mathrm{err}_X^{\phantom X}$ of the sample mean $\bar x_n$:
\bea
\mathrm{err}_X^2
&=&\frac{1}{n^2}\sum_{ij} \cov(X_i, X_j) \nonumber \\
&\approx&\frac{1}{n^2}\sum_{ij}\frac{1}{n}\cov(x) =
\frac{1}{n^2}n^2\frac{1}{n}\cov(x)\nonumber\\
&=&\frac{1}{n}\cov(x)
\label{eq:error_estimate}
\eea
which is nothing but the sample covariance divided by the number of
measurements in the sample.
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
In the special case that the measurements of the sample are
uncorrelated (equivalently the stochastic variables $X_i$ are
uncorrelated) we have that the off-diagonal elements of the covariance
are zero. This gives the following estimate of the sample error:
\bea
\mathrm{err}_X^2 &=& \frac{1}{n^2}\sum_{ij} \cov(X_i, X_j) =
\frac{1}{n^2} \sum_i \var(X_i)\nonumber\\
&\approx&
\frac{1}{n^2} \sum_i \var(x)\nonumber\\ &=& \frac{1}{n}\var(x)
\label{eq:error_estimate_uncorrel}
\eea
where in the second step we have used eq.~(\ref{eq:var_estimate_i_think}).
The error of the sample is then just its standard deviation divided by
the square root of the number of measurements the sample contains.
This is a very useful formula which is easy to compute. It acts as a
first approximation to the error, but in numerical experiments, we
cannot overlook the always present correlations.
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
For computational purposes one usually splits up the estimate of
$\mathrm{err}_X^2$, given by eq.~(\ref{eq:error_estimate}), into two
parts:
\bea
\mathrm{err}_X^2 &=&
\frac{1}{n}\var(x) + \frac{1}{n}(\cov(x)-\var(x))\nonumber\\&=&
\frac{1}{n^2}\sum_{k=1}^n (x_k - \bar x_n)^2 +
\frac{2}{n^2}\sum_{k<l} (x_k - \bar x_n)(x_l - \bar x_n)
\label{eq:error_estimate_split_up}
\eea
The first term is the same as the error in the uncorrelated case,
eq.~(\ref{eq:error_estimate_uncorrel}). This means that the second
term accounts for the error correction due to correlation between the
measurements. For uncorrelated measurements this second term is zero.
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
Computationally the uncorrelated first term is much easier to treat
efficiently than the second.
\bdm
\var(x) = \frac{1}{n}\sum_{k=1}^n (x_k - \bar x_n)^2 =
\left(\frac{1}{n}\sum_{k=1}^n x_k^2\right) - \bar x_n^2
\edm
We just accumulate separately the values $x^2$ and $x$ for every
measurement $x$ we receive. The correlation term, though, has to be
calculated at the end of the experiment since we need all the
measurements to calculate the cross terms. Therefore, all measurements
have to be stored throughout the experiment.
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
Let us analyze the problem by splitting up the correlation term into
partial sums of the form:
\bdm
f_d = \frac{1}{n-d}\sum_{k=1}^{n-d}(x_k - \bar x_n)(x_{k+d} - \bar x_n)
\edm
The correlation term of the error can now be rewritten in terms of
$f_d$:
\bdm
\frac{2}{n}\sum_{k<l} (x_k - \bar x_n)(x_l - \bar x_n) =
2\sum_{d=1}^{n-1} f_d
\edm
The value of $f_d$ reflects the correlation between measurements
separated by the distance $d$ in the sample samples.  Notice that for
$d=0$, $f$ is just the sample variance, $\var(x)$. If we divide $f_d$
by $\var(x)$, we arrive at the so called \emph{autocorrelation
  function}:
\bdm
\kappa_d = \frac{f_d}{\var(x)}
\edm
which gives us a useful measure of the correlation pair correlation
starting always at $1$ for $d=0$.
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
The sample error (see eq.~(\ref{eq:error_estimate_split_up})) can now be
written in terms of the autocorrelation function:
\bea
\mathrm{err}_X^2 &=&
\frac{1}{n}\var(x)+\frac{2}{n}\cdot\var(x)\sum_{d=1}^{n-1}
\frac{f_d}{\var(x)}\nonumber\\ &=&
\left(1+2\sum_{d=1}^{n-1}\kappa_d\right)\frac{1}{n}\var(x)\nonumber\\
&=&\rule{0pt}{17pt}
\frac{\tau}{n}\cdot\var(x)
\label{eq:error_estimate_corr_time}
\eea
and we see that $\mathrm{err}_X$ can be expressed in terms the
uncorrelated sample variance times a correction factor $\tau$ which
accounts for the correlation between measurements. We call this
correction factor the \emph{autocorrelation time}:
\be
\tau = 1+2\sum_{d=1}^{n-1}\kappa_d
\label{eq:autocorrelation_time}
\ee
}
\end{small}
}

\frame
{
  \frametitle{Statistics and blocking}
\begin{small}
{\scriptsize
For a correlation free experiment, $\tau$
equals 1. From the point of view of
eq.~(\ref{eq:error_estimate_corr_time}) we can interpret a sequential
correlation as an effective reduction of the number of measurements by
a factor $\tau$. The effective number of measurements becomes:
\bdm
n_\mathrm{eff} = \frac{n}{\tau}
\edm
To neglect the autocorrelation time $\tau$ will always cause our
simple uncorrelated estimate of $\mathrm{err}_X^2\approx \var(x)/n$ to
be less than the true sample error. The estimate of the error will be
too ``good''. On the other hand, the calculation of the full
autocorrelation time poses an efficiency problem if the set of
measurements is very large.
}
\end{small}
}




\frame
{
  \frametitle{Can we understand this? Time Auto-correlation Function}
\begin{small}
{\scriptsize
The so-called time-displacement autocorrelation $\phi(t)$ for a quantity ${\cal M}$ is given by
\[
\phi(t) = \int dt' \left[{\cal M}(t')-\langle {\cal M} \rangle\right]\left[{\cal M}(t'+t)-\langle {\cal M} \rangle\right],
\]
which can be rewritten as 
\[
\phi(t) = \int dt' \left[{\cal M}(t'){\cal M}(t'+t)-\langle {\cal M} \rangle^2\right],
\]
where $\langle {\cal M} \rangle$ is the average value and
${\cal M}(t)$ its instantaneous value. We can discretize this function as follows, where we used our
set of computed values ${\cal M}(t)$ for a set of discretized times (our Monte Carlo cycles corresponding to moving all electrons?)
\[
\phi(t)  = \frac{1}{t_{\mathrm{max}}-t}\sum_{t'=0}^{t_{\mathrm{max}}-t}{\cal M}(t'){\cal M}(t'+t)
-\frac{1}{t_{\mathrm{max}}-t}\sum_{t'=0}^{t_{\mathrm{max}}-t}{\cal M}(t')\times
\frac{1}{t_{\mathrm{max}}-t}\sum_{t'=0}^{t_{\mathrm{max}}-t}{\cal M}(t'+t).\label{eq:phitf}
\]
}
\end{small}
}


\frame
{
  \frametitle{Time Auto-correlation Function}
\begin{small}
{\scriptsize
One should be careful with times close to $t_{\mathrm{max}}$, the upper limit of the sums 
becomes small and we end up integrating over a rather small time interval. This means that the statistical
error in $\phi(t)$ due to the random nature of the fluctuations in ${\cal M}(t)$ can become large.

One should therefore choose $t \ll t_{\mathrm{max}}$.

Note that the variable ${\cal M}$ can be any expectation values of interest.



The time-correlation function gives a measure of the correlation between the various values of the variable 
at a time $t'$ and a time $t'+t$. If we multiply the values of ${\cal M}$ at these two different times,
we will get a positive contribution if they are fluctuating in the same direction, or a negative value
if they fluctuate in the opposite direction. If we then integrate over time, or use the discretized version of, the time correlation function $\phi(t)$ should take a non-zero value if the fluctuations are 
correlated, else it should gradually go to zero. For times a long way apart 
the different values of ${\cal M}$  are most likely 
uncorrelated and $\phi(t)$ should be zero.
}
\end{small}
}




\frame
{
  \frametitle{Time Auto-correlation Function}
\begin{small}
{\scriptsize
We can derive the correlation time by observing that our Metropolis algorithm is based on a random
walk in the space of all  possible spin configurations. 
Our probability 
distribution function ${\bf \hat{w}}(t)$ after a given number of time steps $t$ could be written as
\[
   {\bf \hat{w}}(t) = {\bf \hat{W}^t\hat{w}}(0),
\]
with ${\bf \hat{w}}(0)$ the distribution at $t=0$ and ${\bf \hat{W}}$ representing the 
transition probability matrix. 
We can always expand ${\bf \hat{w}}(0)$ in terms of the right eigenvectors of 
${\bf \hat{v}}$ of ${\bf \hat{W}}$ as 
\[
    {\bf \hat{w}}(0)  = \sum_i\alpha_i{\bf \hat{v}}_i,
\]
resulting in 
\[
   {\bf \hat{w}}(t) = {\bf \hat{W}}^t{\bf \hat{w}}(0)={\bf \hat{W}}^t\sum_i\alpha_i{\bf \hat{v}}_i=
\sum_i\lambda_i^t\alpha_i{\bf \hat{v}}_i,
\]
with $\lambda_i$ the $i^{\mathrm{th}}$ eigenvalue corresponding to  
the eigenvector ${\bf \hat{v}}_i$. 
}
\end{small}
}



\frame
{
  \frametitle{Time Auto-correlation Function}
\begin{small}
{\scriptsize
If we assume that $\lambda_0$ is the largest eigenvector we see that in the limit $t\rightarrow \infty$,
${\bf \hat{w}}(t)$ becomes proportional to the corresponding eigenvector 
${\bf \hat{v}}_0$. This is our steady state or final distribution. 

We can relate this property to an observable like the mean energy.
With the probabilty ${\bf \hat{w}}(t)$ (which in our case is the squared trial wave function) we
can write the expectation values as 
\[
 \langle {\cal M}(t) \rangle  = \sum_{\mu} {\bf \hat{w}}(t)_{\mu}{\cal M}_{\mu},
\]  
or as the scalar of a  vector product
 \[
 \langle {\cal M}(t) \rangle  = {\bf \hat{w}}(t){\bf m},
\]  
with ${\bf m}$ being the vector whose elements are the values of ${\cal M}_{\mu}$ in its 
various microstates $\mu$.
}
\end{small}
}


\frame
{
  \frametitle{Time Auto-correlation Function}
\begin{small}
{\scriptsize
We rewrite this relation  as
 \[
 \langle {\cal M}(t) \rangle  = {\bf \hat{w}}(t){\bf m}=\sum_i\lambda_i^t\alpha_i{\bf \hat{v}}_i{\bf m}_i.
\]  
If we define $m_i={\bf \hat{v}}_i{\bf m}_i$ as the expectation value of
${\cal M}$ in the $i^{\mathrm{th}}$ eigenstate we can rewrite the last equation as
 \[
 \langle {\cal M}(t) \rangle  = \sum_i\lambda_i^t\alpha_im_i.
\] 
Since we have that in the limit $t\rightarrow \infty$ the mean value is dominated by the 
the largest eigenvalue $\lambda_0$, we can rewrite the last equation as
 \[
 \langle {\cal M}(t) \rangle  = \langle {\cal M}(\infty) \rangle+\sum_{i\ne 0}\lambda_i^t\alpha_im_i.
\] 
We define the quantity
\[
   \tau_i=-\frac{1}{log\lambda_i},
\]
and rewrite the last expectation value as
 \[
 \langle {\cal M}(t) \rangle  = \langle {\cal M}(\infty) \rangle+\sum_{i\ne 0}\alpha_im_ie^{-t/\tau_i}.
\label{eq:finalmeanm}
\] 
}
\end{small}
}


\frame
{
  \frametitle{Time Auto-correlation Function}
\begin{small}
{\scriptsize
The quantities $\tau_i$ are the correlation times for the system. They control also the auto-correlation function 
discussed above.  The longest correlation time is obviously given by the second largest
eigenvalue $\tau_1$, which normally defines the correlation time discussed above. For large times, this is the 
only correlation time that survives. If higher eigenvalues of the transition matrix are well separated from 
$\lambda_1$ and we simulate long enough,  $\tau_1$ may well define the correlation time. 
In other cases we may not be able to extract a reliable result for $\tau_1$. 
Coming back to the time correlation function $\phi(t)$ we can present a more general definition in terms
of the mean magnetizations $ \langle {\cal M}(t) \rangle$. Recalling that the mean value is equal 
to $ \langle {\cal M}(\infty) \rangle$ we arrive at the expectation values
\[
\phi(t) =\langle {\cal M}(0)-{\cal M}(\infty)\rangle \langle {\cal M}(t)-{\cal M}(\infty)\rangle,
\]
resulting in
\[
\phi(t) =\sum_{i,j\ne 0}m_i\alpha_im_j\alpha_je^{-t/\tau_i},
\]
which is appropriate for all times.
}
\end{small}
}



\frame
{
  \frametitle{Correlation Time}
\begin{small}
{\scriptsize
If the correlation function decays exponentially
\[ \phi (t) \sim \exp{(-t/\tau)}\]
then the exponential correlation time can be computed as the average
\[   \tau_{\mathrm{exp}}  =  -\langle  \frac{t}{log|\frac{\phi(t)}{\phi(0)}|} \rangle. \]

If the decay is exponential, then
\[  \int_0^{\infty} dt \phi(t)  = \int_0^{\infty} dt \phi(0)\exp{(-t/\tau)}  = \tau \phi(0),\] 
which  suggests another measure of correlation
\[   \tau_{\mathrm{int}} = \sum_k \frac{\phi(k)}{\phi(0)}, \]
called the integrated correlation time.
}
\end{small}
}







\frame{
  \frametitle{What is blocking?}
\begin{small}
{\scriptsize
  \begin{block}{Blocking}
    \begin{itemize}
    \item Blocking is a cheap (in terms of CPU expenditure) 
way of estimating statistical errors
    \item Say that we have a set of samples from a Monte Carlo
      experiment
    \item Assuming (wrongly) that our samples are uncorrelated our
      best estimate of the standard deviation of the mean $\langle {\cal M}\rangle$ is given
      by
      \begin{equation*}
        \sigma=\sqrt{\frac{1}{n}\left(\langle {\cal M}^2\rangle-\langle {\cal M}\rangle^2\right)} 
      \end{equation*}
    \item If the samples are correlated we can rewrite our results to show  that
      \begin{equation*}
        \sigma=\sqrt{\frac{1+2\tau/\Delta
            t}{n}\left(\langle {\cal M}^2\rangle-\langle {\cal M}\rangle^2\right)}
      \end{equation*}
      where $\tau$ is the correlation time (the time between a sample
      and the next uncorrelated sample) and $\Delta t$ is time between
      each sample
    \end{itemize}
  \end{block}
}
\end{small}

}

\frame{
  \frametitle{What is blocking?}
  \begin{block}{Blocking}
    \begin{itemize}
    \item If $\Delta t\gg\tau$ our first estimate of $\sigma$ still holds
    \item Much more common that $\Delta t<\tau$
    \item In the method of data blocking we divide the sequence of
      samples into blocks
    \item We then take the mean $\langle {\cal M}_i\rangle$ of block $i=1\ldots
      n_{blocks}$ to calculate the total mean and variance
    \item The size of each block must be so large that sample $j$ of
      block $i$ is not correlated with sample $j$ of block $i+1$
    \item The correlation time $\tau$ would be a good choice
    \end{itemize}
  \end{block}
}

\frame{
  \frametitle{What is blocking?}
  \begin{block}{Blocking}
    \begin{itemize}
    \item Problem: We don't know $\tau$ or it is too expensive to compute
    \item Solution: Make a plot of std. dev. as a function of block
      size
    \item The estimate of std. dev. of correlated data is too low $\to$
      the error will increase with increasing block size until the
      blocks are uncorrelated, where we reach a plateau
    \item When the std. dev. stops increasing the blocks are uncorrelated
    \end{itemize}
  \end{block}
}

\frame{
  \frametitle{Implementation}
  \begin{block}{Main ideas}
    \begin{itemize}
    \item Do a parallel Monte Carlo simulation, storing all samples to
      files (one per process)
    \item Do the statistical analysis on these files, independently of
      your Monte Carlo program
    \item Read the files into an array
    \item Loop over various block sizes
    \item For each block size $n_b$, loop over the array in steps of
      $n_b$ taking the mean of elements $i 
n_b,\ldots,(i+1) n_b$
    \item Take the mean and variance of the resulting array
    \item Write the results for each block size to file for later
      analysis
    \end{itemize}
  \end{block}
}


\frame{
  \frametitle{Implementation}
  \begin{block}{Example}
    \begin{itemize}
    \item The files
      \href{http://www.uio.no/studier/emner/matnat/fys/FYS4410/v08/undervisningsmateriale/Material\%20for\%20Part\%20I\%20by\%20Morten\%20HJ/Programs/Programs\%20for\%20Project\%201/vmc\_para.cpp}{\color{blue}
          vmc\_para.cpp} and  \href{http://www.uio.no/studier/emner/matnat/fys/FYS4410/v08/undervisningsmateriale/Material\%20for\%20Part\%20I\%20by\%20Morten\%20HJ/Programs/Programs\%20for\%20Project\%201/vmc\_blocking.cpp}{\color{blue}
          vmc\_blocking.cpp} contain a parallel VMC simulator and a program for doing
          blocking on the samples from the resulting set of files
        \item Will go through the parts related to blocking
    \end{itemize}
  \end{block}
}

\frame{
  \frametitle{Implementation}
  \begin{block}{Parallel file output}
    \begin{itemize}
    \item The total number of samples from all processes may get very
      large
    \item Hence, storing all samples on the master node is not a
      scalable solution
    \item Instead we store the samples from each process in separate
      files
    \item Must make sure these files have different names
    \end{itemize}
  \end{block}
  \begin{block}{String handling}
    \lstinputlisting{ostringstream.cpp}
  \end{block}
}

\frame{
  \frametitle{Implementation}
  \begin{block}{Parallel file output}
    \begin{itemize}
    \item Having separated the filenames it's just a matter of
      taking the samples and store them to file
    \item Note that there is no need for communication between the
      processes in this procedure
    \end{itemize}
  \end{block}
  \begin{block}{File dumping}
    \lstinputlisting{binaryout.cpp}
  \end{block}
}

\lstset{basicstyle=\tiny}
\frame{
  \frametitle{Implementation}
  \begin{block}{Reading the files}
    \begin{itemize}
    \item Reading the files is only about mirroring the output
    \item To make life easier for ourselves we find the filesize, and
      hence the number of samples by using the C function \texttt{stat}
    \end{itemize}
  \begin{block}{File loading}
    \lstinputlisting{binaryin.cpp}
  \end{block}
  \end{block}
}
\lstset{basicstyle=\small}

\frame{
  \frametitle{Implementation}
  \begin{block}{Blocking}
    \begin{itemize}
      \item Loop over block sizes $i
      n_b,\ldots,(i+1) n_b$
    \end{itemize}
  \end{block}
  \begin{block}{Loop over block sizes}
    \lstinputlisting{blocksizeloop.cpp}
  \end{block}
}

\frame{
  \frametitle{Implementation}
  \begin{block}{Blocking}
    \begin{itemize}
    \item The blocking itself is now just a matter of finding the
      number of blocks (note the integer division) and taking the mean of
      each block 
    \item Note the pointer aritmetic: Adding a number $i$ to an array
      pointer moves the pointer to element $i$ in the array
    \end{itemize}
  \end{block}
  \begin{block}{Blocking function}
    \lstinputlisting{blocking.cpp}
  \end{block}
}

\section[Week 11]{Week 11}
\frame
{
  \frametitle{Topics for Week 11, March 14-18}
  \begin{block}{Conjugate gradient method and onebody densities}
\begin{itemize}
\item Repetition from last week
\item Conjugate gradient method
\item Many electrons and Slater determinant
\end{itemize}
Project work this week: finalize 1d and start with 1e. 
  \end{block}
} 


\frame
{
  \frametitle{Conjugate gradient (CG) method}
\begin{small}
{\scriptsize
The success of the CG method  for finding solutions of non-linear problems is based
on the theory of conjugate gradients for linear systems of equations. It belongs
to the class of iterative methods for solving problems from linear algebra of the type
\[
  \hat{{\bf A}}\hat{\bf {x}} = \hat{\bf {b}}.
\]
In the iterative process we end up with a problem like
\[
  \hat{\bf {r}}= \hat{\bf {b}}-\hat{{\bf A}}\hat{\bf {x}},
\]
where $\hat{\bf {r}}$ is the so-called residual or error in the iterative process.
}
\end{small}
}


\frame
{
  \frametitle{Conjugate gradient method}
\begin{small}
{\scriptsize
The residual is zero when we reach the minimum of the quadratic equation
\[
  P(\hat{\bf {x}})=\frac{1}{2}\hat{\bf {x}}^T\hat{{\bf A}}\hat{\bf {x}} - \hat{\bf {x}}^T\hat{\bf {b}},
\]
with the constraint that the matrix $\hat{{\bf A}}$ is positive definite and symmetric.
If we search for a minimum of the quantum mechanical  variance, then the matrix 
$\hat{{\bf A}}$, which is called the Hessian, is given by the second-derivative of the variance.  This quantity is always positive definite. If we vary the energy, the Hessian may not always be positive definite. 
}
\end{small}
}

\frame
{
  \frametitle{Conjugate gradient method}
\begin{small}
{\scriptsize
In the CG method we define so-called conjugate directions and two vectors 
$\hat{\bf {s}}$ and $\hat{\bf {t}}$
are said to be
conjugate if 
\[
\hat{\bf {s}}^T\hat{{\bf A}}\hat{\bf {t}}= 0.
\]
The philosophy of the CG method is to perform searches in various conjugate directions
of our vectors $\hat{{\bf x}}_i$ obeying the above criterion, namely
\[
\hat{\bf {x}}_i^T\hat{{\bf A}}\hat{\bf {x}}_j= 0.
\]
Two vectors are conjugate if they are orthogonal with respect to 
this inner product. Being conjugate is a symmetric relation: if $\hat{\bf {s}}$ is conjugate to $\hat{\bf {t}}$, then $\hat{\bf {t}}$ is conjugate to $\hat{\bf {s}}$.
}
\end{small}
}


\frame
{
  \frametitle{Conjugate gradient method}
\begin{small}
{\scriptsize
An example is given by the eigenvectors of the matrix 
\[
\hat{\bf {v}}_i^T\hat{{\bf A}}\hat{\bf {v}}_j= \lambda\hat{\bf {v}}_i^T\hat{\bf {v}}_j,
\]
which is zero unless $i=j$. 

}
\end{small}
}


\frame
{
  \frametitle{Conjugate gradient method}
\begin{small}
{\scriptsize
Assume now that we have a symmetric positive-definite matrix $\hat{\bf {A}}$ of size
$n\times n$. At each iteration $i+1$ we obtain the conjugate direction of a vector 
\[
\hat{\bf {x}}_{i+1}=\hat{\bf {x}}_{i}+\alpha_i\hat{\bf {p}}_{i}. 
\]
We assume that $\hat{\bf {p}}_{i}$ is a sequence of $n$ mutually conjugate directions. 
Then the $\hat{\bf {p}}_{i}$  form a basis of $R^n$ and we can expand the solution 
$  \hat{{\bf A}}\hat{\bf {x}} = \hat{\bf {b}}$ in this basis, namely
\[
  \hat{\bf {x}}  = \sum^{n}_{i=1} \alpha_i \hat{\bf {p}}_i.
\]
}
\end{small}
}

\frame
{
  \frametitle{Conjugate gradient method}
\begin{small}
{\scriptsize
The coefficients are given by
\[
    \mathbf{A}\mathbf{x} = \sum^{n}_{i=1} \alpha_i \mathbf{A} \mathbf{p}_i = \mathbf{b}.
\]
Multiplying with $\hat{\bf {p}}_k^T$  from the left gives
\[
  \hat{\bf {p}}_k^T \hat{\bf {A}}\hat{\bf {x}} = \sum^{n}_{i=1} \alpha_i\hat{\bf {p}}_k^T \hat{\bf {A}}\hat{\bf {p}}_i= \hat{\bf {p}}_k^T \hat{\bf {b}},
\]
and we can define the coefficients $\alpha_k$ as 
\[
    \alpha_k = \frac{\hat{\bf {p}}_k^T \hat{\bf {b}}}{\hat{\bf {p}}_k^T \hat{\bf {A}} \hat{\bf {p}}_k}
\] 
}
\end{small}
}

\frame
{
  \frametitle{Conjugate gradient method and iterations}
\begin{small}
{\scriptsize
If we choose the conjugate vectors $\hat{\bf {p}}_k$ carefully, 
then we may not need all of them to obtain a good approximation to the solution 
$\hat{\bf {x}}$. 
So, we want to regard the conjugate gradient method as an iterative method. 
This also allows us to solve systems where $n$ is so large that the direct 
method would take too much time.

We denote the initial guess for $\hat{\bf {x}}$ as $\hat{\bf {x}}_0$. 
We can assume without loss of generality that 
\[
\hat{\bf {x}}_0=0,
\]
or consider the system 
\[
\hat{\bf {A}}\hat{\bf {z}} = \hat{\bf {b}}-\hat{\bf {A}}\hat{\bf {x}}_0,
\]
instead.
}
\end{small}
}


\frame
{
  \frametitle{Conjugate gradient method}
\begin{small}
{\scriptsize
Important, one can show that the solution $\hat{\bf {x}}$ is also the unique minimizer of the quadratic form
\[
  f(\hat{\bf {x}}) = \frac{1}{2}\hat{\bf {x}}^T\hat{\bf {A}}\hat{\bf {x}} - \hat{\bf {x}}^T \hat{\bf {x}} , \quad \hat{\bf {x}}\in\mathbf{R}^n. 
\]
This suggests taking the first basis vector $\hat{\bf {p}}_1$ 
to be the gradient of $f$ at $\hat{\bf {x}}=\hat{\bf {x}}_0$, 
which equals 
\[
\hat{\bf {A}}\hat{\bf {x}}_0-\hat{\bf {b}},
\]
and 
$\hat{\bf {x}}_0=0$ it is equal $-\hat{\bf {b}}$.
The other vectors in the basis will be conjugate to the gradient, 
hence the name conjugate gradient method.
}
\end{small}
}


\frame
{
  \frametitle{Conjugate gradient method}
\begin{small}
{\scriptsize
Let  $\hat{\bf {r}}_k$ be the residual at the $k$-th step:
\[
\hat{\bf {r}}_k=\hat{\bf {b}}-\hat{\bf {A}}\hat{\bf {x}}_k.
\]

Note that $\hat{\bf {r}}_k$ is the negative gradient of $f$ at 
$\hat{\bf {x}}=\hat{\bf {x}}_k$, 
so the gradient descent method would be to move in the direction $\hat{\bf {r}}_k$. 
Here, we insist that the directions $\hat{\bf {p}}_k$ are conjugate to each other, 
so we take the direction closest to the gradient $\hat{\bf {r}}_k$  
under the conjugacy constraint. 
This gives the following expression
\[
\hat{\bf {p}}_{k+1}=\hat{\bf {r}}_k-\frac{\hat{\bf {p}}_k^T \hat{\bf {A}}\hat{\bf {r}}_k}{\hat{\bf {p}}_k^T\hat{\bf {A}}\hat{\bf {p}}_k} \hat{\bf {p}}_k.
\]
}
\end{small}
}

\frame
{
  \frametitle{Conjugate gradient method}
\begin{small}
{\scriptsize
We can also  compute the residual iteratively as
\[
\hat{\bf {r}}_{k+1}=\hat{\bf {b}}-\hat{\bf {A}}\hat{\bf {x}}_{k+1},
 \]
which equals
\[
\hat{\bf {b}}-\hat{\bf {A}}(\hat{\bf {x}}_k+\alpha_k\hat{\bf {p}}_k),
 \]
or
\[
(\hat{\bf {b}}-\hat{\bf {A}}\hat{\bf {x}}_k)-\alpha_k\hat{\bf {A}}\hat{\bf {p}}_k,
 \]
which gives
\[
\hat{\bf {r}}_{k+1}=\hat{\bf {r}}_k-\hat{\bf {A}}\hat{\bf {p}}_{k},
 \]
}
\end{small}
}



\frame
{
  \frametitle{Conjugate gradient method, our case}
\begin{small}
{\scriptsize
If we consider finding the minimum of a function $f$ using Newton's method,
that is search for a zero of the gradient of a function.  Near a point $x_i$
we have to second order
\[
f(\hat{\bf {x}})=f(\hat{\bf {x}}_i)+(\hat{\bf {x}}-\hat{\bf {x}}_i)\nabla f(\hat{\bf {x}}_i)
\frac{1}{2}(\hat{\bf {x}}-\hat{\bf {x}}_i)\hat{\bf {A}}(\hat{\bf {x}}-\hat{\bf {x}}_i)
\]
giving
\[
\nabla f(\hat{\bf {x}})=\nabla f(\hat{\bf {x}}_i)+\hat{\bf {A}}(\hat{\bf {x}}-\hat{\bf {x}}_i).
 \]
In Newton's method we set $\nabla f = 0$ and we can thus compute the next iteration point
(here the exact result)
\[
\hat{\bf {x}}-\hat{\bf {x}}_i=\hat{\bf {A}}^{-1}\nabla f(\hat{\bf {x}}_i).
\]
Subtracting this equation from that of $\hat{\bf {x}}_{i+1}$ we have
\[
\hat{\bf {x}}_{i+1}-\hat{\bf {x}}_i=\hat{\bf {A}}^{-1}(\nabla f(\hat{\bf {x}}_{i+1})-\nabla f(\hat{\bf {x}}_i)).
\]
}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{Codes from numerical recipes}
\begin{small}
{\scriptsize
The codes are taken from chapter 10.7 of Numerical recipes.  We use the functions
$dfpmin$ and $lnsrch$.  You can load down the package of programs from the webpage of
the course, see under project 1.  
The package is called $NRcgm107.tar.gz$ and contains the files 
$dfmin.c$, $lnsrch.c$, $nrutil.c$ and $nrutil.h$. 
These codes are  written in C. 
\begin{verbatim}

void dfpmin(double p[], int n, double gtol, int *iter, double *fret,
double(*func)(double []), void (*dfunc)(double [], double []))

\end{verbatim}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{What you have to provide}
\begin{small}
{\scriptsize
The input to $dfpmin$
\begin{verbatim}

void dfpmin(double p[], int n, double gtol, int *iter, double *fret,
double(*func)(double []), void (*dfunc)(double [], double []))

\end{verbatim}
is
\begin{itemize}
\item The starting vector $p$ of length $n$
\item The function $func$ on which minimization is done
\item The function $dfunc$ where the gradient i calculated
\item The convergence requirement for zeroing the gradient $gtol$.
\end{itemize}
It returns in $p$ the location of the minimum, the number of iterations and 
the minimum value of the function under study $fret$.
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Simple example and demonstration}
\begin{small}
{\scriptsize
For the harmonic oscillator in one-dimension with a trial wave function and probability
\[
\psi_T(x) = e^{-\alpha^2 x^2} \qquad ,P_T(x)dx = \frac{e^{-2\alpha^2 x^2}dx}{\int dx e^{-2\alpha^2 x^2}}
\]
with $\alpha$ as the variational parameter. 
We have the following local energy
\[
E_L[\alpha] = \alpha^2+x^2\left(\frac{1}{2}-2\alpha^2\right),
\]
which results in the expectation value
\[
\langle  E_L[\alpha]\rangle = \frac{1}{2}\alpha^2+\frac{1}{8\alpha^2}
\]
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Simple example and demonstration}
\begin{small}
{\scriptsize
The derivative of the energy with respect to $\alpha$ gives 
\[
\frac{d\langle  E_L[\alpha]\rangle}{d\alpha} = \alpha-\frac{1}{4\alpha^3}
\]
and a second derivative which is always positive (meaning that we find a minimum)
\[
\frac{d^2\langle  E_L[\alpha]\rangle}{d\alpha^2} = 1+\frac{3}{4\alpha^4}
\]
The condition 
\[
\frac{d\langle  E_L[\alpha]\rangle}{d\alpha} = 0,
\]
gives the optimal $\alpha=1/\sqrt{2}$.
}
\end{small}
}






\frame[containsverbatim]
{
  \frametitle{Simple example and demonstration}
\begin{small}
{\scriptsize
In general we end up computing the expectation value of the energy in terms 
of some parameters $\mathbf{\alpha}=\{\alpha_0,\alpha_1,\dots,\alpha_n\right})$
and we search for a minimum in parameter space.  
This leads to an energy minimization problem.

The elements of the gradient are ($Ei$ 
is the first derivative wrt to the variational parameter $\alpha_i$)
\beq
\Ei
&\!\!=&\!\! \left\langle \psiibypsi \EL + { H \psii \over \psi}
-2 \Ebar \psiibypsi \right\rangle
\label{first_deriv_nonherm}
\\
&\!\!=&\!\! 2\left\langle \psiibypsi (\EL - \Ebar) \right\rangle
\;\;\;\; {\rm (by\;Hermiticity)}.
\label{first_deriv}
\eeq
For our simple model we get the same expression for the first 
derivative (check it!).
}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{Simple example and demonstration}
\begin{small}
{\scriptsize
Taking the second derivative the Hessian is
\beq
\Ebar_{ij}
&=& 2 \Bigg[
\left\langle \left( \psiijbypsi + {\psii\psij\over \psisq} \right) (\EL-\Ebar) \right\rangle \nonumber \\
&&
-\left\langle \psiibypsi \right\rangle \Ej
-\left\langle \psijbypsi \right\rangle \Ei
+ \left\langle \psiibypsi \ELj \right\rangle \Bigg].
\label{rappe}
\eeq
Note that our conjugate gradient approach does need the Hessian!
Check again that the simple models gives the same second derivative with the
above expression.
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{Simple example and demonstration}
\begin{small}
{\scriptsize
We can also minimize the variance. In our simple model the variance is
\[
\sigma^2[\alpha] = \frac{1}{2}\alpha^4-\frac{1}{4}+\frac{1}{32\alpha^4},
\]
with first derivative 
\[
\frac{d \sigma^2[\alpha]}{d\alpha} = 2\alpha^3-\frac{1}{8\alpha^5}
\]
and a second derivative which is always positive 
\[
\frac{d^2\sigma^2[\alpha]}{d\alpha^2} = 6\alpha^2+\frac{5}{8\alpha^6}
\]
}
\end{small}
}




\frame
{
  \frametitle{Conjugate gradient method, our case}
\begin{small}
{\scriptsize
In Newton's method we set $\nabla f = 0$ and we can thus compute the next iteration point
(here the exact result)
\[
\hat{\bf {x}}-\hat{\bf {x}}_i=\hat{\bf {A}}^{-1}\nabla f(\hat{\bf {x}}_i).
\]
Subtracting this equation from that of $\hat{\bf {x}}_{i+1}$ we have
\[
\hat{\bf {x}}_{i+1}-\hat{\bf {x}}_i=\hat{\bf {A}}^{-1}(\nabla f(\hat{\bf {x}}_{i+1})-\nabla f(\hat{\bf {x}}_i)).
\]
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Simple example and demonstration}
\begin{small}
{\scriptsize
In our case $f$ can be either the energy or the variance.  If we choose the energy then we have 
\[
\hat{\bf {\alpha}}_{i+1}-\hat{\bf {\alpha}}_i=\hat{\bf {A}}^{-1}(\nabla E(\hat{\bf {\alpha}}_{i+1})-\nabla E(\hat{\bf {\alpha}}_i)).
\]
In the simple model gradient and the Hessian $\hat{\bf A}}$ are 
\[
\frac{d\langle  E_L[\alpha]\rangle}{d\alpha} = \alpha-\frac{1}{4\alpha^3}
\]
and a second derivative which is always positive (meaning that we find a minimum)
\[
\hat{\bf A}}= \frac{d^2\langle  E_L[\alpha]\rangle}{d\alpha^2} = 1+\frac{3}{4\alpha^4}
\]
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{Simple example and demonstration}
\begin{small}
{\scriptsize
We get then
\[
\alpha_{i+1}=\frac{4}{3}\alpha_i-\frac{\alpha_i^4}{3\alpha_{i+1}^3},
\]
which can be rewritten as
\[
\alpha_{i+1}^4-\frac{4}{3}\alpha_i\alpha_{i+1}^4+\frac{1}{3}\alpha_i^4.
\]
Our code does however not need the value of the Hessian since it 
produces an estimate of the Hessian.
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Simple example and code (model.cpp on webpage)}
\begin{small}
{\scriptsize
\begin{verbatim}
#include "nrutil.h"
using namespace std;
//     Here we define various functions called by the main program

double E_function(double *x);
void   dE_function(double *x, double *g);
void   dfpmin(double p[], int n, double gtol, int *iter, double *fret,
	    double(*func)(double []), void (*dfunc)(double [], double []));
//   Main function begins here
int main()
{
     int n, iter;
     double gtol, fret;
     double alpha;
     n = 1;
     cout << "Read in guess for alpha" << endl;
     cin >> alpha;
\end{verbatim}
}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{Simple example and code (model.cpp on webpage)}
\begin{small}
{\scriptsize
\begin{verbatim}
//   reserve space in memory for vectors containing the variational
//   parameters
     double *p = new double [2];
     gtol = 1.0e-5;
//   now call dfmin and compute the minimum
     p[1] = alpha;
     dfpmin(p, n, gtol, &iter, &fret,&E_function,&dE_function);
     cout << "Value of energy minimum = " << fret << endl;
     cout << "Number of iterations = " << iter << endl;
     cout << "Value of alpha at minimu = " << p[1] << endl;
      delete [] p;
\end{verbatim}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Simple example and code (model.cpp on webpage)}
\begin{small}
{\scriptsize
\begin{verbatim}
//  this function defines the Energy function
double E_function(double x[])
{
  double value = x[1]*x[1]*0.5+1.0/(8*x[1]*x[1]);
  return value;
} // end of function to evaluate
\end{verbatim}
}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{Simple example and code (model.cpp on webpage)}
\begin{small}
{\scriptsize
\begin{verbatim}
//  this function defines the derivative of the energy 
void dE_function(double x[], double g[])
{
  g[1] = x[1]-1.0/(4*x[1]*x[1]*x[1]);
} // end of function to evaluate
\end{verbatim}
}
\end{small}
}




\frame
{
  \frametitle{Using the conjugate gradient method}
\begin{small}
{\scriptsize
\begin{itemize}
\item Start your program with calling the CGM method (function $dfpmin$).
\item This function needs the function for the expectation value of the local energy and
the derivative of the local energy.  Change the functions $func$ and $dfunc$ in the codes below.
\item Your function $func$ is now the Metropolis part with a call to the local energy function.
For every call to the function $func$ I used 1000 Monte Carlo cycles for the trial wave function
\[
   \Psi_T(\mathbf{r}_1,\mathbf{r}_2) = e^{-\alpha(r_1+r_2)}
\]
\item This gave me an expectation value for the energy which is returned by the function $func$.
\item When I call the local energy I also compute the first derivative of the expectaction value of the local energy
\[
\frac{d\langle E_{L}[\alpha] \rangle}{d\alpha}=
2\left\langle \psiibypsi (E_{L}[\alpha] - \langle E_{L}[\alpha] \rangle) \right\rangle.
\]
\end{itemize}
}
\end{small}
}



\frame
{
  \frametitle{Using the conjugate gradient method}
\begin{small}
{\scriptsize
The expectation value for the local energy of the Helium atom with a simple Slater determinant is given by
\[
\langle E_{L} \rangle = \alpha^2-2\alpha\left(Z-\frac{5}{16}\right)
\]
You should test your numerical derivative with the derivative of the last expression, that is
\[
\frac{d\langle E_{L}[\alpha] \rangle}{d\alpha} = 2\alpha-2\left(Z-\frac{5}{16}\right).
\]
}
\end{small}
}




\frame[containsverbatim]
{
  \frametitle{Simple example and code (model.cpp on webpage)}
\begin{small}
{\scriptsize
\begin{verbatim}
#include "nrutil.h"
using namespace std;
//     Here we define various functions called by the main program

double E_function(double *x);
void   dE_function(double *x, double *g);
void   dfpmin(double p[], int n, double gtol, int *iter, double *fret,
	    double(*func)(double []), void (*dfunc)(double [], double []));
//   Main function begins here
int main()
{
     int n, iter;
     double gtol, fret;
     double alpha;
     n = 1;
     cout << "Read in guess for alpha" << endl;
     cin >> alpha;
\end{verbatim}
}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{Simple example and code (model.cpp on webpage)}
\begin{small}
{\scriptsize
\begin{verbatim}
//   reserve space in memory for vectors containing the variational
//   parameters
     double *p = new double [2];
     gtol = 1.0e-5;
//   now call dfmin and compute the minimum
     p[1] = alpha;
     dfpmin(p, n, gtol, &iter, &fret,&E_function,&dE_function);
     cout << "Value of energy minimum = " << fret << endl;
     cout << "Number of iterations = " << iter << endl;
     cout << "Value of alpha at minimu = " << p[1] << endl;
      delete [] p;
\end{verbatim}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Simple example and code (model.cpp on webpage)}
\begin{small}
{\scriptsize
\begin{verbatim}
//  this function defines the Energy function
double E_function(double x[])
{

//  Change here by calling your Metropolis function which 
//  returns the local energy

  double value = x[1]*x[1]*0.5+1.0/(8*x[1]*x[1]);



  return value;
} // end of function to evaluate
\end{verbatim}
You need to change this function so that you call the local energy for your system. I used 1000
cycles per call to get a new value of $\langle E_L[\alpha]\rangle$.

}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{Simple example and code (model.cpp on webpage)}
\begin{small}
{\scriptsize
\begin{verbatim}
//  this function defines the derivative of the energy 
void dE_function(double x[], double g[])
{

//  Change here by calling your Metropolis function. 
//  I compute both the local energy and its derivative for every call to func

  g[1] = x[1]-1.0/(4*x[1]*x[1]*x[1]);
} // end of function to evaluate
\end{verbatim}
You need to change this function so that you call the local energy for your system. I used 1000
cycles per call to get a new value of $\langle E_L[\alpha]\rangle$.
When I compute the local energy I also compute its derivative.
After roughly 10-20 iterations I got a converged result in terms of $\alpha$.
}
\end{small}
}
\section[Week 13]{Week 13}
\frame
{
  \frametitle{Topics for Week 13, March 28- April 1}
  \begin{block}{Slater determinant and programming strategies}
\begin{itemize}
\item Repetition from last week
\item How to program the Conjugate gradient method, see code qdotsclass.cpp
\item Many electrons and Slater determinant
\item How to implement the Slater determinant
\end{itemize}
Project work this week: finalize 1e and start programming Slater determinant. 
  \end{block}
} 



\frame
{
  \frametitle{Slater determinants}
\begin{small}
{\scriptsize
The potentially most time-consuming part is the
evaluation of the gradient and the Laplacian of an $N$-particle Slater
determinant. We have to differentiate the determinant with respect to
all spatial coordinates of all particles. A brute force
differentiation would involve $N\cdot d$ evaluations of the entire
determinant which would even worsen the already undesirable time
scaling, making it $Nd\cdot\bigO(N^3)\sim \bigO(d\cdot N^4)$.
This poses serious hindrances to the overall efficiency of our code.

The efficiency can be improved however if we move only one electron at the time.
The Slater determinant matrix $\matr D$ is defined by the matrix elements
\be
d_{ij}\equiv\phi_j(x_i)
\ee
where $\phi_j(\mathbf{r}_i)$ is a single particle wave function.
The columns correspond to the position of a given particle
while the rows stand for the various quantum numbers.
}
\end{small}
}

\frame
{
  \frametitle{Slater determinants}
\begin{small}
{\scriptsize
What we need to realize is that when differentiating a Slater
determinant with respect to some given coordinate, only one row of the
corresponding Slater matrix is changed. Therefore, by recalculating
the whole determinant we risk producing redundant information. The
solution turns out to be an algorithm that requires to keep track of
the \emph{inverse} of the Slater matrix.

Let the
current position in phase space be represented by the $(N\cdot
d)$-element vector $\mathbf{r}^{\mathrm{old}}$ and the new suggested
position by the vector $\mathbf{r}^{\mathrm{new}}$.


The inverse of $\matr D$ can be expressed in terms of its
cofactors $C_{ij}$ and its determinant $\det{\matr D}$:
\be
d_{ij}^{-1} = \frac{C_{ji}}{\det{\matr D}}
\label{eq:inverse_cofactor}
\ee
Notice that the interchanged indices indicate that the matrix of
cofactors is to be transposed.
}
\end{small}
}

\frame
{
  \frametitle{Slater determinants}
\begin{small}
{\scriptsize
If $\matr D$ is invertible, then we must obviously have $\matr
D^{-1}\matr D= {\bf 1}$, or explicitly in terms of the individual
elements of $\matr D$ and $\matr D^{-1}$:
\be
\sum_{k=1}^N d_{ik}^{\phantom X}d_{kj}^{-1} = \delta_{ij}^{\phantom X}
\label{eq:unity_explicitely}
\ee
Consider the ratio, which we shall call $R$, between $\det{\matr
  D(\mathbf{r}^{\mathrm{new}})}$ and $\det{\matr D(\mathbf{r}^{\mathrm{old}})}$. 
By definition, each of these determinants can
individually be expressed in terms of the $i$th row of its cofactor
matrix
\be
R\equiv\frac{\det{\matr D(\mathbf{r}^{\mathrm{new}})}}
{\det{\matr D(\mathbf{r}^{\mathrm{old}})}} =
\frac{\sum_{j=1}^N d_{ij}(\mathbf{r}^{\mathrm{new}})\,
C_{ij}(\mathbf{r}^{\mathrm{new}})}
{\sum_{j=1}^N d_{ij}(\mathbf{r}^{\mathrm{old}})\,
C_{ij}(\mathbf{r}^{\mathrm{old}})}
\label{eq:detratio_cofactors}
\ee
}
\end{small}
}

\frame
{
  \frametitle{Slater determinants}
\begin{small}
{\scriptsize
Suppose now that we move only one particle at a time, meaning that
$\mathbf{r}^{\mathrm{new}}$ differs from $\mathbf{r}^{\mathrm{old}}$ by the
position of only one, say the $i$th, particle. This means that $\matr
D(\mathbf{r}^{\mathrm{new}})$ and $\matr D(\mathbf{r}^{\mathrm{old}})$ differ
only by the entries of the $i$th row.  Recall also that the $i$th row
of a cofactor matrix $\matr C$ is independent of the entries of the
$i$th row of its corresponding matrix $\matr D$. In this particular
case we therefore get that the $i$th row of $\matr C(\mathbf{r}^{\mathrm{new}})$ 
and $\matr C(\mathbf{r}^{\mathrm{old}})$ must be
equal. Explicitly, we have:
\be
C_{ij}(\mathbf{r}^{\mathrm{new}}) = C_{ij}(\mathbf{r}^{\mathrm{old}})\quad
\forall\ j\in\{1,\dots,N\}
\ee
}
\end{small}
}

\frame
{
  \frametitle{Slater determinants}
\begin{small}
{\scriptsize
Inserting this into the numerator of eq.~(\ref{eq:detratio_cofactors})
and using eq.~(\ref{eq:inverse_cofactor}) to substitute the cofactors
with the elements of the inverse matrix, we get:
\be
%\frac{\det{\matr D(\mathbf{r}^{\mathrm{new}})}}
%{\det{\matr D(\mathbf{r}^{\mathrm{old}})}}
R =
\frac{\sum_{j=1}^N d_{ij}(\mathbf{r}^{\mathrm{new}})\,
C_{ij}(\mathbf{r}^{\mathrm{old}})}
{\sum_{j=1}^N d_{ij}(\mathbf{r}^{\mathrm{old}})\,
C_{ij}(\mathbf{r}^{\mathrm{old}})} =
\frac{\sum_{j=1}^N d_{ij}(\mathbf{r}^{\mathrm{new}})\,
d_{ji}^{-1}(\mathbf{r}^{\mathrm{old}})}
{\sum_{j=1}^N d_{ij}(\mathbf{r}^{\mathrm{old}})\,
d_{ji}^{-1}(\mathbf{r}^{\mathrm{old}})}
\ee
}
\end{small}
}

\frame
{
  \frametitle{Slater determinants}
\begin{small}
{\scriptsize
Now by eq.~(\ref{eq:unity_explicitely}) the denominator of the rightmost
expression must be unity, so that we finally arrive at:
\be
R =
%\frac{\det{\matr D(\mathbf{r}^{\mathrm{new}})}}
%{\det{\matr D(\mathbf{r}^{\mathrm{old}})}} =
\sum_{j=1}^N d_{ij}(\mathbf{r}^{\mathrm{new}})\,
d_{ji}^{-1}(\mathbf{r}^{\mathrm{old}}) = 
\sum_{j=1}^N \phi_j(\mathbf{r}_i^{\mathrm{new}})\,
d_{ji}^{-1}(\mathbf{r}^{\mathrm{old}})
\label{eq:detratio_inverse}
\ee
What this means is that in order to get the ratio when only the $i$th
particle has been moved, we only need to calculate the dot
product of the vector $\left(\phi_1(\mathbf{r}_i^\mathrm{new}),\,\dots,\,
\phi_N(\mathbf{r}_i^\mathrm{new})\right)$ of single particle wave functions
evaluated at this new position with the $i$th column of the inverse
matrix $\matr D^{-1}$ evaluated at the original position. Such
an operation has a time scaling of $\bigO(N)$. The only extra thing we
need to do is to maintain the inverse matrix $\matr D^{-1}(\vec
x^{\mathrm{old}})$.
}
\end{small}
}

\frame
{
  \frametitle{Slater determinants}
\begin{small}
{\scriptsize
If the new position $\mathbf{r}^{\mathrm{new}}$ is accepted, then the
inverse matrix can by suitably updated by an algorithm having a time
scaling of $\bigO(N^2)$.  This algorithm goes as
follows. First we update all but the $i$th column of $\matr
D^{-1}$. For each column $j\neq i$, we first calculate the quantity:
\be
S_j =
(\matr D(\mathbf{r}^{\mathrm{new}})\times
\matr D^{-1}(\mathbf{r}^{\mathrm{old}}))_{ij} =
\sum_{l=1}^N d_{il}(\mathbf{r}^{\mathrm{new}})\,
d^{-1}_{lj}(\mathbf{r}^{\mathrm{old}})
\label{eq:inverse_update_1}
\ee
The new elements of the $j$th column of $\matr D^{-1}$ are then given
by:
\be
d_{kj}^{-1}(\mathbf{r}^{\mathrm{new}}) =
d_{kj}^{-1}(\mathbf{r}^{\mathrm{old}}) -
\frac{S_j}{R}\,d_{ki}^{-1}(\mathbf{r}^{\mathrm{old}})\quad
\begin{array}{ll}
\forall\ \ k\in\{1,\dots,N\}\\j\neq i
\end{array}
\label{eq:inverse_update_2}
\ee
}
\end{small}
}

\frame
{
  \frametitle{Slater determinants}
\begin{small}
{\scriptsize
Finally the elements of the $i$th column of $\matr D^{-1}$ are updated
simply as follows:
\be
d_{ki}^{-1}(\mathbf{r}^{\mathrm{new}}) =
\frac{1}{R}\,d_{ki}^{-1}(\mathbf{r}^{\mathrm{old}})\quad
\forall\ \ k\in\{1,\dots,N\}
\label{eq:inverse_update_3}
\ee
We see from these formulas that the time scaling of an update of
$\matr D^{-1}$ after changing one row of $\matr D$ is $\bigO(N^2)$.
}
\end{small}
}


\frame
{
  \frametitle{Slater determinants}
\begin{small}
{\scriptsize
The scheme is also applicable for the calculation of the ratios
involving derivatives. It turns
out that differentiating the Slater determinant with respect
to the coordinates of a single particle $\mathbf{r}_i$ changes only the
$i$th row of the corresponding Slater matrix. 
}
\end{small}
}




\frame
{
  \frametitle{Slater determinants}
\begin{small}
{\scriptsize
The gradient and
Laplacian can therefore be calculated as follows:
\be
\frac{\vec\nabla_i\det{\matr D(\mathbf{r})}}
{\det{\matr D(\mathbf{r})}} =
\sum_{j=1}^N \vec\nabla_i d_{ij}(\mathbf{r})\,
d_{ji}^{-1}(\mathbf{r}) =
\sum_{j=1}^N \vec\nabla_i \phi_j(\mathbf{r}_i)\,
d_{ji}^{-1}(\mathbf{r})
\ee
and
\be
\frac{\nabla^2_i\det{\matr D(\mathbf{r})}}
{\det{\matr D(\mathbf{r})}} =
\sum_{j=1}^N \nabla^2_i d_{ij}(\mathbf{r})\,
d_{ji}^{-1}(\mathbf{r}) =
\sum_{j=1}^N \nabla^2_i \phi_j(\mathbf{r}_i)\,
d_{ji}^{-1}(\mathbf{r})
\ee
}
\end{small}
}

\frame
{
  \frametitle{Slater determinants}
\begin{small}
{\scriptsize
Thus, to calculate all the derivatives of the Slater determinant, we
only need the derivatives of the single particle wave functions
($\vec\nabla_i \phi_j(\mathbf{r}_i)$ and $\nabla^2_i \phi_j(\mathbf{r}_i)$)
and the elements of the corresponding inverse Slater matrix ($\matr
D^{-1}(\mathbf{r}_i)$). A calculation of a single derivative is by the
above result an $\bigO(N)$ operation. Since there are $d\cdot N$
derivatives, the time scaling of the total evaluation becomes
$\bigO(d\cdot N^2)$. With an $\bigO(N^2)$ updating algorithm for the
inverse matrix, the total scaling is no worse, which is far better
than the brute force approach yielding $\bigO(d\cdot N^4)$.\newline
{\bf Important note:} In most cases you end with closed form expressions for the single-particle wave functions. It is then useful to calculate the various derivatives and make separate functions
for them.

}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{Slater determinant: Explicit expressions for various Atoms, beryllium}
\begin{small}
{\scriptsize
The Slater determinant takes the form  
\[
   \Phi({\bf r}_1,{\bf r}_2,,{\bf r}_3,{\bf r}_4, \alpha,\beta,\gamma,\delta)=\frac{1}{\sqrt{4!}}
\left| \begin{array}{cccc} \psi_{100\uparrow}({\bf r}_1)& \psi_{100\uparrow}({\bf r}_2)& \psi_{100\uparrow}({\bf r}_3)&\psi_{100\uparrow}({\bf r}_4) \\
\psi_{100\downarrow}({\bf r}_1)& \psi_{100\downarrow}({\bf r}_2)& \psi_{100\downarrow}({\bf r}_3)&\psi_{100\downarrow}({\bf r}_4) \\
\psi_{200\uparrow}({\bf r}_1)& \psi_{200\uparrow}({\bf r}_2)& \psi_{200\uparrow}({\bf r}_3)&\psi_{200\uparrow}({\bf r}_4) \\
\psi_{200\downarrow}({\bf r}_1)& \psi_{200\downarrow}({\bf r}_2)& \psi_{200\downarrow}({\bf r}_3)&\psi_{200\downarrow}({\bf r}_4) \end{array} \right|.
\]

The Slater determinant as written is zero since the spatial wave functions for the spin up and spin down 
states are equal.  
But we can rewrite it as the product of two Slater determinants, one for spin up and one for spin down.
 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Slater determinant: Explicit expressions for various Atoms, beryllium}
\begin{small}
{\scriptsize
We can rewrite it as 
\[
   \Phi({\bf r}_1,{\bf r}_2,,{\bf r}_3,{\bf r}_4, \alpha,\beta,\gamma,\delta)=Det\uparrow(1,2)Det\downarrow(3,4)-
Det\uparrow(1,3)Det\downarrow(2,4)
\]
\[
-Det\uparrow(1,4)Det\downarrow(3,2)+Det\uparrow(2,3)Det\downarrow(1,4)-Det\uparrow(2,4)Det\downarrow(1,3)
\]
\[
+Det\uparrow(3,4)Det\downarrow(1,2),
\]
where we have defined
\[
Det\uparrow(1,2)=\frac{1}{\sqrt{2}}\left| \begin{array}{cc} \psi_{100\uparrow}({\bf r}_1)& \psi_{100\uparrow}({\bf r}_2)\\
\psi_{200\uparrow}({\bf r}_1)& \psi_{200\uparrow}({\bf r}_2) \end{array} \right|,
\]
and 
\[
Det\downarrow(3,4)=\frac{1}{\sqrt{2}}\left| \begin{array}{cc} \psi_{100\downarrow}({\bf r}_3)& \psi_{100\downarrow}({\bf r}_4)\\
\psi_{200\downarrow}({\bf r}_3)& \psi_{200\downarrow}({\bf r}_4) \end{array} \right|.
\]

The total determinant is still zero!
 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Slater determinant: Explicit expressions for various Atoms, beryllium}
\begin{small}
{\scriptsize

We want to avoid to sum over spin variables, in particular when the interaction does not depend on spin.

It can be shown, see for example Moskowitz and Kalos, Int.~J.~Quantum Chem.~{\bf 20} (1981) 1107, that for the variational energy
we can approximate the Slater determinant as  
\[
   \Phi({\bf r}_1,{\bf r}_2,,{\bf r}_3,{\bf r}_4, \alpha,\beta,\gamma,\delta) \propto Det\uparrow(1,2)Det\downarrow(3,4),
\]
or more generally as 
\[
   \Phi({\bf r}_1,{\bf r}_2,\dots {\bf r}_N) \propto Det\uparrow Det\downarrow,
\]
where we have the Slater determinant as the product of a spin up part involving the number of electrons with spin up only (3 for six-electron QD
and 6 in 12-electron QD) and a spin down part involving the electrons with spin down.

This ansatz is not antisymmetric under the exchange of electrons with  opposite spins but it can be shown that it gives the same
expectation value for the energy as the full Slater determinant.

As long as the Hamiltonian is spin independent, the above is correct. Exercise for next week: convince yourself that this is correct.
 }
 \end{small}
 }



\frame
{
  \frametitle{Slater determinants}
\begin{small}
{\scriptsize
We will thus
factorize the full determinant $\det{\matr D}$ into two smaller ones, where 
each can be identified with $\uparrow$ and $\downarrow$
respectively:
\be
\det{\matr D} = \det{\matr D}_\uparrow\cdot\det{\matr D}_\downarrow
\ee
The combined dimensionality of the two smaller determinants equals the
dimensionality of the full determinant. Such a factorization is
advantageous in that it makes it possible to perform the calculation
of the ratio $R$ and the updating of the inverse matrix separately for
$\det{\matr D}_\uparrow$ and $\det{\matr D}_\downarrow$:
\be
\frac{\det{\matr D}^\mathrm{new}}{\det{\matr D}^\mathrm{old}} =
\frac{\det{\matr D}^\mathrm{new}_\uparrow}
{\det{\matr D}^\mathrm{old}_\uparrow}\cdot
\frac{\det{\matr D}^\mathrm{new}_\downarrow
}{\det{\matr D}^\mathrm{old}_\downarrow}
\ee
}
\end{small}
}

\frame
{
  \frametitle{Slater determinants}
\begin{small}
{\scriptsize
This reduces the calculation time by a constant factor. The maximal
time reduction happens in a system of equal numbers of $\uparrow$ and
$\downarrow$ particles, so that the two factorized determinants are
half the size of the original one.


Consider the case of moving only one particle at a time which
originally had the following time scaling for one transition:
\be
\bigO_R(N)+\bigO_\mathrm{inverse}(N^2)
\ee
For the factorized determinants one of the two determinants is
obviously unaffected by the change so that it cancels from the ratio
$R$. 
}
\end{small}
}

\frame
{
  \frametitle{Slater determinants}
\begin{small}
{\scriptsize
Therefore, only one determinant of size $N/2$ is involved in each
calculation of $R$ and update of the inverse matrix. The scaling of
each transition then becomes:
\be
\bigO_R(N/2)+\bigO_\mathrm{inverse}(N^2/4)
\ee
and the time scaling when the transitions for all $N$ particles are
put together:
\be
\bigO_R(N^2/2)+\bigO_\mathrm{inverse}(N^3/4)
\ee
which gives the same reduction as in the case of moving all particles
at once.
}
\end{small}
}


\frame
{
  \frametitle{Updating the Slater matrix}
\begin{small}
{\scriptsize
Computing the ratios discussed above requires that we maintain 
the inverse of the Slater matrix evaluated at the current position. 
Each time a trial position is accepted, the row number $i$ of the Slater 
matrix changes and updating its inverse has to be carried out. 
Getting the inverse of an $N \times N$ matrix by Gaussian elimination has a 
complexity of order of $\mathcal{O}(N^3)$ operations, a luxury that we 
cannot afford for each time a particle move is accepted.
We will use the expression
\begin{eqnarray}\label{updatingInverse}
\boxed{d^{-1}_{kj}(\bfv{x^{new}})  = \left\{ 
\begin{array}{l l}
  d^{-1}_{kj}(\bfv{x^{old}}) - \frac{d^{-1}_{ki}(\bfv{x^{old}})}{R} \sum_{l=1}^{N} d_{il}(\bfv{x^{new}})  d^{-1}_{lj}(\bfv{x^{old}}) & \mbox{if $j \neq i$}\nonumber \\ \\
 \frac{d^{-1}_{ki}(\bfv{x^{old}})}{R} \sum_{l=1}^{N} d_{il}(\bfv{x^{old}}) d^{-1}_{lj}(\bfv{x^{old}}) & \mbox{if $j=i$}
\end{array} \right.}\\
\end{eqnarray}
}
\end{small}
}


\frame
{
  \frametitle{Updating the Slater matrix}
\begin{small}
{\scriptsize
This equation scales as $O(N^2)$.
The evaluation of the determinant of an $N \times N$ matrix by standard Gaussian elimination requires ${\cal O}(N^3)$
calculations. 
As there are $Nd$ independent coordinates we need to evaluate $Nd$ Slater determinants 
for the gradient (quantum force) and $Nd$ for the Laplacian (kinetic energy). 
With the updating algorithm we need only to invert the Slater 
determinant matrix once. This can be done by standard LU decomposition methods.\\

}
\end{small}
}





\frame
{
  \frametitle{Slater Determinant and  VMC}
\begin{small}
{\scriptsize

Determining a determinant of an $N \times N$ matrix by
standard Gaussian elimination is of the order of ${\cal O}(N^3)$
calculations. As there are $N\cdot d$ independent coordinates we need
to evaluate $Nd$ Slater determinants for the gradient (quantum force) and
$N\cdot d$ for the Laplacian (kinetic energy)

With the updating algorithm we need only to invert the Slater determinant matrix once.
This is done by calling standard LU decomposition methods.
}
\end{small}
}





\frame
{
  \frametitle{How to compute the Slater Determinant}
\begin{small}
{\scriptsize
If you choose to implement the above recipe for the computation of the Slater determinant,
you need to LU decompose the Slater matrix. This is described in chapter 4 of the lecture notes.

You need to call the function ludcmp in lib.cpp.
You need to transfer the Slater matrix and its dimension. You get back an LU decomposed matrix.
}
\end{small}
}

\frame
{
  \frametitle{LU Decomposition}
\begin{small}
{\scriptsize
The LU decomposition method means that we can rewrite
this matrix as the product of two matrices ${\bf B}$ and ${\bf C}$
where 
\[
\label{eq3}
   \left(\begin{array}{cccc}
                          a_{11} & a_{12} & a_{13} & a_{14} \\
                          a_{21} & a_{22} & a_{23} & a_{24} \\
                          a_{31} & a_{32} & a_{33} & a_{34} \\
                          a_{41} & a_{42} & a_{43} & a_{44} 
                      \end{array} \right)
                      = \left( \begin{array}{cccc}
                              1  & 0      & 0      & 0 \\
                          b_{21} & 1      & 0      & 0 \\
                          b_{31} & b_{32} & 1      & 0 \\
                          b_{41} & b_{42} & b_{43} & 1 
                      \end{array} \right) 
                        \left( \begin{array}{cccc}
                          c_{11} & c_{12} & c_{13} & c_{14} \\
                               0 & c_{22} & c_{23} & c_{24} \\
                               0 & 0      & c_{33} & c_{34} \\
                               0 & 0      &  0     & c_{44} 
             \end{array} \right).
\]
The matrix ${\bf A}\in \mathbb{R}^{n\times n}$ has an LU factorization if the determinant 
is different from zero. If the LU factorization exists and ${\bf A}$ is non-singular, then the LU factorization
is unique and the determinant is given by 
\[
det\{{\bf A}\}
  = c_{11}c_{22}\dots c_{nn}.
\]
}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{How should we structure our code?}
What do you think is reasonable to split into subtasks defined by classes?
\begin{itemize}
\item Single-particle wave functions?
\item External potentials?
\item Operations on $r_{ij}$ and the correlation function?
\item Mathematical operations like the first and second derivative of the
trial wave function? How can you split the derivatives into various subtasks?
\item Matrix and vector operations? 
\end{itemize}
Your task is to figure out how to structure your code in order
to compute the Slater determinant for the six electron dot. This should be compared with the brute force case. Do not include the correlation factor in the first attempt nor the electron-electron repulsion. 
}


\frame[containsverbatim]
{
  \frametitle{A useful piece of code, distances}
\begin{lstlisting}
double r_i(double**, int); 
//distance between  nucleus and electron i
double r_ij(double**, int, int); 
//distance between electrons i and j
\end{lstlisting}
You should also make functions for the single-particle wave functions, their first and second derivatives as well.
}


\frame[containsverbatim]
{
  \frametitle{The function to set up a determinant}
\begin{lstlisting}
//Determinant function
double determinant(double** A, int dim) {
  if (dim == 2)
    return A[0][0]*A[1][1] - A[0][1]*A[1][0];
  double sum = 0;
  for (int i = 0; i < dim; i++) {
    double** sub = new double*[dim-1];
    for (int j = 0; j < i; j++)
      sub[j] = &A[j][1];
    for (int j = i+1; j < dim; j++)
      sub[j-1] = &A[j][1];
    if(i % 2 == 0)
      sum += A[i][0] * determinant(sub,dim-1);
    else
      sum -= A[i][0] * determinant(sub,dim-1);

    delete[] sub;
  }
  return sum;
}
\end{lstlisting}

}


\frame[containsverbatim]
{
  \frametitle{Set up the Slater determinant}
$N$ is the number of electrons and $N2$ is half the number of electrons.
\begin{lstlisting}
//Slater-determinant
double slater(double** R, double alpha, double N, double N2) {
  double** DUp = (double**) matrix(N2,N2,sizeof(double));
  double** DDown = (double**) matrix(N2,N2,sizeof(double));
  for (int i = 0; i < N2; i++) {
    for (int j = 0; j < N2; j++) {
      DUp[i][j] = phi(j,R,i,alpha);
      DDown[i][j] = phi(j,R,i+N2,alpha);
    }
  }
  //Returns product of spin up and spin down dets
  double det = determinant(DUp,N2)*determinant(DDown,N2);
  free_matrix((void**) DUp);
  free_matrix((void**) DDown);
  return det;
}
\end{lstlisting}

}


\frame[containsverbatim]
{
  \frametitle{Jastrow factor}
\begin{lstlisting}
//Jastrow factor
double jastrow(double** R, double beta, double N, double N2) {
  double arg = 0;
  for (int i = 1; i < N; i++)
    for (int j = 0; j < i; j++)
      if ((i < N2 && j < N2) || (i >= N2 && j >= N2)) {
	double rij = r_ij(R,i,j);
	arg += 0.33333333*rij / (1+beta*rij); //same spin
      }
      else {
	double rij = r_ij(R,i,j);
	arg += 1.0*rij / (1+beta*rij); //opposite spin
      }
  return exp(arg);
}
\end{lstlisting}

}


\frame[containsverbatim]
{
  \frametitle{}
\begin{lstlisting}
//Check of singularity at R = 0
bool Singularity(double** R, int N) {

  for (int i = 0; i < N; i++)
    if (r_i(R,i) < 1e-10)
      return true;

  for (int i = 0; i < N - 1; i++)
    for (int j = i+1; j < N; j++)
      if (r_ij(R,i,j) < 1e-10)
	return true;
  return false;
}
\end{lstlisting}

}


\frame
 {
   \frametitle{Efficient calculations of wave function ratios}
 \begin{small}
 {\scriptsize

The expectation value of the kinetic energy expressed in atomic units for electron $i$ is 
\begin{equation}
 \langle \Op{K}_i \rangle = -\frac{1}{2}\frac{\langle\Psi|\nabla_{i}^2|\Psi \rangle}{\langle\Psi|\Psi \rangle},
\end{equation}

\begin{equation}\label{kineticE}
K_i = -\frac{1}{2}\frac{\nabla_{i}^{2} \Psi}{\Psi}.
\end{equation}
\begin{eqnarray}
\frac{\nabla^2 \Psi}{\Psi} & = & \frac{\nabla^2 ({\Psi_{D} \,  \Psi_C})}{\Psi_{D} \,  \Psi_C} = \frac{\Grad \cdot [\Grad {(\Psi_{D} \,  \Psi_C)}]}{\Psi_{D} \,  \Psi_C} = \frac{\Grad \cdot [ \Psi_C \Grad \Psi_{D} + \Psi_{D} \Grad  \Psi_C]}{\Psi_{D} \,  \Psi_C}\nonumber\\
&  = & \frac{\Grad  \Psi_C \cdot \Grad \Psi_{D} +  \Psi_C \nabla^2 \Psi_{D} + \Grad \Psi_{D} \cdot \Grad  \Psi_C + \Psi_{D} \nabla^2  \Psi_C}{\Psi_{D} \,  \Psi_C}\nonumber\\
\end{eqnarray}
\begin{eqnarray}
\frac{\nabla^2 \Psi}{\Psi}
& = & \frac{\nabla^2 \Psi_{D}}{\Psi_{D}} + \frac{\nabla^2  \Psi_C}{ \Psi_C} + 2 \frac{\Grad \Psi_{D}}{\Psi_{D}}\cdot\frac{\Grad  \Psi_C}{ \Psi_C}
\end{eqnarray}
 }
 \end{small}
 }


\frame
{
  \frametitle{Summing up: Bringing it all together, Local energy}
\begin{small}
{\scriptsize

The second derivative of the Jastrow factor divided by the Jastrow factor (the way it enters the kinetic energy) is
\[
\left[\frac{\nabla^2 \Psi_C}{\Psi_C}\right]_x =\  
2\sum_{k=1}^{N}
\sum_{i=1}^{k-1}\frac{\partial^2 g_{ik}}{\partial x_k^2}\ +\ 
\sum_{k=1}^N
\left(
\sum_{i=1}^{k-1}\frac{\partial g_{ik}}{\partial x_k} -
\sum_{i=k+1}^{N}\frac{\partial g_{ki}}{\partial x_i}
\right)^2
\]
But we have a simple form for the function, namely
\[
\Psi_{C}=\prod_{i< j}\exp{f(r_{ij})}= \exp{\left\{\sum_{i<j}\frac{ar_{ij}}{1+\beta r_{ij}}\right\}},
\]
and it is easy to see that for particle $k$
we have
\[
  \frac{\nabla^2_k \Psi_C}{\Psi_C }=
\sum_{ij\ne k}\frac{({\bf r}_k-{\bf r}_i)({\bf r}_k-{\bf r}_j)}{r_{ki}r_{kj}}f'(r_{ki})f'(r_{kj})+
\sum_{j\ne k}\left( f''(r_{kj})+\frac{2}{r_{kj}}f'(r_{kj})\right)
\]
}
\end{small}
}



\frame
{
  \frametitle{Bringing it all together, Local energy}
\begin{small}
{\scriptsize
Using 
\[
f(r_{ij})= \frac{ar_{ij}}{1+\beta r_{ij}},
\]
and $g'(r_{kj})=dg(r_{kj})/dr_{kj}$ and 
$g''(r_{kj})=d^2g(r_{kj})/dr_{kj}^2$  we find that for particle $k$
we have
\[
  \frac{\nabla^2_k \Psi_C}{\Psi_C }=
\sum_{ij\ne k}\frac{({\bf r}_k-{\bf r}_i)({\bf r}_k-{\bf r}_j)}{r_{ki}r_{kj}}\frac{a}{(1+\beta r_{ki})^2}
\frac{a}{(1+\beta r_{kj})^2}+
\sum_{j\ne k}\left(\frac{2a}{r_{kj}(1+\beta r_{kj})^2}-\frac{2a\beta}{(1+\beta r_{kj})^3}\right)
\]
}
\end{small}
}

\frame
{
  \frametitle{Local energy}
\begin{small}
{\scriptsize
The gradient and
Laplacian can be calculated as follows:
\[
\frac{\vec\nabla_i\det{\matr D(\mathbf{r})}}
{\det{\matr D(\mathbf{r})}} =
\sum_{j=1}^N \vec\nabla_i d_{ij}(\mathbf{r})\,
d_{ji}^{-1}(\mathbf{r}) =
\sum_{j=1}^N \vec\nabla_i \phi_j(\mathbf{r}_i)\,
d_{ji}^{-1}(\mathbf{r})
\]
and
\[
\frac{\nabla^2_i\det{\matr D(\mathbf{r})}}
{\det{\matr D(\mathbf{r})}} =
\sum_{j=1}^N \nabla^2_i d_{ij}(\mathbf{r})\,
d_{ji}^{-1}(\mathbf{r}) =
\sum_{j=1}^N \nabla^2_i \phi_j(\mathbf{r}_i)\,
d_{ji}^{-1}(\mathbf{r})
\]
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Local energy function}
\begin{lstlisting}
double E_local(double** R, double alpha, double beta, int N, double** F, double** DinvUp,
	       double** DinvDown, int N2, double** detgrad, double** jastgrad) {

  //Kinetic energy
  double kinetic = 0;
  //Determinant part
  for (int i = 0; i < N; i++) {
    for (int j = 0; j < dimension; j++) {
      
      if (i < N2)
	for (int l = 0; l < N2; l++)
	  kinetic -= phi_deriv2(l,R,i,j,alpha)*DinvUp[l][i];
      else
	for (int l = 0; l < N2; l++)
	  kinetic -= phi_deriv2(l,R,i,j,alpha)*DinvDown[l][i-N2];
    }
  }
\end{lstlisting}

}


\frame[containsverbatim]
{
  \frametitle{Jastrow part}
\begin{lstlisting}
  //Jastrow part
  double rij,a;
  for (int i = 0; i < N; i++) {
    for (int j = 0; j < dimension; j++) {
      kinetic -= jastgrad[i][j]*jastgrad[i][j];
    }
  }
  for (int i = 0; i < N-1; i++) {
    for (int j = i+1; j < N; j++) {
      if ((j < N2 && i < N2) || (j >= N2 && i >= N2))
	a = 0.33333333;
      else
	a = 1.0;
      rij = r_ij(R,i,j);
      kinetic -= 4*a / (rij*pow(1+beta*rij,3));
    }
  }
\end{lstlisting}

}


\frame[containsverbatim]
{
  \frametitle{Local energy}
\begin{lstlisting}
  //"Interference" part
  for (int i = 0; i < N; i++) {
    for (int j = 0; j < dimension; j++) {
      kinetic -= 2*detgrad[i][j]*jastgrad[i][j];
    }
  }

  kinetic *= .5;
\end{lstlisting}

}


\frame[containsverbatim]
{
  \frametitle{}
\begin{lstlisting}
  //Potential energy
  //electron-nucleus potential
  double potential = 0;
  for (int i = 0; i < N; i++)
    potential -= Z / r_i(R,i);

  //electron-electron potential
  for (int i = 0; i < N - 1; i++)
    for (int j = i+1; j < N; j++)
      potential += 1 / r_ij(R,i,j);
  return potential + kinetic;
}
\end{lstlisting}

}


\frame
{
  \frametitle{Determinant part in quantum force}
\begin{small}
{\scriptsize
The gradient for the determinant is 
\[
\frac{\vec\nabla_i\det{\matr D(\mathbf{r})}}
{\det{\matr D(\mathbf{r})}} =
\sum_{j=1}^N \vec\nabla_i d_{ij}(\mathbf{r})\,
d_{ji}^{-1}(\mathbf{r}) =
\sum_{j=1}^N \vec\nabla_i \phi_j(\mathbf{r}_i)\,
d_{ji}^{-1}(\mathbf{r}).
\]
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Quantum force}
\begin{lstlisting}
void calcQF(double** R, double** F, double alpha, double beta, 
	    int N, double** DinvUp, double** DinvDown, int N2, double** detgrad, double** jastgrad) {
  double sum;
  //Determinant part
  for (int i = 0; i < N; i++) {
    for (int j = 0; j < dimension; j++) {
      sum = 0;
      if (i < N2)
	for (int l = 0; l < N2; l++)
	  sum += phi_deriv(l,R,i,j,alpha)*DinvUp[l][i];
      else
	for (int l = 0; l < N2; l++)
	  sum += phi_deriv(l,R,i,j,alpha)*DinvDown[l][i-N2];
      detgrad[i][j] = sum;
    }
  }
\end{lstlisting}

}

\frame
{
  \frametitle{Jastrow gradient in quantum force}
\begin{small}
{\scriptsize
We have
\[
\Psi_C=\prod_{i< j}g(r_{ij})= \exp{\left\{\sum_{i<j}\frac{ar_{ij}}{1+\beta r_{ij}}\right\}},
\]
the gradient needed for the quantum force and local energy is easy to compute.  
We get for particle $k$
\[
\frac{ \nabla_k \Psi_C}{ \Psi_C }= \sum_{j\ne k}\frac{{\bf r}_{kj}}{r_{kj}}\frac{a}{(1+\beta r_{kj})^2},
\]
which is rather easy to code.  Remember to sum over all particles  when you compute the local energy.
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Jastrow part}
\begin{lstlisting}
  //Jastrow part
  double ril,a;
  for (int i = 0; i < N; i++) {
    for(int j = 0; j < dimension; j++) {
      sum = 0;
      for (int l = 0; l < N; l++) {
	if (l != i) {
	  if ((l < N2 && i < N2) || (l >= N2 && i >= N2))
	    a = 0.33333333;
	  else
	    a = 1.0;
\end{lstlisting}

}


\frame[containsverbatim]
{
  \frametitle{}
\begin{lstlisting}
	  ril = r_ij(R,i,l);
	  sum += (R[i][j]-R[l][j])*a / (ril*pow(1+beta*ril,2));
	}
      }
      jastgrad[i][j] = sum;
    }
  }
  for (int i = 0; i < N; i++)
    for(int j = 0; j < dimension; j++)
      F[i][j] = 2*(detgrad[i][j] + jastgrad[i][j]);
}
\end{lstlisting}
}

\frame[containsverbatim]
{
  \frametitle{Metropolis-Hastings part}
\begin{lstlisting}
  //Initialize positions
  double** R = (double**) matrix(N,dimension,sizeof(double));
  for (int i = 0; i < N; i++)
    for (int j = 0; j < dimension; j++)
      R[i][j] = gaussian_deviate(&idum);


  int N2 = N/2; //dimension of Slater matrix
\end{lstlisting}
}

\frame
{
  \frametitle{Metropolis Hastings part}
\begin{small}
{\scriptsize

We need to compute the ratio between wave functions, in particular  for the Slater determinants.
\[
R =
%\frac{\det{\matr D(\mathbf{r}^{\mathrm{new}})}}
%{\det{\matr D(\mathbf{r}^{\mathrm{old}})}} =
\sum_{j=1}^N d_{ij}(\mathbf{r}^{\mathrm{new}})\,
d_{ji}^{-1}(\mathbf{r}^{\mathrm{old}}) = 
\sum_{j=1}^N \phi_j(\mathbf{r}_i^{\mathrm{new}})\,
d_{ji}^{-1}(\mathbf{r}^{\mathrm{old}})
\]
What this means is that in order to get the ratio when only the $i$th
particle has been moved, we only need to calculate the dot
product of the vector $\left(\phi_1(\mathbf{r}_i^\mathrm{new}),\,\dots,\,
\phi_N(\mathbf{r}_i^\mathrm{new})\right)$ of single particle wave functions
evaluated at this new position with the $i$th column of the inverse
matrix $\matr D^{-1}$ evaluated at the original position. Such
an operation has a time scaling of $\bigO(N)$. The only extra thing we
need to do is to maintain the inverse matrix $\matr D^{-1}(\vec
x^{\mathrm{old}})$.
}
\end{small}
}



\frame
 {
   \frametitle{Jastrow factor in Metropolis Hastings}
 \begin{small}
 {\scriptsize
We have
\begin{equation}
 \boxed{R_{C} = \frac{\Psi_{C}^\mathrm{new}}{\Psi_{C}^\mathrm{cur}} = \frac{e^{U_{new}}}{e^{U_{cur}}} = e^{\Delta U},}
\end{equation}
where
\begin{equation}
\Delta U =
\sum_{i=1}^{k-1}\big(f_{ik}^\mathrm{new}-f_{ik}^\mathrm{cur}\big)
+
\sum_{i=k+1}^{N}\big(f_{ki}^\mathrm{new}-f_{ki}^\mathrm{cur}\big)
\end{equation}
One needs to develop a special algorithm 
that runs only through the elements of the upper triangular
matrix $\bfv{g}$ and have $k$ as an index. 

 }
 \end{small}
 }




\frame[containsverbatim]
{
  \frametitle{Metropolis-Hastings part}
\begin{lstlisting}
  //Initialize inverse Slater matrices for spin up and spin down
  double** DinvUp = (double**) matrix(N2,N2,sizeof(double));
  double** DinvDown = (double**) matrix(N2,N2,sizeof(double));
  for (int i = 0; i < N2; i++) {
    for (int j = 0; j < N2; j++) {
      DinvUp[i][j] = phi(j,R,i,alpha);
      DinvDown[i][j] = phi(j,R,i+N2,alpha);
    }
  }
  inverse(DinvUp,N2);
  inverse(DinvDown,N2);
\end{lstlisting}
}

\frame[containsverbatim]
{
  \frametitle{Metropolis-Hastings part}
\begin{lstlisting}
  //Inverse Slater matrix in new position
  double** DinvUp_new = (double**) matrix(N2,N2,sizeof(double));
  double** DinvDown_new = (double**) matrix(N2,N2,sizeof(double));
  for (int i = 0; i < N2; i++) {
    for (int j = 0; j < N2; j++) {
      DinvUp_new[i][j] = DinvUp[i][j];
      DinvDown_new[i][j] = DinvDown[i][j];
    }
  }
\end{lstlisting}
}

\frame[containsverbatim]
{
  \frametitle{Metropolis-Hastings part}
\begin{lstlisting}
  //Gradients of determinant and and Jastrow factor
  double** detgrad = (double**) matrix(N,dimension,sizeof(double));
  double** jastgrad = (double**) matrix(N,dimension,sizeof(double));
  double** detgrad_new = (double**) matrix(N,dimension,sizeof(double));
  double** jastgrad_new = (double**) matrix(N,dimension,sizeof(double));
  
  //Initialize quantum force
  double** F = (double**) matrix(N,dimension,sizeof(double));
  calcQF(R,F,alpha,beta,N,DinvUp,DinvDown,N2,detgrad,jastgrad);
\end{lstlisting}
}

\frame[containsverbatim]
{
  \frametitle{Metropolis-Hastings part}
\begin{lstlisting}
  double EL; //Local  energy
  double sqrtdt = sqrt(delta_t);
  double D = .5; //diffusion constant
  //For Metropolis-Hastings algo:
  double** R_new = (double**) matrix(N,dimension,sizeof(double));
  double** F_new = (double**) matrix(N,dimension,sizeof(double));
  double greensratio; // Ratio between Green's functions
  double detratio; //Ratio between Slater determinants
  double jastratio; //Ratio between Jastrow factors
  double rold,rnew,a;
  double alphaderiv,betaderiv;
\end{lstlisting}
}

\frame[containsverbatim]
{
  \frametitle{Metropolis-Hastings part, inside Monte Carlo loop}
\begin{lstlisting}
      //Ratio between Slater determinants
      if (i < N2) {
	detratio = 0;
	for (int l = 0; l < N2; l++)
	  detratio += phi(l,R_new,i,alpha) * DinvUp[l][i];
      }
      else {
	detratio = 0;
	for (int l = 0; l < N2; l++)
	  detratio += phi(l,R_new,i,alpha) * DinvDown[l][i-N2];
      }
\end{lstlisting}
}

\frame[containsverbatim]
{
  \frametitle{Metropolis-Hastings part}
\begin{lstlisting}
      //Inverse Slater matrix in new position
      if (i < N2) { //Spinn up
	for (int j = 0; j < N2; j++) {
	  if (j != i) {
	    Sj = 0;
	    for (int l = 0; l < N2; l++) {
	      Sj += phi(l,R_new,i,alpha) * DinvUp[l][j];
	    }
	    for (int l = 0; l < N2; l++) 
	      DinvUp_new[l][j] = DinvUp[l][j] - Sj * DinvUp[l][i] / detratio;
	  }
	}
	for (int l = 0; l < N2; l++)
	  DinvUp_new[l][i] = DinvUp[l][i] / detratio;
      }
\end{lstlisting}
}

\frame[containsverbatim]
{
  \frametitle{Metropolis-Hastings part}
\begin{lstlisting}
      else { //Spinn-ned
	for (int j = 0; j < N2; j++) {
	  if (j != i-N2) {
	    Sj = 0;
	    for (int l = 0; l < N2; l++) {
	      Sj += phi(l,R_new,i,alpha) * DinvDown[l][j];
	    }
	    for (int l = 0; l < N2; l++) 
	      DinvDown_new[l][j] = DinvDown[l][j] - Sj * DinvDown[l][i-N2] / detratio;
	  }
	}
	for (int l = 0; l < N2; l++)
	  DinvDown_new[l][i-N2] = DinvDown[l][i-N2] / detratio;
      }
\end{lstlisting}
}

\frame[containsverbatim]
{
  \frametitle{Jastrow ratio}
\begin{lstlisting}
      //Ratio between Jastrow factors
      jastratio = 0;
      for (int l = 0; l < N; l++) {
	if (l != i) {
	  if ((l < N2 && i < N2) || (l >= N2 && i >= N2))
	    a = 0.33333333;
	  else
	    a = 1.0;
	  rold = r_ij(R,l,i);
	  rnew = r_ij(R_new,l,i);
	  jastratio += a * (rnew/(1+beta*rnew) - rold/(1+beta*rold));
	}
      }
      jastratio = exp(jastratio);
\end{lstlisting}
}
\frame[containsverbatim]
{
  \frametitle{Green's functions}
\begin{lstlisting}
      //quantum force in new position
      calcQF(R_new,F_new,alpha,beta,N,DinvUp_new,DinvDown_new,N2,detgrad_new,jastgrad_new);

      //Ratio between Green's functions
      greensratio = 0;
      for (int ii = 0; ii < N; ii++)
	for (int j = 0; j < 3; j++)
	  greensratio += .5*(F_new[ii][j]+F[ii][j]) * (.5*D*delta_t*(F[ii][j]-F_new[ii][j]) + R[ii][j] - R_new[ii][j]);
      greensratio = exp(greensratio);

\end{lstlisting}
}
\frame[containsverbatim]
{
  \frametitle{Metropolis Hastings test}
\begin{lstlisting}
      //Metropolis-Hastings-test
      if (ran2(&idum) < greensratio*detratio*detratio*jastratio*jastratio) {
	//Accept move abd update invers Slater matrix
	if (i < N2)
	  for (int l = 0; l < N2; l++)
	    for (int m = 0; m < N2; m++)
	      DinvUp[l][m] = DinvUp_new[l][m];
	else
	  for (int l = 0; l < N2; l++)
	    for (int m = 0; m < N2; m++)
	      DinvDown[l][m] = DinvDown_new[l][m];
\end{lstlisting}
}
\frame[containsverbatim]
{
  \frametitle{}
\begin{lstlisting}
	//Update position, quantum force and gradients
	for (int ii = 0; ii < N; ii++) {
	  for (int j = 0; j < 3; j++) {
	    R[ii][j] = R_new[ii][j];
	    F[ii][j] = F_new[ii][j];
	    detgrad[ii][j] = detgrad_new[ii][j];
	    jastgrad[ii][j] = jastgrad_new[ii][j];
      .......
    } //End loop of electron that has been moved
\end{lstlisting}
}




\frame
{
  \frametitle{Proof for updating algorithm of the Slater matrix}
\begin{small}
{\scriptsize
As a starting point we may consider that each time a new position is suggested in the Metropolis algorithm, a row of the current Slater matrix experiences some kind of perturbation. Hence, the Slater matrix with its orbitals evaluated at the new position equals the old Slater matrix plus a perturbation matrix,

\begin{equation}\label{oldSM}
d_{jk}(\bfv{x^{new}}) = d_{jk}(\bfv{x^{old}}) + \Delta_{jk},
\end{equation}

where

\begin{equation}\label{pertMatrix}
\Delta_{jk} = \delta_{ik}[\phi_j(\bfv{x_{i}^{new}}) - \phi_j(\bfv{x_{i}^{old}})] = \delta_{ik}(\Delta\phi)_j .
\end{equation}

}
\end{small}
}


\frame
{
  \frametitle{Proof for updating algorithm of the Slater matrix}
\begin{small}
{\scriptsize
Computing the inverse of the transposed matrix we arrive to

\begin{equation}\label{invDkj}
 d_{kj}(\bfv{x^{new}})^{-1} = [d_{kj}(\bfv{x^{old}}) + \Delta_{kj}]^{-1}.
\end{equation}

The evaluation of the right hand side (rhs) term above is carried out by applying the identity $(A +  B)^{-1} = A^{-1} - (A + B)^{-1} B A^{-1}$. In compact notation it yields

\begin{eqnarray*}
 [\bfv{D}^{T}(\bfv{x^{new}})]^{-1} & = & [\bfv{D}^{T}(\bfv{x^{old}}) + \Delta^T]^{-1}\\
& = & [\bfv{D}^{T}(\bfv{x^{old}})]^{-1} - [\bfv{D}^{T}(\bfv{x^{old}}) + \Delta^T]^{-1} \Delta^T [\bfv{D}^{T}(\bfv{x^{old}})]^{-1}\\
& = & [\bfv{D}^{T}(\bfv{x^{old}})]^{-1} - \underbrace{{[\bfv{D}^{T}(\bfv{x^{new}})]^{-1}}}_{\text{By Eq.}{\ref{invDkj}}}  \Delta^T [\bfv{D}^{T}(\bfv{x^{old}})]^{-1}.
\end{eqnarray*}

}
\end{small}
}
\frame
{
  \frametitle{Proof for updating algorithm of the Slater matrix}
\begin{small}
{\scriptsize
Using index notation, the last result may be expanded by
\begin{eqnarray*}
d^{-1}_{kj}(\bfv{x^{new}}) & = & d^{-1}_{kj}(\bfv{x^{old}}) -  \sum_{l} \sum_{m} d^{-1}_{km}(\bfv{x^{new}}) \Delta^{T}_{ml}  d^{-1}_{lj}(\bfv{x^{old}})\\
& = & d^{-1}_{kj}(\bfv{x^{old}}) -  \sum_{l} \sum_{m} d^{-1}_{km}(\bfv{x^{new}}) \Delta_{lm}  d^{-1}_{lj}(\bfv{x^{cur}})\\
& = & d^{-1}_{kj}(\bfv{x^{old}}) -  \sum_{l} \sum_{m} d^{-1}_{km}(\bfv{x^{new}}) \underbrace{\delta_{im} (\Delta \phi)_{l}}_{\text{By Eq. }\ref{pertMatrix}}  d^{-1}_{lj}(\bfv{x^{old}})\\
& = & d^{-1}_{kj}(\bfv{x^{old}}) - d^{-1}_{ki}(\bfv{x^{new}}) \sum_{l=1}^{N}(\Delta \phi)_{l}  d^{-1}_{lj}(\bfv{x^{old}})\\
& = & d^{-1}_{kj}(\bfv{x^{old}}) - d^{-1}_{ki}(\bfv{x^{new}}) \sum_{l=1}^{N}\underbrace{[\phi_{l}(\bfv{r_{i}^{new}}) - \phi_{l}(\bfv{r_{i}^{old}})]}_{\text{By Eq.}\ref{pertMatrix}}  D^{-1}_{lj}(\bfv{x^{old}}).
\end{eqnarray*}
}
\end{small}
}

\frame
{
  \frametitle{Proof for updating algorithm of the Slater matrix}
\begin{small}
{\scriptsize
Using 
$$\bfv{D}^{-1}(\bfv{x^{old}}) = \frac{adj \bfv{D}}{|\bfv{D}(\bfv{x^{old}})|} \, \quad \text{and} \, \quad \bfv{D}^{-1}(\bfv{x^{new}}) = \frac{adj \bfv{D}}{|\bfv{D}(\bfv{x^{new}})|},$$
and dividing these two equations we get
$$\frac{\bfv{D}^{-1}(\bfv{x^{old}})}{\bfv{D}^{-1}(\bfv{x^{new}})} = \frac{|\bfv{D}(\bfv{x^{new}})|}{|\bfv{D}(\bfv{x^{old}})|} = R \Rightarrow d^{-1}_{ki}(\bfv{x^{new}}) = \frac{d^{-1}_{ki}(\bfv{x^{old}})}{R}.$$

Therefore,

$$d^{-1}_{kj}(\bfv{x^{new}})  =  d^{-1}_{kj}(\bfv{x^{old}}) - \frac{d^{-1}_{ki}(\bfv{x^{old}})}{R} \sum_{l=1}^{N}[\phi_{l}(\bfv{r_{i}^{new}}) - \phi_{l}(\bfv{r_{i}^{old}})]  d^{-1}_{lj}(\bfv{x^{old}}),$$
}
\end{small}
}



\frame
{
  \frametitle{Proof for updating algorithm of the Slater matrix}
\begin{small}
{\scriptsize
or
\begin{align}
 d^{-1}_{kj}(\bfv{x^{new}})  =  d^{-1}_{kj}(\bfv{x^{old}}) \qquad & - & \frac{d^{-1}_{ki}(\bfv{x^{old}})}{R} \sum_{l=1}^{N}\phi_{l}(\bfv{r_{i}^{new}})  d^{-1}_{lj}(\bfv{x^{old}}) \nonumber\\
  & + &  \frac{d^{-1}_{ki}(\bfv{x^{old}})}{R} \sum_{l=1}^{N}\phi_{l}(\bfv{r_{i}^{old}})  d^{-1}_{lj}(\bfv{x^{old}})\nonumber\\
                             =  d^{-1}_{kj}(\bfv{x^{old}}) \qquad & - & \frac{d^{-1}_{ki}(\bfv{x^{old}})}{R} \sum_{l=1}^{N} d_{il}(\bfv{x^{new}})  d^{-1}_{lj}(\bfv{x^{old}}) \nonumber\\
& + &  \frac{d^{-1}_{ki}(\bfv{x^{old}})}{R} \sum_{l=1}^{N} d_{il}(\bfv{x^{old}}) d^{-1}_{lj}(\bfv{x^{old}}).\nonumber
\end{align}
}
\end{small}
}


\frame
{
  \frametitle{Proof for updating algorithm of the Slater matrix}
\begin{small}
{\scriptsize
In this equation, the first line becomes zero for $j=i$ and the second for $j \neq i$. Therefore, the update of the inverse for the new Slater matrix is given by

\begin{eqnarray}
\boxed{d^{-1}_{kj}(\bfv{x^{new}})  = \left\{ 
\begin{array}{l l}
  d^{-1}_{kj}(\bfv{x^{old}}) - \frac{d^{-1}_{ki}(\bfv{x^{old}})}{R} \sum_{l=1}^{N} d_{il}(\bfv{x^{new}})  d^{-1}_{lj}(\bfv{x^{old}}) & \mbox{if $j \neq i$}\nonumber \\ \\
 \frac{d^{-1}_{ki}(\bfv{x^{old}})}{R} \sum_{l=1}^{N} d_{il}(\bfv{x^{old}}) d^{-1}_{lj}(\bfv{x^{old}}) & \mbox{if $j=i$}
\end{array} \right.}
\end{eqnarray}
}
\end{small}
}


\section[Week 15]{Week 15}
\frame
{
  \frametitle{Topics for Week 15, April 11-15}
  \begin{block}{Slater determinants and Density functional theory}
\begin{itemize}
\item Repetition from last week
\item Hints and tips when computing the Slater determinant
\item Begin density functional theory (DFT): 
\begin{enumerate} \item The equations, overview \item Reminder on variational calculus and \item 
Hartree-Fock theory
\end{enumerate}
\end{itemize}
Project work this week: start programming the  Slater determinant. This part should be finalized before
May 1. Thursday April 28 we discuss how to program the Kohn-Sham equations. Read chapters 4.1-4.5 (Hartree-Fock) and  
5.1-5.3 (DFT) of Thijssen. 
  \end{block}
} 

\frame
{
  \begin{small}
    {\scriptsize
      \frametitle{DFT: Selected literature}
      \begin{itemize}
      \item R. van Leeuwen: \emph{Density functional approach to the many-body problem: key concepts and exact functionals}, Adv. Quant. Chem. \textbf{43}, 25 (2003). (Mathematical foundations of DFT)\\
        %$\quad \quad $ Article on seminar web page. \alert{Description}: Mathematical foundations of \\
        %$\quad \quad $ DFT for chemists and physicists.  
      \item R. M. Dreizler and E. K. U. Gross: \emph{Density functional theory: An approach to the quantum many-body problem}. (Introductory book)\\
        %$\quad \quad $ These lectures based to a large extent on this book. \alert{Description}: Much of\\
        %$\quad \quad $ the same things as in article by van Leeuwen, but less mathematically rigorous. 
      \item W. Koch and M. C. Holthausen: \emph{A chemist's guide to density functional theory}. (Introductory book, less formal than Dreizler/Gross)
      \item E. H. Lieb: Density functionals for Coulomb systems, Int. J. Quant. Chem. \textbf{24}, 243-277 (1983). (Mathematical analysis of DFT) 
      \end{itemize}

    }
  \end{small}
}

\frame[containsverbatim]
{
  \frametitle{Density Functional Theory (DFT)}
\begin{small}
{\scriptsize
Hohenberg and Kohn proved that the total energy of a system including that of the many-body 
effects of electrons (exchange and correlation) in the presence of static external potential (for example, the atomic nuclei) 
is a unique functional of the charge density. The minimum value of the total energy functional 
is the ground state energy of the system. The electronic charge density which yields this 
minimum defines the ground state energy.

In Hartree-Fock theory  one works with large basis sets. This
poses a problem for large systems. An alternative to the HF methods is
DFT. DFT  takes into 
account electron correlations but is less demanding computationally
than full scale diagonalization or Monte Carlo methods.


 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Density Functional Theory}
\begin{small}
{\scriptsize
The electronic energy $E$ is said to be a \emph{functional} of the
electronic density, $E[\rho]$, in the sense that for a given function
$\rho(r)$, there is a single corresponding energy. The  
\emph{Hohenberg-Kohn theorem} confirms that such
a functional exists, but does not tell us the form of the
functional. As shown by Kohn and Sham, the exact ground-state energy
$E$ of an $N$-electron system can be written as

\begin{equation*}
  E[\rho] = -\frac{1}{2} \sum_{i=1}^N\int
  \Psi_i^*(\mathbf{r_1})\nabla_1^2 \Psi_i(\mathbf{r_1}) d\mathbf{r_1}
  - \int \frac{Z}{r_1} \rho(\mathbf{r_1}) d\mathbf{r_1} +
  \frac{1}{2} \int\frac{\rho(\mathbf{r_1})\rho(\mathbf{r_2})}{r_{12}}
  d\mathbf{r_1}d\mathbf{r_2} + E_{EXC}[\rho]
\end{equation*}

with $\Psi_i$ the \emph{Kohn-Sham} (KS) \emph{orbitals}.
 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Density Functional Theory}
\begin{small}
{\scriptsize
 The ground-state charge density is given by
\begin{equation*}
  \rho(\mathbf{r}) = \sum_{i=1}^N|\Psi_i(\mathbf{r})|^2, 
  %\label{}
\end{equation*}

where the sum is over the occupied Kohn-Sham orbitals. The last term,
$E_{EXC}[\rho]$, is the \emph{exchange-correlation energy} which in
theory takes into account all non-classical electron-electron
interaction. However, we do not know how to obtain this term exactly,
and are forced to approximate it. The KS orbitals are found by solving
the \emph{Kohn-Sham equations}, which can be found by applying a
variational principle to the electronic energy $E[\rho]$. This approach
is similar to the one used for obtaining the HF equation.
 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Density Functional Theory}
\begin{small}
{\scriptsize
The KS equations reads

\begin{equation*}
  \left\{ -\frac{1}{2}\nabla_1^2 - \frac{Z}{r_1} + \int 
  \frac{\rho(\mathbf{r_2})}{r_{12}} d\mathbf{r_2} +
  V_{EXC}(\mathbf{r_1}) \right\} \Psi_i(\mathbf{r_1}) =
  \epsilon_i \Psi_i(\mathbf{r_1})
  %\label{}
\end{equation*}

where $\epsilon_i$ are the KS orbital energies, and where the 
\emph{exchange-correlation potential} is given by

\begin{equation*}
  V_{EXC}[\rho] = \frac{\delta E_{EXC}[\rho]}{\delta \rho}.
  %\label{}
\end{equation*}
 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Density Functional Theory}
\begin{small}
{\scriptsize
The KS equations are solved in a self-consistent fashion. A variety of
basis set functions  can be used, and the experience gained in HF
calculations are often useful. The computational time needed for a DFT
calculation formally scales as the third power of the number of basis
functions. 

The main source of error in DFT usually arises from the approximate
nature of $E_{EXC}$. In the \emph{local density approximation} (LDA) it
is approximated as

\begin{equation*}
  E_{EXC} = \int \rho(\mathbf{r})\epsilon_{EXC}[\rho(\mathbf{r})]
  d\mathbf{r},
  %\label{}
\end{equation*}

where $\epsilon_{EXC}[\rho(\mathbf{r})]$ is the exchange-correlation
energy per electron in a homogeneous electron gas of constant density.
The LDA approach is clearly an approximation as the charge is not
continuously distributed. To account for the inhomogeneity of the
electron density, a nonlocal 
 }
 \end{small}
 }

\frame
{
  \frametitle{The Hohenberg-Kohn theorem}
%\begin{small}
{\scriptsize
%\begin{center}
%\begin{equation}
%\begin{center}
Assume \alert{Hamiltonian} of many-fermion system
\[
\hat{H} = \hat{T}+\hat{V}+\hat{W},
\]
\pause
%\end{equation}
or second quantized form
%\[
%\begin{flushleft}
%\begin{eqnarray}
%\lefteqn{ \hat{H} = -\frac{\hbar^{2}}{2m}\sum_{i}\int d^{3}r\hat{\Psi}_{i}^{\dagger}(\mathbf{r})\nabla^{2}\hat{\Psi}_{i}(\mathbf{r})+\sum_{i}\int d^{3}r\hat{\Psi}_{i}^{\dagger}(\mathbf{r})v(\mathbf{r})\hat{\Psi}_{i}(\mathbf{r}) } \nonumber\\
%& & {}+\frac{1}{2}\sum_{i,j}\int d^{3}r\int d^{3}r'\hat{\Psi}_{i}^{\dagger}(\mathbf{r})\hat{\Psi}_{j}^{\dagger}(\mathbf{r'}) w(\mathbf{r},\mathbf{r'})\hat{\Psi}_{j}(\mathbf{r'})\hat{\Psi}_{i}(\mathbf{r}),\nonumber
%\end{eqnarray}
%\end{flushleft}
%\begin{flushleft}
\begin{eqnarray}
\lefteqn{ \hat{H} = -\frac{\hbar^{2}}{2m}\int d^{3}r\hat{\Psi}^{\dagger}(\mathbf{r})\nabla^{2}\hat{\Psi}(\mathbf{r})+\int d^{3}r\hat{\Psi}^{\dagger}(\mathbf{r})v(\mathbf{r})\hat{\Psi}(\mathbf{r}) } \nonumber\\
& & {}+\frac{1}{2}\int d^{3}r\int d^{3}r'\hat{\Psi}^{\dagger}(\mathbf{r})\hat{\Psi}^{\dagger}(\mathbf{r'}) w(\mathbf{r},\mathbf{r'})\hat{\Psi}(\mathbf{r'})\hat{\Psi}(\mathbf{r}),\nonumber
\end{eqnarray}
%\end{flushleft}
$\hat{\Psi}$, $\hat{\Psi}^{\dagger } =$ annihilation, creation \emph{field operators}
%\]
%\end{center}
%\begin{flushleft}
%  \hat{T}= 
%\end{flushleft}
}
%\end{small}
}

\frame
{
  \frametitle{}
\begin{small}
{\scriptsize
  \begin{equation}
    \hat{\Psi}(\mathbf{r})\equiv \sum_{\mathbf{k}}\psi_{\mathbf{k}}(\mathbf{r})a_{\mathbf{k}} \nonumber
  \end{equation}
  \begin{equation}
    \hat{\Psi}^{\dagger }(\mathbf{r})\equiv \sum_{\mathbf{k}}\psi_{\mathbf{k}}^{*}(\mathbf{r})a_{\mathbf{k}}^{\dagger } \nonumber
  \end{equation}
\begin{center}
$\mathbf{k} = $ collection of quantum numbers
\end{center}

\begin{align}
  \hat{T} &= \text{kinetic energy operator} \nonumber \\
  \hat{V} &= \text{external single-particle potential operator} \nonumber \\
  \hat{W} &= \text{two-particle interaction operator} \nonumber
\end{align}
}
\end{small}
}

\frame
{
  \frametitle{The Hohenberg-Kohn theorem}
%\begin{small}
{\scriptsize
%\begin{center}
%\begin{equation}
%\begin{center}
Assume \alert{Hamiltonian} of many-fermion system
\[
\hat{H} = \hat{T}+\hat{V}+\hat{W},
\]
\pause
%\end{equation}
or second quantized form
%\[
%\begin{flushleft}
%\begin{eqnarray}
%\lefteqn{ \hat{H} = -\frac{\hbar^{2}}{2m}\sum_{i}\int d^{3}r\hat{\Psi}_{i}^{\dagger}(\mathbf{r})\nabla^{2}\hat{\Psi}_{i}(\mathbf{r})+\sum_{i}\int d^{3}r\hat{\Psi}_{i}^{\dagger}(\mathbf{r})v(\mathbf{r})\hat{\Psi}_{i}(\mathbf{r}) } \nonumber\\
%& & {}+\frac{1}{2}\sum_{i,j}\int d^{3}r\int d^{3}r'\hat{\Psi}_{i}^{\dagger}(\mathbf{r})\hat{\Psi}_{j}^{\dagger}(\mathbf{r'}) w(\mathbf{r},\mathbf{r'})\hat{\Psi}_{j}(\mathbf{r'})\hat{\Psi}_{i}(\mathbf{r}),\nonumber
%\end{eqnarray}
%\end{flushleft}
%\begin{flushleft}
\begin{eqnarray}
\lefteqn{ \hat{H} = -\frac{\hbar^{2}}{2m}\int d^{3}r\hat{\Psi}^{\dagger}(\mathbf{r})\nabla^{2}\hat{\Psi}(\mathbf{r})+\int d^{3}r\hat{\Psi}^{\dagger}(\mathbf{r})v(\mathbf{r})\hat{\Psi}(\mathbf{r}) } \nonumber\\
& & {}+\frac{1}{2}\int d^{3}r\int d^{3}r'\hat{\Psi}^{\dagger}(\mathbf{r})\hat{\Psi}^{\dagger}(\mathbf{r'}) w(\mathbf{r},\mathbf{r'})\hat{\Psi}(\mathbf{r'})\hat{\Psi}(\mathbf{r}),\nonumber
\end{eqnarray}
%\end{flushleft}
$\hat{\Psi}$, $\hat{\Psi}^{\dagger } =$ annihilation, creation \emph{field operators}
%\]
%\end{center}
%\begin{flushleft}
%  \hat{T}= 
%\end{flushleft}
}
%\end{small}
}

\frame
{
  \frametitle{}
\begin{small}
{\scriptsize

$\mathcal{V} =$ set of external single-particle \alert{potentials} $v$ s.t. 
\begin{equation}
\hat{H}\ket{\phi} = \left(\hat{T}+\hat{V}+\hat{W}\right)=E\ket{\phi},\qquad \hat{V}\in \mathcal{V},\nonumber
\end{equation} 
gives a \alert{non-degenerate} N-particle ground state $\ket{\Psi }$

\pause
\vspace{0.5cm}
\begin{equation}
  \Longrightarrow \qquad C:\mathcal{V}(C)\longrightarrow \Psi \qquad \text{\alert{surjective},}  \nonumber 
\end{equation}
where $\Psi = $ set of ground states (GS) $\ket{\Psi }$
\begin{figure}[h]
\includegraphics[scale=0.35]{surjection1}
\end{figure}
%(}\mathcal{V}(C)=\Psi \text{)

% The basic idea of Gaussian elimination is to use the first equation to eliminate the first unknown $x_1$
% from the remaining $n-1$ equations. Then we use the new second equation to eliminate the second unknown
% $x_2$ from the remaining $n-2$ equations. With $n-1$ such eliminations
% we obtain a so-called upper triangular set of equations of the form
% \begin{eqnarray}\label{eq:gaussbacksub}
%  b_{11}x_1 +b_{12}x_2 +b_{13}x_3 + b_{14}x_4=&y_1 \nonumber \\
%  b_{22}x_2 + b_{23}x_3 + b_{24}x_4=&y_2 \nonumber \\
% b_{33}x_3 + b_{34}x_4=&y_3 \nonumber \\
% b_{44}x_4=&y_4. \nonumber
% \end{eqnarray}
% We can solve this system of equations recursively starting from $x_n$ (in our case $x_4$) and proceed with 
% what is called a backward substitution. This process can be expressed mathematically as
% \begin{equation}
%    x_m = \frac{1}{b_{mm}}\left(y_m-\sum_{k=m+1}^nb_{mk}x_k\right)\hspace{0.5cm} m=n-1,n-2,\dots,1.
% \end{equation}
% To arrive at such an upper triangular system of equations, we start by eliminating
% the unknown $x_1$ for $j=2,n$. We achieve this by multiplying the first equation by $a_{j1}/a_{11}$ and then subtract
% the result from the $j$th equation. We assume obviously that $a_{11}\ne 0$ and that
% ${\bf A}$ is not singular. 
}
\end{small}
}




\frame
{
  \frametitle{}
\begin{small}
{\scriptsize
The density 
\begin{equation}
  \rho(\mathbf{r})=N\sum_{i}\int dx_{2}\dots \int dx_{N}\vert \Psi(\mathbf{r}i,x_{2},\dots ,x_{N})\vert^{2} \nonumber
\end{equation}
gives a second map 
\begin{equation}
  D:\Psi \longrightarrow \mathcal{N}, \nonumber
\end{equation}
where $\mathcal{N} =$ set of GS densities. The map trivially surjective.
\pause

\vspace{1cm}
\begin{lemma}
Hohenberg-Kohn states: $C$ and $D$ also \alert{injective} (one-to-one; $x_{1}\neq x_{2} \Rightarrow Tx_{1}\neq Tx_{2}$)
\begin{align}
\Longrightarrow \qquad & C \text{ and } D \text{ bijective (surjective and bijective)} \nonumber \\
\Longrightarrow \qquad & CD:\mathcal{V}(CD)\longrightarrow \mathcal{N} \qquad \text{\alert{bijective}} \nonumber
\end{align}
\end{lemma} 

% Our actual $4\times 4$ example reads after the first operation
% \[
% \left(\begin{array}{cccc}
%                            a_{11}& a_{12} &a_{13}& a_{14}\\
%                            0& (a_{22}-\frac{a_{21}a_{12}}{a_{11}}) &(a_{23}-\frac{a_{21}a_{13}}{a_{11}}) & (a_{24}-\frac{a_{21}a_{14}}{a_{11}})\\
% 0& (a_{32}-\frac{a_{31}a_{12}}{a_{11}})& (a_{33}-\frac{a_{31}a_{13}}{a_{11}})& (a_{34}-\frac{a_{31}a_{14}}{a_{11}})\\
% 0&(a_{42}-\frac{a_{41}a_{12}}{a_{11}}) &(a_{43}-\frac{a_{41}a_{13}}{a_{11}}) & (a_{44}-\frac{a_{41}a_{14}}{a_{11}}) \\
%                       \end{array} \right)\left(\begin{array}{c}
%                            x_1\\
%                            x_2\\
%                            x_3 \\
%                            x_4  \\
%                       \end{array} \right)
%   =\left(\begin{array}{c}
%                            y_1\\
%                            w_2^{(2)}\\
%                            w_3^{(2)} \\
%                            w_4^{(2)}\\
%                       \end{array} \right),
% \]
% or 
% \begin{eqnarray}
%  b_{11}x_1 +b_{12}x_2 +b_{13}x_3 + b_{14}x_4=&y_1 \nonumber \\
%  a^{(2)}_{22}x_2 + a^{(2)}_{23}x_3 + a^{(2)}_{24}x_4=&w^{(2)}_2 \nonumber \\
%  a^{(2)}_{32}x_2 + a^{(2)}_{33}x_3 + a^{(2)}_{34}x_4=&w^{(2)}_3 \nonumber \\
%  a^{(2)}_{42}x_2 + a^{(2)}_{43}x_3 + a^{(2)}_{44}x_4=&w^{(2)}_4, \nonumber \\
% \end{eqnarray}
}
\end{small}
}




%\frame[containsverbatim]
\frame
 {
   \frametitle{}
 \begin{small}
 {\scriptsize
\begin{proof}[Proof I]
Let us prove $C:\mathcal{V}(C)\longrightarrow \Psi$  injective:
%\vspace{0.5cm}

\begin{equation}
  \hat{V}\neq \hat{V}'+\text{constant} \qquad \stackrel{{\LARGE ?}}{\Longrightarrow } \qquad \ket{\Psi}\neq \ket{\Psi'}, \nonumber
\end{equation}
where $\hat{V}, \hat{V}' \in \mathcal{V}$
\vspace{0.5cm}
\pause

\textbf{\emph{Reductio ad absurdum}}: 
\vspace{1mm}

Assume $\ket{\Psi}=\ket{\Psi'}$ for some $\hat{V}\neq \hat{V}'+\text{const}$, $\hat{V}, \hat{V}' \in \mathcal{V}$ 
\pause

$\hat{T}\neq \hat{T}[V]$, $\hat{W}\neq \hat{W}[V] \quad \Longrightarrow $\footnote{Unique continuation theorem: $\ket{\Psi}\neq 0$ on a set of positive measure}
\begin{equation}
  \left(\hat{V}-\hat{V}'\right)\ket{\Psi }=\left(E_{gs}-E_{gs}'\right)\ket{\Psi }.\nonumber  
\end{equation}
\pause
\begin{align}
  \Longrightarrow \qquad & \hat{V}-\hat{V}'=E_{gs}-E_{gs}' \nonumber \\
  \Longrightarrow \qquad & \hat{V}=\hat{V}'+\text{constant} \qquad \alert{\text{Contradiction!}}\nonumber
\end{align}
\end{proof}
%\stackrel{{\LARGE ?}}{\Longrightarrow }
%\genfrac{}{}{0pt}{}{?}{\Longrightarrow }

% \begin{verbatim}
% // random numbers with gaussian distribution
% double gaussian_deviate(long * idum)
% {
%   static int iset = 0;
%   static double gset;
%   double fac, rsq, v1, v2;
%   if ( idum < 0) iset =0;
%   if (iset == 0) {
%     do {
%       v1 = 2.*ran0(idum) -1.0;
%       v2 = 2.*ran0(idum) -1.0;
%       rsq = v1*v1+v2*v2;
%     } while (rsq >= 1.0 || rsq == 0.);
%     fac = sqrt(-2.*log(rsq)/rsq);
%     gset = v1*fac;
%     iset = 1;
%     return v2*fac;
%   } else {
%     iset =0;
%     return gset;
%   }
% \end{verbatim}
 }
 \end{small}
 }



\frame
 {
   \frametitle{}
 \begin{small}
 {\scriptsize
\begin{proof}[Proof II]
  Let us prove $D:\Psi \longrightarrow \mathcal{N}$ injective:

\begin{equation}
  \ket{\Psi}\neq \ket{\Psi'} \qquad \stackrel{{\LARGE ?}}{\Longrightarrow } \qquad \rho(\mathbf{r})\neq n'(\mathbf{r}) \nonumber
\end{equation}
%\vspace{0.1cm}
\pause
\textbf{\emph{Reductio ad absurdum}}: 
\vspace{1mm}

Assume $\rho(\mathbf{r})=n'(\mathbf{r})$ for some $\ket{\Psi}\neq \ket{\Psi'}$ 
\pause

Ritz principle $\quad \Longrightarrow $
\begin{equation}
  E_{gs}=\bra{\Psi}\hat{H}\ket{\Psi}<\bra{\Psi'}\hat{H}\ket{\Psi'} \nonumber   
\end{equation}
\pause 
\begin{equation}
  \bra{\Psi'}\hat{H}\ket{\Psi'}=\bra{\Psi'}\hat{H}'+\hat{V}-\hat{V}'\ket{\Psi'}=E_{gs}'+\int n'(\mathbf{r})[v(\mathbf{r})-v'(\mathbf{r})]d^{3}r \nonumber
\end{equation}
\pause
\begin{equation}\label{eq:ineq1}
  \Longrightarrow \qquad E_{gs}'<E_{gs}+\int n'(\mathbf{r})[v(\mathbf{r})-v'(\mathbf{r})]d^{3}r %\nonumber
\end{equation}
By symmetry
\begin{equation}\label{eq:ineq2}
  \Longrightarrow \qquad E_{gs}<E_{gs}'+\int n'(\mathbf{r})[v'(\mathbf{r})-v(\mathbf{r})]d^{3}r %\nonumber
\end{equation}
\pause
(\ref{eq:ineq1}) \& (\ref{eq:ineq2}) $\qquad \Longrightarrow $
\begin{equation}
  E_{gs}+E_{gs}'<E_{gs}+E_{gs}' \qquad \text{\alert{Contradiction!}} \nonumber
\end{equation}
\end{proof}
 }
 \end{small}
 }




\frame
{ 
\frametitle{}
\begin{small}
{\scriptsize
    
Define
\begin{equation}
  E_{v_{0}}[\rho]:=\bra{\Psi[\rho]}\hat{T}+\hat{W}+\hat{V_{0}}\ket{\Psi[\rho]} \nonumber
\end{equation}
$\hat{V_{0}} =$ external potential, $n_{0}(\mathbf{r}) =$ corresponding GS density, $E_{0} =$ GS energy
\pause

\vspace{2mm}
Rayleigh-Ritz principle $\quad \Longrightarrow \quad $ \alert{second statement of H-K theorem}:
\begin{equation}
  E_{0}=\min_{n\in \mathcal{N}}E_{v_{0}}[\rho] \nonumber
\end{equation}
\pause
\alert{Last satement of H-K theorem}:
\begin{equation}
  F_{HK}[\rho]\equiv \bra{\Psi[\rho]}\hat{T}+\hat{W}\ket{\Psi[\rho]} \nonumber
\end{equation}
is \emph{universal} ($F_{HK}\neq F_{HK}[\hat{V_{0}}]$)
} 
\end{small}
}



\section[Week 17]{Week 17}
\frame
{
  \frametitle{Topics for Week 17, April 25-29}
  \begin{block}{Density functional theory}
\begin{itemize}
\item Repetition from last week
\item More density functional theory (DFT): 
\begin{enumerate} \item The equations, overview \item Reminder on variational calculus and \item 
Hartree-Fock theory
\end{enumerate}
\end{itemize}
Project work this week: Try to finalize the  Slater determinant part. 
This part should be finalized before
May 1. 
  \end{block}
} 





\frame[containsverbatim]
{
  \frametitle{Intermezzo: Variational Calculus and Lagrangian Multiplier}
\begin{small}
{\scriptsize
The calculus of variations involves 
problems where the quantity to be minimized or maximized is an integral. 

In the general case we have an integral of the type
\[ E[\Phi]= \int_a^b f(\Phi(x),\frac{\partial \Phi}{\partial x},x)dx,\]
where $E$ is the quantity which is sought minimized or maximized.
The problem is that although $f$ is a function of the variables $\Phi$, $\partial \Phi/\partial x$ and $x$, the exact dependence of
$\Phi$ on $x$ is not known.  This means again that even though the integral has fixed limits $a$ and $b$, the path of integration is
not known. In our case the unknown quantities are the single-particle wave functions and we wish to choose an integration path which makes
the functional $E[\Phi]$ stationary. This means that we want to find minima, or maxima or saddle points. In physics we search normally for minima.
Our task is therefore to find the minimum of $E[\Phi]$ so that its variation $\delta E$ is zero  subject to specific
constraints. In our case the constraints appear as the integral which expresses the orthogonality of the  single-particle wave functions.
The constraints can be treated via the technique of Lagrangian multipliers
 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Euler-Lagrange equations}
\begin{small}
{\scriptsize
We assume the existence of an optimum path, that is a path for which $E[\Phi]$ is stationary. There are infinitely many such paths.
The difference between two paths $\delta\Phi$ is called the variation of $\Phi$.

We call the variation $\eta(x)$  and it is scaled by a factor $\alpha$.  The function $\eta(x)$ is arbitrary except
for 
\[ 
\eta(a)=\eta(b)=0,
\]
and we assume that we can model the change in $\Phi$ as 
\[
\Phi(x,\alpha) = \Phi(x,0)+\alpha\eta(x),
\]
and 
\[
\delta\Phi = \Phi(x,\alpha) -\Phi(x,0)=\alpha\eta(x).
\]
 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Euler-Lagrange equations}
\begin{small}
{\scriptsize
We choose $\Phi(x,\alpha=0)$ as the unkonwn path  that will minimize $E$.  The value 
$\Phi(x,\alpha\ne 0)$  describes a neighbouring path. 
 
We have
\[ E[\Phi(\alpha)]= \int_a^b f(\Phi(x,\alpha),\frac{\partial \Phi(x,\alpha)}{\partial x},x)dx.\] 
In the slides I will use  the shorthand
\[
\Phi_x(x,\alpha) = \frac{\partial \Phi(x,\alpha)}{\partial x}.
\]
In our case $a=0$ and $b=\infty$ and we know the value of the wave function.
 }
 \end{small}
 }




\frame[containsverbatim]
{
  \frametitle{Euler-Lagrange equations}
\begin{small}
{\scriptsize
The condition for an extreme of
\[ E[\Phi(\alpha)]= \int_a^b f(\Phi(x,\alpha),\Phi_x(x,\alpha),x)dx,\]
is
\[
\left[\frac{\partial  E[\Phi(\alpha)]}{\partial x}\right]_{\alpha=0} =0.\]
The $\alpha$ dependence is contained in $\Phi(x,\alpha)$ and $\Phi_x(x,\alpha)$ meaning that
\[
\left[\frac{\partial  E[\Phi(\alpha)]}{\partial \alpha}\right]=\int_a^b \left( \frac{\partial f}{\partial \Phi}\frac{\partial \Phi}{\partial \alpha}+\frac{\partial f}{\partial \Phi_x}\frac{\partial \Phi_x}{\partial \alpha}        \right)dx.\]
We have defined 
\[
\frac{\partial \Phi(x,\alpha)}{\partial \alpha}=\eta(x)
\]
and thereby 
\[
\frac{\partial \Phi_x(x,\alpha)}{\partial \alpha}=\frac{d(\eta(x))}{dx}.
\]

 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Euler-Lagrange equations}
\begin{small}
{\scriptsize
Using
\[
\frac{\partial \Phi(x,\alpha)}{\partial \alpha}=\eta(x),
\]
and 
\[
\frac{\partial \Phi_x(x,\alpha)}{\partial \alpha}=\frac{d(\eta(x))}{dx},
\]
in the integral gives 
\[
\left[\frac{\partial  E[\Phi(\alpha)]}{\partial \alpha}\right]=\int_a^b \left( \frac{\partial f}{\partial \Phi}\eta(x)+\frac{\partial f}{\partial \Phi_x}\frac{d(\eta(x))}{dx}        \right)dx.\]
Integrate the second term by parts

\[
\int_a^b \frac{\partial f}{\partial \Phi_x}\frac{d(\eta(x))}{dx}dx =\eta(x)\frac{\partial f}{\partial \Phi_x}|_a^b-
\int_a^b \eta(x)\frac{d}{dx}\frac{\partial f}{\partial \Phi_x}dx, 
\]
and since the first term dissappears due to $\eta(a)=\eta(b)=0$, we obtain
\[
\left[\frac{\partial  E[\Phi(\alpha)]}{\partial \alpha}\right]=\int_a^b \left( \frac{\partial f}{\partial \Phi}-\frac{d}{dx}\frac{\partial f}{\partial \Phi_x}
\right)\eta(x)dx=0.\]

 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Euler-Lagrange equations}
\begin{small}
{\scriptsize
\[
\left[\frac{\partial  E[\Phi(\alpha)]}{\partial \alpha}\right]=\int_a^b \left( \frac{\partial f}{\partial \Phi}-\frac{d}{dx}\frac{\partial f}{\partial \Phi_x}
\right)\eta(x)dx=0,\]
can also be written as 
\[
\alpha\left[\frac{\partial  E[\Phi(\alpha)]}{\partial \alpha}\right]_{\alpha=0}=\int_a^b \left( \frac{\partial f}{\partial \Phi}-\frac{d}{dx}\frac{\partial f}{\partial \Phi_x}
\right)\delta\Phi(x)dx=\delta E = 0.\]

The condition for a stationary value is thus a partial differential equation
\[
\frac{\partial f}{\partial \Phi}-\frac{d}{dx}\frac{\partial f}{\partial \Phi_x}=0,\]
known as Euler's equation.
Can easily be generalized to more variables.
 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Lagrangian Multipliers}
\begin{small}
{\scriptsize
Consider a function of three independent variables $f(x,y,z)$ . For the function $f$ to be an 
extreme we have
\[
df=0.
\]
A necessary and sufficient condition is
\[
\frac{\partial f}{\partial x} =\frac{\partial f}{\partial y}=\frac{\partial f}{\partial z}=0,
\]
due to 
\[
df = \frac{\partial f}{\partial x}dx+\frac{\partial f}{\partial y}dy+\frac{\partial f}{\partial z}dz.
\]
In physical problems the variables $x,y,z$ are often subject to constraints (in our case $\Phi$ and the orthogonality constraint)
so that they are no longer all independent. It is possible at least in principle to use each constraint to eliminate one variable
and to proceed with a new and smaller set of independent varables.
 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Lagrangian Multipliers}
\begin{small}
{\scriptsize
The use of so-called Lagrangian  multipliers is an alternative technique  when the elimination of
of variables is incovenient or undesirable.  Assume that we have an equation of constraint on the variables $x,y,z$
\[
\phi(x,y,z) = 0,
\]
 resulting in
\[
d\phi = \frac{\partial \phi}{\partial x}dx+\frac{\partial \phi}{\partial y}dy+\frac{\partial \phi}{\partial z}dz =0.
\]
Now we cannot set anymore 
\[
\frac{\partial f}{\partial x} =\frac{\partial f}{\partial y}=\frac{\partial f}{\partial z}=0,
\]
if $df=0$ is wanted 
because there are now only two independent variables!  Assume $x$ and $y$ are the independent variables.
Then $dz$ is no longer arbitrary. 
 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Lagrangian Multipliers}
\begin{small}
{\scriptsize
However, we can add to
\[
df = \frac{\partial f}{\partial x}dx+\frac{\partial f}{\partial y}dy+\frac{\partial f}{\partial z}dz,
\]
a multiplum of $d\phi$, viz. $\lambda d\phi$, resulting  in

\[
df+\lambda d\phi = (\frac{\partial f}{\partial z}+\lambda\frac{\partial \phi}{\partial x})dx+(\frac{\partial f}{\partial y}+\lambda\frac{\partial \phi}{\partial y})dy+
(\frac{\partial f}{\partial z}+\lambda\frac{\partial \phi}{\partial z})dz =0.
\]
Our multiplier is chosen so that
\[
\frac{\partial f}{\partial z}+\lambda\frac{\partial \phi}{\partial z} =0.
\]
 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Lagrangian Multipliers}
\begin{small}
{\scriptsize
However, we took $dx$ and $dy$ as to be arbitrary and thus we must have
\[
\frac{\partial f}{\partial x}+\lambda\frac{\partial \phi}{\partial x} =0,
\]
and
\[
\frac{\partial f}{\partial y}+\lambda\frac{\partial \phi}{\partial y} =0.
\]
When all these equations are satisfied, $df=0$.  We have four unknowns, $x,y,z$ and
$\lambda$. Actually we want only $x,y,z$, $\lambda$ need not to be determined, it is therefore often called
Lagrange's undetermined multiplier. 
If we have a set of constraints $\phi_k$ we have the equations
\[
\frac{\partial f}{\partial x_i}+\sum_k\lambda_k\frac{\partial \phi_k}{\partial x_i} =0.
\]
 }
 \end{small}
 }








\frame[containsverbatim]
{
  \frametitle{Variational Calculus and Lagrangian Multipliers}
\begin{small}
{\scriptsize
Let us specialize to the expectation value of the energy for one particle in three-dimensions.
This expectation value reads
\[
  E=\int dxdydz \psi^*(x,y,z) \hat{H} \psi(x,y,z),
\]
with the constraint
\[
 \int dxdydz \psi^*(x,y,z) \psi(x,y,z)=1,
\]
and a Hamiltonian
\[
\hat{H}=-\frac{1}{2}\nabla^2+V(x,y,z).
\]
I will skip the variables $x,y,z$ below, and write for example $V(x,y,z)=V$.
 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Variational Calculus and Lagrangian Multiplier}
\begin{small}
{\scriptsize
The integral involving the kinetic energy can be written as, if we assume periodic boundary conditions or that the function $\psi$ vanishes
strongly for large values of $x,y,z$, 
 \[
  \int dxdydz \psi^* \left(-\frac{1}{2}\nabla^2\right) \psi dxdydz = \psi^*\nabla\psi|+\int dxdydz\frac{1}{2}\nabla\psi^*\nabla\psi.
\]
Inserting this expression into the expectation value for the energy and taking the variational minimum  we obtain
\[
\delta E = \delta \left\{\int dxdydz\left( \frac{1}{2}\nabla\psi^*\nabla\psi+V\psi^*\psi\right)\right\} = 0.
\]
 }
 \end{small}
 }

\frame[containsverbatim]
{
  \frametitle{Variational Calculus and Lagrangian Multiplier}
\begin{small}
{\scriptsize
The constraint appears in integral form as 
\[
 \int dxdydz \psi^* \psi=\mathrm{constant},
\]
and multiplying with a Lagrangian multiplier $\lambda$ and taking the variational minimum we obtain the final variational equation
\[
\delta \left\{\int dxdydz\left( \frac{1}{2}\nabla\psi^*\nabla\psi+V\psi^*\psi-\lambda\psi^*\psi\right)\right\} = 0.
\]
Introducing the function  $f$
\[
  f =  \frac{1}{2}\nabla\psi^*\nabla\psi+V\psi^*\psi-\lambda\psi^*\psi=
\frac{1}{2}(\psi^*_x\psi_x+\psi^*_y\psi_y+\psi^*_z\psi_z)+V\psi^*\psi-\lambda\psi^*\psi,
\]
where we have skipped the dependence on $x,y,z$ and introduced the shorthand $\psi_x$, $\psi_y$ and $\psi_z$  for the various derivatives.
 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Variational Calculus and Lagrangian Multiplier}
\begin{small}
{\scriptsize
For $\psi^*$ the Euler  equation results in
\[
\frac{\partial f}{\partial \psi^*}- \frac{\partial }{\partial x}\frac{\partial f}{\partial \psi^*_x}-\frac{\partial }{\partial y}\frac{\partial f}{\partial \psi^*_y}-\frac{\partial }{\partial z}\frac{\partial f}{\partial \psi^*_z}=0,
\] 
which yields 
\[
    -\frac{1}{2}(\psi_{xx}+\psi_{yy}+\psi_{zz})+V\psi=\lambda \psi.
\]
We can then identify the  Lagrangian multiplier as the energy of the system. Then the last equation is 
nothing but the standard 
Schr\"odinger equation and the variational  approach discussed here provides 
a powerful method for obtaining approximate solutions of the wave function.

 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Finding the Hartree-Fock functional $E[\Phi]$}
\begin{small}
{\scriptsize
We rewrite our Hamiltonian 
\[
  \hat{H} = -\sum_{i=1}^N \frac{1}{2} \nabla^2_i 
  - \sum_{i=1}^N \frac{Z}{r_i} + \sum_{i<j}^N \frac{1}{r_{ij}},
\]
as
\[
    \hat{H} = \hat{H_0} + \hat{H_1} 
    = \sum_{i=1}^N\hat{h_i} + \sum_{i<j=1}^N\frac{1}{r_{ij}},
\]

\[
  \hat{h_i} = - \frac{1}{2} \nabla^2_i - \frac{Z}{r_i}.
\]
 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Finding the Hartree-Fock functional $E[\Phi]$}
\begin{small}
{\scriptsize
Let us denote the ground state energy by $E_0$. According to the
variational principle we have
\begin{equation*}
  E_0 \le E[\Phi] = \int \Phi^*\hat{H}\Phi d\mathbf{\tau}
\end{equation*}
where $\Phi$ is a trial function which we assume to be normalized
\begin{equation*}
  \int \Phi^*\Phi d\mathbf{\tau} = 1,
\end{equation*}
where we have used the shorthand $d\mathbf{\tau}=d\mathbf{r}_1d\mathbf{r}_2\dots d\mathbf{r}_N$.
 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Finding the Hartree-Fock functional $E[\Phi]$}
\begin{small}
{\scriptsize
In the Hartree-Fock method the trial function is the Slater
determinant which can be rewritten as 
\[
  \Psi(\mathbf{r}_1,\mathbf{r}_2,\dots,\mathbf{r}_N,\alpha,\beta,\dots,\nu) = \frac{1}{\sqrt{N!}}\sum_{P} (-)^PP\psi_{\alpha}(\mathbf{r}_1)
    \psi_{\beta}(\mathbf{r}_2)\dots\psi_{\nu}(\mathbf{r}_N)=\sqrt{N!}{\cal A}\Phi_H,
\]
where we have introduced the anti-symmetrization operator ${\cal A}$ defined by the 
summation over all possible permutations of two eletrons.
It is defined as
\[
  {\cal A} = \frac{1}{N!}\sum_{P} (-)^PP,
\]
with the the Hartree-function given by the simple product of all possible single-particle function (two for helium, four for beryllium and ten for
neon)
\[
  \Phi_H(\mathbf{r}_1,\mathbf{r}_2,\dots,\mathbf{r}_N,\alpha,\beta,\dots,\nu) =
  \psi_{\alpha}(\mathbf{r}_1)
    \psi_{\beta}(\mathbf{r}_2)\dots\psi_{\nu}(\mathbf{r}_N).
\]

 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Finding the Hartree-Fock functional $E[\Phi]$}
\begin{small}
{\scriptsize
Both $\hat{H_1}$ and $\hat{H_2}$ are invariant under electron
permutations, and hence commute with ${\cal A}$
\[
  [H_0,{\cal A}] = [H_1,{\cal A}] = 0.
\]
Furthermore, ${\cal A}$ satisfies
\[
  {\cal A}^2 = {\cal A},
\]
since every permutation of the Slater
determinant reproduces it.
 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Finding the Hartree-Fock functional $E[\Phi]$}
\begin{small}
{\scriptsize
 The expectation value of $\hat{H_1}$ 
\[
  \int \Phi^*\hat{H_0}\Phi d\mathbf{\tau} 
  = N! \int \Phi_H^*{\cal A}\hat{H_0}{\cal A}\Phi_H d\mathbf{\tau}
\]
is readily reduced to
\[
  \int \Phi^*\hat{H_0}\Phi d\mathbf{\tau} 
  = N! \int \Phi_H^*\hat{H_0}{\cal A}\Phi_H d\mathbf{\tau},
\]
which can be rewritten as 
\[
  \int \Phi^*\hat{H_0}\Phi  d\mathbf{\tau}
  = \sum_{i=1}^N \sum_{P} (-)^P\int 
  \Phi_H^*\hat{h_i}P\Phi_H d\mathbf{\tau}.
\]
 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Finding the Hartree-Fock functional $E[\Phi]$}
\begin{small}
{\scriptsize
The integral vanishes if two or more electrons are permuted in only one
of the Hartree-functions $\Phi_H$ because the individual orbitals are
orthogonal. We obtain then
\[
  \int \Phi^*\hat{H_0}\Phi  d\mathbf{\tau}
  = \sum_{i=1}^N \int \Phi_H^*\hat{h_i}\Phi_H  d\mathbf{\tau}.
\]
Orthogonality allows us to further simplify the integral, and we
arrive at the following expression for the expectation values of the
sum of one-body Hamiltonians 
\[
  \int \Phi^*\hat{H_0}\Phi  d\mathbf{\tau}
  = \sum_{\mu=1}^N \int \psi_{\mu}^*(\mathbf{r}_i)\hat{h_i}\psi_{\mu}(\mathbf{r}_i)
  d\mathbf{r}_i, 
\]
or just as 
\[
  \int \Phi^*\hat{H_0}\Phi  d\mathbf{\tau}
  = \sum_{\mu=1}^N \langle \mu | h | \mu\rangle. 
\]
 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Finding the Hartree-Fock functional $E[\Phi]$}
\begin{small}
{\scriptsize
The expectation value of the two-body Hamiltonian is obtained in a
similar manner. We have
\[
  \int \Phi^*\hat{H_1}\Phi d\mathbf{\tau} 
  = N! \int \Phi_H^*{\cal A}\hat{H_1}{\cal A}\Phi_H d\mathbf{\tau},
\]
which reduces to
\[
 \int \Phi^*\hat{H_1}\Phi d\mathbf{\tau} 
  = \sum_{i\le j=1}^N \sum_{P} (-)^P\int 
  \Phi_H^*\frac{1}{r_{ij}}P\Phi_H d\mathbf{\tau},
\]
by following the same arguments as for the one-body
Hamiltonian.
 Because of the dependence on the inter-electronic distance $1/r_{ij}$,  permutations of
two electrons no longer vanish, and we get
\[
  \int \Phi^*\hat{H_1}\Phi d\mathbf{\tau} 
  = \sum_{i < j=1}^N \int  
  \Phi_H^*\frac{1}{r_{ij}}(1-P_{ij})\Phi_H d\mathbf{\tau}.
\]
where $P_{ij}$ is the permutation operator that interchanges
electrons $i$ and $j$.
 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Finding the Hartree-Fock functional $E[\Phi]$}
\begin{small}
{\scriptsize
We use the assumption that the orbitals
are orthogonal, 
and obtain
\[
  \int \Phi^*\hat{H_1}\Phi d\mathbf{\tau} 
  = \frac{1}{2}\sum_{\mu=1}^N\sum_{\nu=1}^N
    \left[ \int \psi_{\mu}^*(\mathbf{r}_i)\psi_{\nu}^*(\mathbf{r}_j)\frac{1} 
    {r_{ij}}\psi_{\mu}(\mathbf{r}_i)\psi_{\nu}(\mathbf{r}_j)
    d\mathbf{r}_id\mathbf{r}_j \right.
\]
\[
\left.
  - \int \psi_{\mu}^*(\mathbf{r}_i)\psi_{\nu}^*(\mathbf{r}_j)
  \frac{1}{r_{ij}}\psi_{\mu}(\mathbf{r}_j)\psi_{\nu}(\mathbf{r}_i)
  d\mathbf{r_i}d\mathbf{r_j}
  \right].
\]
The first term is the so-called direct term or Hartree term, while the second is due to the Pauli principle and is called
exchange term or Fock term.
The factor  $1/2$ is introduced because we now run over
all pairs twice. 

The compact notation is 
\[
   \frac{1}{2}\sum_{\mu=1}^N\sum_{\nu=1}^N\left[\langle \mu\nu |\frac{1}{r_{ij}}|\mu\nu\rangle-\langle \mu\nu |\frac{1}{r_{ij}}|\nu\mu\rangle\right].
\]
 }
 \end{small}
 }




\frame[containsverbatim]
{
  \frametitle{Variational Calculus and Lagrangian Multiplier, Hartree-Fock}
\begin{small}
{\scriptsize
Our functional is written as 
\[
  E[\Phi] = \sum_{\mu=1}^N \int \psi_{\mu}^*(\mathbf{r}_i)\hat{h_i}\psi_{\mu}(\mathbf{r}_i) d\mathbf{r}_i 
  + \frac{1}{2}\sum_{\mu=1}^N\sum_{\nu=1}^N
   \left[ \int \psi_{\mu}^*(\mathbf{r}_i)\psi_{\nu}^*(\mathbf{r}_j)\frac{1} 
    {r_{ij}}\psi_{\mu}(\mathbf{r}_i)\psi_{\nu}(\mathbf{r}_j)
    d\mathbf{r}_id\mathbf{r}_j \right.
\]
\[ \left.
  - \int \psi_{\mu}^*(\mathbf{r}_i)\psi_{\nu}^*(\mathbf{r}_j)
  \frac{1}{r_{ij}}\psi_{\nu}(\mathbf{r}_i)\psi_{\mu}(\mathbf{r}_j)
  d\mathbf{r}_id\mathbf{r}_j\right]
\]
The more compact version is
\[
  E[\Phi] 
  = \sum_{\mu=1}^N \langle \mu | h | \mu\rangle+ \frac{1}{2}\sum_{\mu=1}^N\sum_{\nu=1}^N\left[\langle \mu\nu |\frac{1}{r_{ij}}|\mu\nu\rangle-\langle \mu\nu |\frac{1}{r_{ij}}|\nu\mu\rangle\right].
\]
 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Variational Strategies}
\begin{small}
{\scriptsize
With the given functional, we can perform at least two types of variational strategies.
\begin{itemize}
\item Vary the Slater determinant by changing the spatial part of the single-particle
wave functions themselves.  This is what we will do.
\item   Expand the single-particle functions in a known basis  and vary the coefficients, 
that is, the new single-particle wave function $|a\rangle$ is written as a linear expansion
in terms of a fixed basis (harmonic oscillator, Laguerre polynomials etc)
\[
\psi_a  = \sum_{\lambda} C_{a\lambda}\psi_{\lambda},
\]
 \end{itemize}
Both cases lead to a new Slater determinant which is related to the previous via  a unitary transformation.

 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Small exercise}
\begin{small}
{\scriptsize
\begin{enumerate}
\item Consider a Slater determinant built up of single-particle orbitals $\psi_{\lambda}$, 
with $\lambda = 1,2,\dots,N$.

The unitary transformation
\[
\psi_a  = \sum_{\lambda} C_{a\lambda}\psi_{\lambda},
\]
brings us into the new basis.  Show that the new basis is orthonormal.
\item Show that the new Slater determinant constructed from the new single-particle wave functions can be
written as the determinant based on the previous basis and the determinant of the matrix $C$.
\item Show that the old and the new Slater determinants are equal up to a complex constant with absolute value unity.
(Hint, $C$ is a unitary matrix). 
 \end{enumerate}

 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Hartree-Fock by varying the coefficients of a wave function expansion}
\begin{small}
{\scriptsize

We will use the second method and 
expand the single-particle functions in a known basis  and vary the coefficients, 
that is, the new single-particle wave function is written as a linear expansion
in terms of a fixed chosen orthogonal basis (for example harmonic oscillator, Laguerre polynomials etc)
\be
\psi_a  = \sum_{\lambda} C_{a\lambda}\psi_{\lambda}.
\label{eq:newbasis}
\ee
In this case we vary the coefficients $C_{a\lambda}$. If the basis has infinitely many solutions, we need
to truncate the above sum.  In all our equations we assume a truncation has been made.

The single-particle wave functions $\psi_{\lambda}({\bf r})$, defined by the quantum numbers $\lambda$ and ${\bf r}$
are defined as the overlap 
\[
   \psi_{\lambda}({\bf r})  = \langle {\bf r} | \lambda \rangle .
\]
 }
 \end{small}
 }

\frame[containsverbatim]
{
  \frametitle{Hartree-Fock by varying the coefficients of a wave function expansion}
\begin{small}
{\scriptsize
We will omit the radial dependence of the wave functions and 
introduce first the following shorthands for the Hartree and Fock integrals
\[
\langle \mu\nu|V|\mu\nu\rangle =  \int \psi_{\mu}^*(\mathbf{r}_i)\psi_{\nu}^*(\mathbf{r}_j)V(r_{ij})\psi_{\mu}(\mathbf{r}_i)\psi_{\nu}(\mathbf{r}_j)
    d\mathbf{r}_id\mathbf{r}_j,
\]
and 
\[
\langle \mu\nu|V|\nu\mu\rangle = \int \psi_{\mu}^*(\mathbf{r}_i)\psi_{\nu}^*(\mathbf{r}_j)
  V(r_{ij})\psi_{\nu}(\mathbf{r}_i)\psi_{\mu}(\mathbf{r}_j)
  d\mathbf{r}_id\mathbf{r}_j.  
\]
 }
 \end{small}
 }

\frame[containsverbatim]
{
  \frametitle{Hartree-Fock by varying the coefficients of a wave function expansion}
\begin{small}
{\scriptsize
Since the interaction is invariant under the interchange of two particles it means for example that we have
\[
\langle \mu\nu|V|\mu\nu\rangle =  \langle \nu\mu|V|\nu\mu\rangle,  
\]
or in the more general case
\[
\langle \mu\nu|V|\sigma\tau\rangle =  \langle \nu\mu|V|\tau\sigma\rangle.  
\]
 }
 \end{small}
 }

\frame[containsverbatim]
{
  \frametitle{Hartree-Fock by varying the coefficients of a wave function expansion}
\begin{small}
{\scriptsize
The direct and exchange matrix elements can be  brought together if we define the antisymmetrized matrix element
\[
\langle \mu\nu|V|\mu\nu\rangle_{AS}= \langle \mu\nu|V|\mu\nu\rangle-\langle \mu\nu|V|\nu\mu\rangle,
\]
or for a general matrix element  
\[
\langle \mu\nu|V|\sigma\tau\rangle_{AS}= \langle \mu\nu|V|\sigma\tau\rangle-\langle \mu\nu|V|\tau\sigma\rangle.
\]
It has the symmetry property
\[
\langle \mu\nu|V|\sigma\tau\rangle_{AS}= -\langle \mu\nu|V|\tau\sigma\rangle_{AS}=-\langle \nu\mu|V|\sigma\tau\rangle_{AS}.
\]
The antisymmetric matrix element is also hermitian, implying 
\[
\langle \mu\nu|V|\sigma\tau\rangle_{AS}= \langle \sigma\tau|V|\mu\nu\rangle_{AS}.
\]
 }
 \end{small}
 }

\frame[containsverbatim]
{
  \frametitle{Hartree-Fock by varying the coefficients of a wave function expansion}
\begin{small}
{\scriptsize
With these notations we rewrite the Hartree-Fock functional as
\begin{equation}
  \int \Phi^*\hat{H_1}\Phi d\mathbf{\tau} 
  = \frac{1}{2}\sum_{\mu=1}^A\sum_{\nu=1}^A \langle \mu\nu|V|\mu\nu\rangle_{AS}.
\label{H2Expectation2}
\end{equation}


Combining Eqs.~(\ref{H1Expectation1}) and
(\ref{H2Expectation2}) we obtain the energy functional 
\begin{equation}
  E[\Phi] 
  = \sum_{\mu=1}^N \langle \mu | h | \mu \rangle +
  \frac{1}{2}\sum_{{\mu}=1}^N\sum_{{\nu}=1}^N \langle \mu\nu|V|\mu\nu\rangle_{AS}.
\label{FunctionalEPhi}
\end{equation}
which we will use as our starting point for the Hartree-Fock calculations. 
 }
 \end{small}
 }

\frame[containsverbatim]
{
  \frametitle{Hartree-Fock by varying the coefficients of a wave function expansion}
\begin{small}
{\scriptsize
If we vary the above energy functional with respect to the basis functions $|\mu \rangle$, this corresponds to 
what was done in the previous case. We are however interested in defining a new basis defined in terms of
a chosen basis as defined in Eq.~(\ref{eq:newbasis}). We can then rewrite the energy functional as
\begin{equation}
  E[\Psi] 
  = \sum_{a=1}^N \langle a | h | a \rangle +
  \frac{1}{2}\sum_{ab=1}^N\langle ab|V|ab\rangle_{AS},
\label{FunctionalEPhi2}
\end{equation}
where $\Psi$ is the new Slater determinant defined by the new basis of Eq.~(\ref{eq:newbasis}). 

 }
 \end{small}
 }

\frame[containsverbatim]
{
  \frametitle{Hartree-Fock by varying the coefficients of a wave function expansion}
\begin{small}
{\scriptsize
Using Eq.~(\ref{eq:newbasis}) we can rewrite Eq.~(\ref{FunctionalEPhi2}) as 
\begin{equation}
  E[\Psi] 
  = \sum_{a=1}^N \sum_{\alpha\beta} C^*_{a\alpha}C_{a\beta}\langle \alpha | h | \beta \rangle +
  \frac{1}{2}\sum_{ab=1}^N\sum_{{\alpha\beta\gamma\delta}} C^*_{a\alpha}C^*_{b\beta}C_{a\gamma}C_{b\delta}\langle \alpha\beta|V|\gamma\delta\rangle_{AS}.
\label{FunctionalEPhi3}
\end{equation}
 }
 \end{small}
 }

\frame[containsverbatim]
{
  \frametitle{Hartree-Fock by varying the coefficients of a wave function expansion}
\begin{small}
{\scriptsize
We wish now to minimize the above functional. We introduce again a set of Lagrange multipliers, noting that
since $\langle a | b \rangle = \delta_{a,b}$ and $\langle \alpha | \beta \rangle = \delta_{\alpha,\beta}$, 
the coefficients $C_{a\gamma}$ obey the relation
\[
 \langle a | b \rangle=\delta_{a,b}=\sum_{\alpha\beta} C^*_{a\alpha}C_{a\beta}\langle \alpha | \beta \rangle=
\sum_{\alpha} C^*_{a\alpha}C_{a\alpha},
\]
which allows us to define a functional to be minimized that reads
\begin{equation}
  E[\Psi] - \sum_{a=1}^N\epsilon_a\sum_{\alpha} C^*_{a\alpha}C_{a\alpha}.
\end{equation}
 }
 \end{small}
 }

\frame[containsverbatim]
{
  \frametitle{Hartree-Fock by varying the coefficients of a wave function expansion}
\begin{small}
{\scriptsize
Minimizing with respect to $C^*_{k\alpha}$, remembering that $C^*_{k\alpha}$ and $C_{k\alpha}$
are independent, we obtain
\be
\frac{d}{dC^*_{k\alpha}}\left[  E[\Psi] - \sum_{a}\epsilon_a\sum_{\alpha} C^*_{a\alpha}C_{a\alpha}\right]=0,
\ee
which yields for every single-particle state $k$ the following Hartree-Fock equations
\be
\sum_{\gamma} C_{k\gamma}\langle \alpha | h | \gamma \rangle+
\sum_{a=1}^N\sum_{\beta\gamma\delta} C^*_{a\beta}C_{a\delta}C_{k\gamma}\langle \alpha\beta|V|\gamma\delta\rangle_{AS}=\epsilon_kC_{k\alpha}.
\ee
 }
 \end{small}
 }

\frame[containsverbatim]
{
  \frametitle{Hartree-Fock by varying the coefficients of a wave function expansion}
\begin{small}
{\scriptsize
We can rewrite this equation as 
\be
\sum_{\gamma} \left\{\langle \alpha | h | \gamma \rangle+
\sum_{a}^N\sum_{\beta\delta} C^*_{a\beta}C_{a\delta}\langle \alpha\beta|V|\gamma\delta\rangle_{AS}\right\}C_{k\gamma}=\epsilon_kC_{k\alpha}.
\ee
Note that the sums over greek indices run over the number of basis set functions (in principle an infinite number).
 }
 \end{small}
 }

\frame[containsverbatim]
{
  \frametitle{Hartree-Fock by varying the coefficients of a wave function expansion}
\begin{small}
{\scriptsize
Defining 
\[
h_{\alpha\gamma}^{HF}=\langle \alpha | h | \gamma \rangle+
\sum_{a=1}^N\sum_{\beta\delta} C^*_{a\beta}C_{a\delta}\langle \alpha\beta|V|\gamma\delta\rangle_{AS},
\]
we can rewrite the new equations as 
\be
\sum_{\gamma}h_{\alpha\gamma}^{HF}C_{k\gamma}=\epsilon_kC_{k\alpha}.
\label{eq:newhf}
\ee
Note again that the sums over greek indices run over the number of basis set functions (in principle an infinite number).
 }
 \end{small}
 }

\frame[containsverbatim]
{
  \frametitle{Hartree-Fock by varying the coefficients of a wave function expansion}
\begin{small}
{\scriptsize
The advantage of this approach is that we can calculate and tabulate the matrix elements
$\alpha | h | \gamma \rangle$ and  $\langle \alpha\beta|V|\gamma\delta\rangle_{AS}$ once and for all.
If the basis $|\alpha\rangle$ is chosen properly, then the matrix elements can also serve as a good starting
point for a Hartree-Fock calculation. Eq.~(\ref{eq:newhf}) is nothing but an eigenvalue problem. The eigenvectors
are defined by the coefficients $C_{k\gamma}$. 

The size of the matrices to diagonalize are seldomly larger than $100\times 100$ and can be solved
by the standard eigenvalue methods that are discussed in chapter 12 of the lecture notes. Jacobi's 
method is enough!! 
 }
 \end{small}
 }


\section[Week 18]{Week 18}
\frame
{
  \frametitle{Topics for Week 18, May 2-6}
  \begin{block}{Density functional theory}
\begin{itemize}
\item Repetition from last week and the final equations to program
\item More density functional theory (DFT), Kohn-Sham equations
\item Expressions for the Coulomb interaction and the single-particle wave functions. 
\end{itemize}
Project work this week: start programming the Kohn-Sham equations with only a Hartree term.
Set up the Hartree-Fock matrix and write a program which iterates the HF/Kohn-Sham equations.
  \end{block}
} 



\frame
{ 
  \frametitle{The Basic Kohn-Sham Equations}
  \begin{small}
    {\scriptsize
      \begin{itemize}
      \item So far: \\
        $\quad \quad $ H-K \alert{variational principle} $\quad \Longrightarrow \quad $ \\
        $\quad \quad $ exact GS density of many-particle system \\
        $\quad \quad $ \alert{Practically intractable !!}  
      %\item So far: The exact GS density of a many-particle system can in principle be determined by \alert{the variational principle} of Hohenberg and Kohn. This is, however, a \alert{practically intractable} problem.  
      \item Next step: \\
        $\quad \quad $ Kohn and Sham (1965): \alert{single-particle picture} \\
 $\quad \quad $ $\longrightarrow \quad $ equations solved \alert{selfconsistently} (iterative scheme) 
      \end{itemize}
    }
  \end{small}
}

\frame
{ 
  \frametitle{}
  \begin{small}
    {\scriptsize
      Hamiltonian of $N$ \emph{non-interacting} particles:
      \begin{equation}
        \hat{H}_{s}=\hat{T}+\hat{V}_{s} \nonumber
      \end{equation}
      Hohenberg and Kohn $\quad \Longrightarrow \quad $ $\exists \;$ unique energy functional 
      \begin{equation}
        E_{s}[\rho]=T_{s}[\rho]+\int v_{s}(\mathbf{r})\rho(\mathbf{r})d^{3}r \nonumber
      \end{equation}
      s. t. $\delta E_{s}[\rho]=0$ gives GS density $\rho_{s}(\mathbf{r})$ corresp. to $\hat{H}_{s}$


    }
  \end{small}
}

\frame
{ 
  \frametitle{}
  \begin{small}
    {\scriptsize


      \begin{theorem}
        Let 
        \begin{align}
          v_{s}(\mathbf{r})\quad &= \quad \text{local single-particle pot.}, \nonumber \\
          \rho(\mathbf{r})\quad &=\quad \text{GS density of interacting system}, \nonumber \\
          \rho_{s}(\mathbf{r})\quad &=\quad \text{GS density of non-interacting system} \nonumber 
        \end{align}
        $\Longrightarrow \quad $ for \alert{any interacting system},
        \begin{equation}
          \exists \;\; \text{ a }\;\; v_{s}(\mathbf{r}) \;\; \text{s. t.} \;\; \rho_{s}(\mathbf{r})=\rho(\mathbf{r})  \nonumber
        \end{equation}
      \end{theorem}
      \pause
      Proof in book by Dreizler/Gross, Sec. 4.2 \\
%      In proof: $F_{HK}[\rho]$ replaced by $F_{L}[\rho]$ 
%      rove this important theorem, but in the proof it is necessary to replace the H-K  functional $F_{HK}[\rho]$ by the Lieb functional $F_{L}[\rho]$ in the expression of $v_{s}(\mathbf{r})$.
    }
  \end{small}
}

\frame
{ 
  \frametitle{}
  \begin{small}
    {\scriptsize
      Assume \structure{nondegenerate GS}. Then
      \begin{equation}
        \rho(\mathbf{r})=\rho_{s}(\mathbf{r})=\sum_{i=1}^{N}\left\vert \phi_{i}(\mathbf{r})\right\vert^{2}, \nonumber
      \end{equation}
      where $\phi_{i}(\mathbf{r})$ are determined by
      \begin{equation}
        \left( -\frac{\hbar^{2}}{2m}\nabla^{2}+v_{s}(\mathbf{r})\right)\phi_{i}(\mathbf{r})=\varepsilon_{i}\phi_{i}(\mathbf{r}), \qquad \varepsilon_{1}\leq \varepsilon_{2}\leq \dots \;. \nonumber
      \end{equation}
      \pause

      \vspace{12mm}
      If $\exists $ $v_{s}(\mathbf{r})$, then H-K theorem gives \emph{uniqueness} of $v_{s}(\mathbf{r})$ \\
      Consequently, we may write
      \begin{equation}
        \phi_{i}(\mathbf{r})=\phi_{i}([\rho(\mathbf{r})]) \qquad \alert{\textbf{!!}} \nonumber
      \end{equation}
      
    }
  \end{small}
}

\frame
{ 
  \frametitle{}
  \begin{small}
    {\scriptsize
      Assume \\
      $\qquad \qquad \qquad \qquad \qquad $ $v_{0}(\mathbf{r}) =$ ext. potential \\
      $\qquad \qquad \qquad \qquad \qquad $ $\rho_{0}(\mathbf{r}) =$ GS density \\
      of \alert{interacting} system 
      \begin{itemize}
      \item Wanted: \structure{single-particle potential} $v_{s}(\mathbf{r})$ of \alert{non-interacting} system
      \end{itemize}
    }
  \end{small}
}

\frame
{ 
  \frametitle{Exchange-correlation functional}
  \begin{small}
    {\scriptsize
      Many-particle energy functional:
      \begin{align}
        E_{v_{0}}[\rho]&=F_{L}[\rho]+\int d^{3}v_{0}(\mathbf{r})\rho(\mathbf{r}) \nonumber \\
        &=\left(T_{s}[\rho]+\frac{1}{2}\iint d^{3}r d^{3}r' \rho(\mathbf{r})w(\mathbf{r},\mathbf{r}')\rho(\mathbf{r}')+E_{\mathrm{exc}}[\rho]\right) + \int d^{3}r v_{0}(\mathbf{r})\rho(\mathbf{r}) \nonumber 
      \end{align}
      Here \alert{exchange-correlation functional} defined: 
      \begin{equation}
        E_{\mathrm{exc}}[\rho]=F_{L}[\rho]-\frac{1}{2}\iint d^{3}r d^{3}r' \rho(\mathbf{r})w(\mathbf{r},\mathbf{r}')\rho(\mathbf{r}')-T_{s}[\rho] \nonumber
      \end{equation}
    }
  \end{small}
}

\frame
{ 
  \frametitle{}
  \begin{small}
    {\scriptsize
      The exchange-correlation functional defined: 
      \begin{equation}
        E_{\mathrm{exc}}[\rho]=F_{L}[\rho]-\frac{1}{2}\iint d^{3}r d^{3}r' \rho(\mathbf{r})w(\mathbf{r},\mathbf{r}')\rho(\mathbf{r}')-T_{s}[\rho] \nonumber
      \end{equation}
      \vspace{5mm}
      \alert{Explicit form} of $F_{L}[\rho]$ as functional of $\rho$ \alert{unknown}
      \begin{itemize}
        \item $E_{\mathrm{exc}}[\rho]$ unknown functional, must be approximated \\
          Otherwise, Kohn-Sham scheme exact 
      \end{itemize}
    }
  \end{small}
}

\frame
{ 
  \frametitle{}
  \begin{small}
    {\scriptsize
      %Side-remark:
      
      \begin{definition}
        Let $F:B\rightarrow \mathbb{R}$ be a \emph{functional} from  normed function space $B$ to real numbers $\mathbb{R}$. \\
        
        \vspace{2mm}
        The \alert{functional derivative} \\
        $\delta F[\rho]\equiv \delta F[\rho]/\delta \rho(\mathbf{r})$ is defined as
        \begin{equation}
          \frac{\delta F}{\delta \rho}[\varphi ]=\lim_{\varepsilon \rightarrow 0}\frac{F[\rho+\varepsilon \varphi ]-F[\rho]}{\varepsilon } \nonumber
        \end{equation}
        Another useful definition of $\delta F[\rho]$:
        \begin{equation}
          \left< \delta F[\rho],\varphi \right>=\frac{d}{d\varepsilon }F[\rho+\varepsilon \phi ]\Bigg\vert_{\varepsilon =0}, \nonumber
        \end{equation}
        where
        \begin{equation}
          \left< \delta F[\rho],\varphi \right>\equiv \int d\mathbf{r}(\delta F[\rho(\mathbf{r})])\varphi(\mathbf{r}), \nonumber
        \end{equation}
        $\varphi =$ test function 
      \end{definition}
    }
  \end{small}
}

\frame
{ 
  \frametitle{}
  \begin{small}
    {\scriptsize
      Let us \alert{derive} expression for \alert{single-particle potential} $v_{s}(\mathbf{r})$ of non-interacting system:
      
      \vspace{10mm}
      H-K variational principle:
      \begin{align} \label{eq:hkVar}
        0&=\delta E_{v_{0}}=E_{v_{0}}[\rho_{0}+\delta \rho]-E_{v_{0}}[\rho_{0}] \nonumber \\
        &=\delta T_{s}+\int d^{3}r\delta \rho(\mathbf{r})\left[v_{0}(\mathbf{r})+\int w(\mathbf{r},\mathbf{r}')d^{3}r'+v_{\mathrm{exc}}([\rho_{0}];\mathbf{r})\right],   
      \end{align}
      where exchange-coorelation potential 
      \begin{equation}
        v_{\mathrm{exc}}([\rho_{0}];\mathbf{r})=\frac{\delta E_{\mathrm{exc}}[\rho]}{\delta \rho(\mathbf{r})}\Bigg\vert_{\rho_{0}}, \nonumber
      \end{equation}
      $\rho_{0}(\mathbf{r}) =$ GS density
    }
  \end{small}
}

\frame
{ 
  \frametitle{}
  \begin{small}
    {\scriptsize
      $\rho_{0}(\mathbf{r})+\delta \rho(\mathbf{r})$ non-interacting $v$-representable $\quad \Longrightarrow \quad $ unique representation $\phi_{i,0}(\mathbf{r})+\delta \phi_{i}(\mathbf{r})$ 
      \vspace{10mm}
      
      \begin{align} \label{eq:dT}
        \delta T_{s}&=\sum_{i}^{N}\int d^{3}r\left[\delta \phi_{i}^{*}(\mathbf{r})\left(-\frac{\hbar^{2}}{2m}\nabla^{2}\right)\phi_{i,0}(\mathbf{r})+\phi_{i,0}^{*}(\mathbf{r})\left(-\frac{\hbar^{2}}{2m}\nabla^{2}\right)\delta \phi_{i}(\mathbf{r})\right] \nonumber \\
        &=\sum_{i}^{N}\int d^{3}r\left[\delta \phi_{i}^{*}(\mathbf{r})\left(-\frac{\hbar^{2}}{2m}\nabla^{2}\right)\phi_{i,0}(\mathbf{r})+\delta \phi_{i,0}^{*}(\mathbf{r})\left(-\frac{\hbar^{2}}{2m}\nabla^{2}\right)\phi_{i}(\mathbf{r})\right]
      \end{align}
      \put(38,60){\vector(0,1){20}}
      \put(30,45){Green's first identity}
      

    }
  \end{small}
}

\frame
{ 
  \frametitle{}
  \begin{small}
    {\scriptsize
      Green's first identity:
      \begin{equation}
        \int_{V}f\:\nabla^{2}g\:dV=\oint_{S}f(\nabla g\cdot n)\:dS-\int_{V}\nabla f\cdot \nabla g\:dV, \nonumber
      \end{equation}
      where $V\in \mathbb{R}^{3}$, $S\equiv \partial V\in \mathbb{R}^{2}$ and $f$, $g =$ arb. real scalar functions

      \vspace{10mm}
      Let surface $\partial V$ approach infinity w.r.t. origin, \\
      $\quad \quad $ assume $f,g \longrightarrow 0$ on $\partial V$, \\
      $\quad \quad $ Apply Green's first identity twice $\quad \Longrightarrow $
      \begin{align}
        \int_{V}f\:\nabla^{2}g\:dV&=0-\int_{V}\nabla f\cdot \nabla g\:dV \nonumber \\
        &=-\left(0-\int_{V}\nabla f\cdot \nabla g\:dV\right) \nonumber \\
        &=\int_{V}g\:\nabla^{2}f\:dV \nonumber
      \end{align}

    }
  \end{small}
}


\frame
{ 
  \frametitle{}
  \begin{small}
    {\scriptsize
      The orbitals $\phi_{i,0}(\mathbf{r})$ in Eq. (\ref{eq:dT}) satisfy
      \begin{equation}
        \left(-\frac{\hbar^{2}}{2m}\nabla^{2}+v_{s,0}(\mathbf{r})\right)\phi_{i,0}(\mathbf{r})=\varepsilon_{i}\phi_{i,0}(\mathbf{r}), \qquad \varepsilon_{1}\geq \varepsilon_{2} \geq \dots \;.
      \end{equation}
      Using this relation, we may rewrite Eq. (\ref{eq:dT}) as
      \begin{align} \label{eq:dT2}
        \delta T_{s}&=\sum_{i}^{N}\int d^{3}r\left[\delta \phi_{i}^{*}(\mathbf{r})\left(\varepsilon_{i}-v_{s,0}(\mathbf{r})\right)\phi_{i,0}(\mathbf{r})+\delta \phi_{i}(\mathbf{r})\left(\varepsilon_{i}-v_{s,0}(\mathbf{r})\right)\phi_{i}^{*}(\mathbf{r})\right] \nonumber \\
        &=\sum_{i=1}^{N}\varepsilon_{i}\int d^{3}r \delta \vert \phi_{i}(\mathbf{r})\vert^{2}-\sum_{i=1}^{N}\int d^{3}r v_{s,0}(\mathbf{r})\delta \vert \phi_{i}(\mathbf{r})\vert^{2}.
      \end{align}
   
    }
  \end{small}
}

\frame
{ 
  \frametitle{}
  \begin{small}
    {\scriptsize
      Since
      \begin{align}
        \int d^{3}r \delta\vert \phi_{i}(\mathbf{r})\vert^{2}&=\int d^{3}r \left[\vert \phi_{i,0}(\mathbf{r})+\delta \phi_{i,0}(\mathbf{r})\vert^{2}-\vert \phi_{i,0}(\mathbf{r})\vert^{2}\right] \nonumber \\
        &=1-1=0,
      \end{align}
      the first term of Eq. (\ref{eq:dT2}) vanishes, and we get
      \begin{equation} \label{eq:dT3}
        \delta T_{s}=-\int d^{3}r v_{s,0}(\mathbf{r})\delta \rho(\mathbf{r}).
      \end{equation}
      Combine Eqs. (\ref{eq:hkVar}) and (\ref{eq:dT3}): $\quad \Longrightarrow \quad $ total single-particle potential: 
      \begin{equation}
        v_{s,0}(\mathbf{r})=v_{0}(\mathbf{r})+\int d^{3}r' w(\mathbf{r},\mathbf{r}')\rho_{0}(\mathbf{r}')+v_{\mathrm{exc}}([\rho_{0}];\mathbf{r})
      \end{equation}

    }
  \end{small}
}

\frame
{ 
  \frametitle{The Kohn-Sham scheme I}
  \begin{small}
    {\scriptsize
      The \alert{classic Kohn-Sham} scheme:
      \begin{equation}
         \left(-\frac{\hbar^{2}}{2m}\nabla^{2}+v_{s,0}(\mathbf{r})\right)\phi_{i,0}(\mathbf{r})=\varepsilon_{i}\phi_{i,0}(\mathbf{r}), \qquad \varepsilon_{1}\geq \varepsilon_{2} \geq \dots \;, \nonumber
      \end{equation}
      where
      \begin{equation}
        v_{s,0}(\mathbf{r})=v_{0}(\mathbf{r})+\int d^{3}r' w(\mathbf{r},\mathbf{r}')\rho_{0}(\mathbf{r}')+v_{\mathrm{exc}}([\rho_{0}];\mathbf{r}) \nonumber 
      \end{equation}
      The density calculated as 
      \begin{equation}
        \rho_{0}(\mathbf{r})=\sum_{i=1}^{N}\vert \phi_{i,0}(\mathbf{r})\vert^{2}, \nonumber
      \end{equation}
      Equation \alert{solved selfconsistently} 
      \pause
      
      Total energy: 
      \begin{equation}
        E=\sum_{i=1}^{N}\varepsilon_{i}-\frac{1}{2}\int d^{3}r d^{3}r'\rho(\mathbf{r})w(\mathbf{r},\mathbf{r}')\rho(\mathbf{r}')+E_{\mathrm{exc}}[\rho]-\int d^{3}r v_{\mathrm{exc}}([\rho];\mathbf{r})\rho(\mathbf{r}) \nonumber
      \end{equation}
    }
  \end{small}
}


\frame
{ 
  \frametitle{The Kohn-Sham scheme II}
  \begin{small}
    {\scriptsize
      \alert{Kohn-Sham} scheme for systems with \alert{degenerate} GS:
            \begin{equation}
         \left(-\frac{\hbar^{2}}{2m}\nabla^{2}+v_{s,0}(\mathbf{r})\right)\phi_{i,0}(\mathbf{r})=\varepsilon_{i}\phi_{i,0}(\mathbf{r}), \qquad \varepsilon_{1}\geq \varepsilon_{2} \geq \dots \;, \nonumber
      \end{equation}
      where
      \begin{equation}
        v_{s,0}(\mathbf{r})=v_{0}(\mathbf{r})+\int d^{3}r' w(\mathbf{r},\mathbf{r}')\rho_{0}(\mathbf{r}')+v_{\mathrm{exc}}([\rho_{0}];\mathbf{r}) \nonumber 
      \end{equation}
      and
      \begin{align}
        v_{\mathrm{exc}}([\rho];\mathbf{r})&=\frac{\delta E_{\mathrm{exc}}[\rho]}{\delta \rho(\mathbf{r})} \nonumber \\
        &=\frac{\delta }{\delta \rho(\mathbf{r})}\left(F_{L}[\rho]-\frac{1}{2}\iint d^{3}r d^{3}r' \rho(\mathbf{r})w(\mathbf{r},\mathbf{r}')\rho(\mathbf{r}')-T_{L}[\rho]\right) \nonumber 
      \end{align}

    }
  \end{small}
}

\frame
{ 
  \frametitle{The Kohn-Sham scheme II}
  \begin{small}
    {\scriptsize
      Density of degen. K-S scheme:
      \begin{equation}
        \rho_{0}(\mathbf{r})=\sum_{i=1}^{N}\gamma_{i}\vert \phi_{i,0}(\mathbf{r})\vert^{2}, \nonumber
      \end{equation}
      occupation numbers $\gamma_{i}$ satisfy 
      \begin{align}
        \gamma_{i}=1:\; &\varepsilon_{i}<\mu \nonumber \\
        0\leq \gamma_{i}\leq 1:\; &\varepsilon_{i}=\mu \nonumber \\
        \gamma_{i}=0:\; &\varepsilon_{i}>\mu \nonumber
      \end{align}
      and
      \begin{equation}
        \sum_{i=1}^{N}\gamma_{i}=N \nonumber
      \end{equation}
    }
  \end{small}
}

\frame
{ 
  \frametitle{Exchange Energy and Correlation Energy}
  \begin{small}
    {\scriptsize

      \structure{Hartree-Fock} equation:
      \begin{align}
        \left(-\frac{\hbar^{2}}{2m}\nabla^{2}+v_{0}(\mathbf{r})+\int d^{3}r'w(\mathbf{r},\mathbf{r}')\rho(\mathbf{r}')\right)\phi_{k}(\mathbf{r})& \nonumber \\
        \underbrace{-\sum_{l=1}^{N}\int d^{3}r'\phi_{l}^{*}(\mathbf{r}')w(\mathbf{r},\mathbf{r}')\phi_{k}(\mathbf{r}')\phi_{l}(\mathbf{r})}_{\text{exchange term}}
        &=\varepsilon_{k}\phi_{k}(\mathbf{r}), \nonumber
      \end{align}
      
      %\begin{itemize}
      $\qquad \qquad $ \alert{Non-local} exchange term (Pauli exclusion principle)
      %\end{itemize}

      \vspace{5mm}     
      \structure{Kohn-Sham} equation:

      \begin{equation}
         \left(-\frac{\hbar^{2}}{2m}\nabla^{2}+v_{0}(\mathbf{r})
           +\int d^{3}r'w(\mathbf{r},\mathbf{r}')\rho(\mathbf{r}')
           +\underbrace{v_{\mathrm{exc}}([\rho];\mathbf{r})}_{\text{exchange + correlation}}
         \right)\phi_{k}(\mathbf{r})=\varepsilon_{k}\phi_{k}(\mathbf{r}), \nonumber
       \end{equation}
       %\begin{itemize}
       $\qquad \qquad $  \alert{Local} exchange-correlation term
       %\end{itemize}
      
    }
  \end{small}
}

\frame
{ 
  \frametitle{}
  \begin{small}
    {\scriptsize
      Exchange-correlation energy $=$ \structure{Exchange energy} $+$ \alert{Correlation energy}
      \begin{equation}
        E_{\mathrm{exc}}[\rho]=E_{x}[\rho]+E_{c}[\rho] \nonumber
      \end{equation}
      From earlier: 
      \begin{equation}
        E_{\mathrm{exc}}[\rho]=F_{L}[\rho]-T_{s}[\rho]-\frac{1}{2}\iint d^{3}r d^{3}r' \rho(\mathbf{r})w(\mathbf{r},\mathbf{r}')\rho(\mathbf{r}') \nonumber
      \end{equation}
      \vspace{10mm}
      \begin{center}
        We want to show: $\quad $$E_{c}[\rho]\leq 0$
      \end{center}
    }
  \end{small}
}

\frame
{ 
  \frametitle{}
  \begin{small}
    {\scriptsize
      Here we have (assume $F_{L}[\rho]=F_{LL}[\rho]$)
      \begin{align}
        F_{L}[\rho]&\equiv \inf_{\Psi \rightarrow n}\bra{\Psi }\hat{T}+\hat{W}\ket{\Psi } \nonumber \\
        &=\bra{\Psi_{n}^{min}}\hat{T}+\hat{W}\ket{\Psi_{n}^{min}}, \nonumber 
      \end{align}
      and 
      \begin{align}
        T_{s}[\rho]&\equiv \inf_{\Psi \rightarrow n}\bra{\Psi }\hat{T}\ket{\Psi }=\bra{\Phi_{n}^{min}}\hat{T}\ket{\Phi_{n}^{min}}, \nonumber 
      \end{align}
      $\Psi = $ normalized, antisymm. $N$-particle wavefunction, \\
      $\Phi_{n}^{min}$ lin. komb. of Slater determinants of \\
      single-particle orbitals $\psi_{i}(r_{j})$ 
    }
  \end{small}
}

\frame
{ 
  \frametitle{}
  \begin{small}
    {\scriptsize
      Eq. (4.35) in J. M. Thijssen: \emph{Computational Physics}:
      \begin{align}
        \bra{\Phi_{n}^{min}}\hat{W}\ket{\Phi_{n}^{min}}&=\frac{1}{2}\sum_{k,l}\bigg[ \iint d^{3}r d^{3}r' \rho(\mathbf{r})w(\mathbf{r},\mathbf{r}')\rho(\mathbf{r}') \nonumber \\
        &-\iint d^{3}r d^{3}r' \psi_{l}^{*}(\mathbf{r})\psi_{l}(\mathbf{r}')w(\mathbf{r},\mathbf{r}')\psi_{k}^{*}(\mathbf{r}')\psi_{k}(\mathbf{r})\bigg] \nonumber
      \end{align}
      By definition,
      \begin{equation}
        E_{x}[\rho]\equiv -\frac{1}{2}\sum_{k,l}\iint d^{3}r d^{3}r' \psi_{l}^{*}(\mathbf{r})\psi_{l}(\mathbf{r}')w(\mathbf{r},\mathbf{r}')\psi_{k}^{*}(\mathbf{r}')\psi_{k}(\mathbf{r})  \nonumber
      \end{equation}
    }
  \end{small}
}

\frame
{ 
  \frametitle{}
  \begin{small}
    {\scriptsize
      Using expressions from previous pages gives
      \begin{align}
        E_{c}[\rho]&=E_{\mathrm{exc}}[\rho]-E_{x}[\rho] \nonumber \\
        &=F_{L}[\rho]-T_{s}[\rho]-\frac{1}{2}\iint d^{3}r d^{3}r' \rho(\mathbf{r})w(\mathbf{r},\mathbf{r}')\rho(\mathbf{r}') \nonumber \\
        &+\frac{1}{2}\sum_{k,l}\iint d^{3}r d^{3}r' \psi_{l}^{*}(\mathbf{r})\psi_{l}(\mathbf{r}')w(\mathbf{r},\mathbf{r}')\psi_{k}^{*}(\mathbf{r}')\psi_{k}(\mathbf{r}) \nonumber \\
        &=\bra{\Psi_{n}^{min}}\hat{T}+\hat{W}\ket{\Psi_{n}^{min}}-\bra{\Phi_{n}^{min}}\hat{T}+\hat{W}\ket{\Phi_{n}^{min}} \nonumber
      \end{align}

      Since
      \begin{equation}
        \bra{\Psi_{n}^{min}}\hat{T}+\hat{W}\ket{\Psi_{n}^{min}}=\inf_{\Psi \rightarrow n}\bra{\Psi }\hat{T}+\hat{W}\ket{\Psi }, \nonumber
      \end{equation}
      we see that  
      \begin{equation}
        E_{c}[\rho]\leq 0 \nonumber
      \end{equation}
    }
  \end{small}
}




\frame
{
  \frametitle{Structure of part 2}
The structure of the Hartree-Fock/DFT part involves
\begin{enumerate}
\item  Choice of basis: Harmonic oscillator. If we use polar coordinates
we need a function to compute the Laguerre polynomials.
This function will be discussed next week.
\item  Diagonalization of an eigenvalue problem in order to find the coefficients.
One can use Jacobi's method or Householder's with Givens' transformations, see chapter 12
of lecture notes. Included in lib.cpp as tred2 and tqli.
\item Computation of the Coulomb matrix elements. We provide a function and closed-form expression
for programming the Coulomb interaction. There is no need for numerical integration.
\end{enumerate}
} 



\frame[containsverbatim]
{
  \frametitle{Useful expressions for the Coulomb term and code}
\begin{small}
{\scriptsize
The expression is taken from E.~Anisimovas and A.~Matulis, J.~Phys.: Condens.~Matter {\bf 10},  601
(1998).

This function computes the Coulomb matrix element $\langle \alpha \beta |V|\gamma \delta \rangle_{as}=\langle \alpha(\bfv{r_{i}}) \beta(\bfv{r_{j}}) |V(\bfv{r_{ij}})| \gamma(\bfv{r_{i}})  \delta(\bfv{r_{j}}) \rangle_{as}$ where the $\alpha, \, \beta,\, \gamma$ and $\delta$ are four state indices and $\bf{r_i}, \, \bf{r_j}$ the positions of particle $i$ and $j$.
Each state $|k \rangle$ can be rewritten in terms of its quantum numbers. In two dimensions, it reads $|k \rangle=|n_k m_k s_k \rangle$. For simplicity the angular momentum projection quantum number $m_l$ will just be written as $m$ in the following equations.

 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Useful expressions for the Coulomb term and code}
\begin{small}
{\scriptsize
The complete anti-symmetrized Coulomb matrix element reads
\begin{equation}
\langle \alpha \beta |V|\gamma \delta \rangle_{as} =  \underbrace{\langle \alpha \beta |V|\gamma \delta \rangle}_{\begin{smallmatrix}
  \text{direct} \\
  \text{term}
\end{smallmatrix}} - \underbrace{\langle \alpha \beta |V|\delta \gamma\rangle}_{\begin{smallmatrix}
  \text{exchange} \\
  \text{term}
\end{smallmatrix}},
\end{equation} 

 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Useful expressions for the Coulomb term and code}
\begin{small}
{\scriptsize
The exchange term $ \langle \alpha \beta |V| \delta\gamma \rangle$ expands as follow
\begin{align}
\langle \alpha \beta |V|\delta\gamma \rangle &= \delta_{m_{s1},m_{s4}} \; \delta_{m_{s2},m_{s3}} \; \langle (n_1,m_1),(n_2,m_2) | V | (n_4,m_4), (n_3,m_3), \rangle\\
&= \delta_{m_{s1},m_{s4}} \; \delta_{m_{s2},m_{s3}} \;  V_{\alpha \beta\delta\gamma}
\end{align} 
where we separate the spin part from the spatial part $V_{\alpha \beta |V|\delta\gamma}$.
Which are the constraints for the direct part?

Note that the function \texttt{coulomb(n1,m1,n2,m2,n3,m3,n4,m4)} only computes  $V_{1234}$ where the numbers $1\rightarrow4$ are state indices similar to $\alpha,\, \beta,\, \gamma, \, \delta$.

We will not need the exchange in our Kohn-Sham equations!
 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Useful expressions for the Coulomb term and code}
\begin{small}
{\scriptsize
The expression for the Coulomb integral can be written as 
\begin{align}
\begin{split}
& V_{1234}= \delta_{m_1+m_2,m_3+m_4} \; \sqrt{ \left[ \prod_{i=1}^4 \frac{n_i !}{(n_i+|m_i|!)} \right] } \\
&\times \sum_{j_1=0,\dots,j_4=0}^{n_1,\dots,n_4} \Bigg[ \frac{(-1)^{j_1+j_2+j_3+j_4}} {j_1!j_2!j_3!j_4!} \; \left[ \prod_{k=1}^4 \begin{pmatrix} n_k+|m_k|\\\n_k-j_k\end{pmatrix}  \right]  \; \frac{1}{2^{\frac{G+1}{2}}}   \\
&\times  \sum_{l_1=0,\dots,l_4=0}^{\gamma_1=0,\dots,\gamma_4=0} \left( \delta_{l_1,l_2} \; \delta_{l_3,l_4} \; (-1)^{\gamma_2+\gamma_3-l_2-l_3} \left[ \prod_{t=1}^4 \begin{pmatrix} \gamma_t\\l_t\end{pmatrix} \right] \; \Gamma \left(1+\frac{\Lambda}{2} \right) \; \Gamma \left(\frac{G - \Lambda +1}{2}\right)    \right)  \Bigg]
\end{split}
\end{align} 

where
\begin{align*}
&\gamma_1=j_1+j_4+\frac{|m_1|+m_1}{2}+\frac{|m_4|-m_4}{2} \\
&\gamma_2=j_2+j_3+\frac{|m_2|+m_2}{2}+\frac{|m_3|-m_3}{2} \\
&\gamma_3=j_3+j_2+\frac{|m_3|+m_3}{2}+\frac{|m_2|-m_2}{2} \\
&\gamma_4=j_4+j_1+\frac{|m_4|+m_4}{2}+\frac{|m_1|-m_1}{2} \\
&G=\gamma_1+\gamma_2+\gamma_3+\gamma_4 \\
&\Lambda = l_1 +l_2+l_3 +l_4,
\end{align*}
when the basis set is built upon the single harmonic oscillator orbitals.
 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Useful expressions for the Coulomb term and code}
\begin{small}
{\scriptsize
As a note, our implementation of the function \texttt{coulomb($\dots$)} includes the following subfunctions:
\begin{itemize}
 \item \texttt{minusPower(int $k$)} which computes $(-1)^k$

\item \texttt{LogFac(int $n$)} which computes $log_e(n!)$

\item  \texttt{LogRatio1(int $j_1$,int $j_2$,int $j_3$,int $j_4$)} which computes the  $log_e$ of $\frac{1}{j_1!j_2!j_3!j_4!}$

\item \texttt{LogRatio2(int $G$)} which computes the $log_e$ of $\frac{1}{2^{\frac{G+1}{2}}}$


\end{itemize}
and
 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Useful expressions for the Coulomb term and code}
\begin{small}
{\scriptsize
and
\begin{itemize}
\item \texttt{Product1 (int $n_1$,int $m_1$,int $n_2$,int $m_2$, int $n_3$,int $m_3$,int $n_4$,int $m_4$)} which computes the explicit (not the $log_e$) product $\sqrt{ \left[ \prod_{i=1}^4 \frac{n_i !}{(n_i+|m_i|!)} \right] }$ 

\item \texttt{LogProduct2(int $n_1$,int $m_1$,int $n_2$,int $m_2$, int $n_3$,int $m_3$,int $n_4$,int $m_4$, int $j_1$,int $j_2$,int $j_3$,int $j_4$ )} which computes the  $log_e$ of $\prod_{k=1}^4 \begin{pmatrix} n_k+|m_k|\\n_k-j_k\end{pmatrix}$

\item \texttt{LogProduct3(int $l_1$,int $l_2$,int $l_3$,int $l_4$, int $\gamma_1$,int $\gamma_2$,int $\gamma_3$,int $\gamma_4$)} which computes the  $log_e$ of $\prod_{t=1}^4 \begin{pmatrix} \gamma_t\\l_t\end{pmatrix}$

\item \texttt{lgamma(double $x$)} which computes the  $log_e\left[ \Gamma(x) \right]$
\end{itemize}

 }
 \end{small}
 }



\section[Week 19]{Week 19}
\frame
{
  \frametitle{Topics for Week 19, May 9-13}
  \begin{block}{Density functional theory}
\begin{itemize}
\item Repetition from last week and the final equations to program
\item How to compute the local density term and Laguerre polynomials
\item How to use the variational Monte Carlo results to obtain the
correlation and exchange energy $E_{XC}$
\end{itemize}

  \end{block}
} 


\frame[containsverbatim]
{
  \frametitle{Laguerre functions}
\begin{small}
{\scriptsize
In our VMC codes we have used Cartesian coordinates and Hermite polynomials.
In our HF/DFT codes it is more convenient to work with spherical coordinates
and thereby Laguerre polynomials.  The single-particle wave function is
given by (with $\alpha = \sqrt{m\omega/\hbar}$)
\begin{equation}\label{eq:horm}
\psi_{nm_l}^{\mathrm{HO}}(r,\theta)=\alpha\exp{(\imath m\theta)}\sqrt{\frac{n!}{\pi(n+|m|)!}}(\alpha r)^{|m|}L_{n}^{|m|}(\alpha^2 r^2))\exp{(-\alpha^2 r^2/2)},
\end{equation}
with energy $\hbar\omega(2n+|m|+1)$.
 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Laguerre functions}
\begin{small}
{\scriptsize
In cartesian coordinates we have
\[
\phi_{n_x,n_y}(x,y) = A H_{n_x}(\sqrt{\omega}x)H_{n_y}(\sqrt{\omega}y)\exp{(-\omega(x^2+y^2)/2}.
\]
with energy  $\hbar\omega(n_x+n_y+1)$.
A function for computing the generalized Laguerre  polynomials $L_{n}^{|m|}(\alpha^2 r^2)$ is provided at the webpage of
the course under the program link (laguerre.cpp). 
 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Laguerre functions and densities}
\begin{small}
{\scriptsize
The reason we focus on this is that when we want to compare our densities from the HF/DFT calculations with those from the Monte Carlo calculations we must pay attention to the fact that
one calculation runs in cartesian coordinates while the other is set up in spherical coordinates.  This means that in the Monte Carlo calculation we have a density given by 
\[
\rho_{VMC}({\bf r}_1) = \int d{\bf r}_2d{\bf r}_3\dots d{\bf r}_N|\Psi({\bf r}_1,{\bf r}_2,\dots,{\bf r}_N)|^2
\]
with $\Psi$ our best possible VMC wave function while for the DFT/HF calculation we have
\[
 \rho_{DFT}({\bf r}_1)=\sum_{a=1}^N|\psi_{a}({\bf r}_1)|^2,
\]
where $\psi_{a}$ are the Kohn-Sham or Hartree-Fock single-particle wave functions and the sum
runs over all single-particle up till the Fermi level.
 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Laguerre functions and densities}
\begin{small}
{\scriptsize
In our DFT/HF code (due to the matrix eigenvalue problem)  we do not obtain the 
explicit radial dependence of the Kohn-Sham or Hartree-Fock single-particle wave functions $\psi_{a}$  that enter
\[
 \rho_{DFT}({\bf r}_1)=\sum_{a=1}^N|\psi_{a}({\bf r}_1)|^2,
\]
since we obtain only the single particle energies $e_{a}$ and the coefficients 
$C_{a\lambda}$ in 
\[
\psi_a({\bf r}_1)  = \sum_{\lambda} C_{a\lambda}\psi_{\lambda}^{\mathrm{HO}}({\bf r}_1).
\]
 }
 \end{small}
 }




\frame[containsverbatim]
{
  \frametitle{Laguerre functions and densities}
\begin{small}
{\scriptsize
To compute the Kohn-Sham or Hartree-Fock single-particle wave functions $\psi_{a}$  that enter
\[
 \rho_{DFT}({\bf r}_1)=\sum_{a=1}^N|\psi_{a}({\bf r}_1)|^2,
\]
we need therefore the harmonic oscillator wave functions 
of Eq.~(\ref{eq:horm})
in
\[
\psi_a({\bf r}_1)  = \sum_{\lambda} C_{a\lambda}\psi_{\lambda}^{\mathrm{HO}}({\bf r}_1).
\]
The coefficients 
$C_{a\lambda}$ result from our DFT calculations. With these ingredients we can then
compare densities and see if there are large differences.
 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Computing $E_{XC}$ from {\em ab initio} calculations}
\begin{small}
{\scriptsize
Question: can we compute the 'exact' $E_{XC}$ that enters DFT calculations? Yes! 

Let us define a continuous variable $\lambda$ and a Hamiltonian which depends on this variable
\[
\hat{H}_{\lambda}=\hat{T}+\lambda \hat{V}+\hat{v}_{\mathrm{ext}},
\]
where $\hat{T}$ is the kinetic energy, $\hat{V}$ is in our case the Coulomb interaction between two electrons an $\hat{v}_{\mathrm{ext}}$ is our external potential, here the two-dimensional
harmonic oscillator potential. 

For $\lambda=0$ we have the non-interacting system, whose solution in our case is a single
Slater determinant for the ground state (non-degenerate case). For $\lambda=1$ we
have the full interacting case. 
 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Computing $E_{XC}$ from {\em ab initio} calculations}
\begin{small}
{\scriptsize
The standard variational principle is to find the minimum of 
\[
E_{\lambda}[\hat{v}_{\mathrm{ext}}]=\inf_{\Psi\rightarrow\rho}\langle \Psi_{\lambda}|\hat{H}_{\lambda}|\Psi_{\lambda}\rangle,
\]
with respect to the wave function $\Psi_{\lambda}$.  If a maximizing potential 
$\hat{v}_{\mathrm{ext}}^{\lambda}$ exists, then according  to the Hohenberg and Kohn, it is the one which has the density $\rho$ as the ground state density and we have a functional
\[
F_{\lambda}[\rho] = E_{\lambda}[\hat{v}_{\mathrm{ext}}^{\lambda}]
-\int d{\bf r}\rho({\bf r}) \hat{v}_{\mathrm{ext}}^{\lambda}({\bf r}).
\]
 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Computing $E_{XC}$ from {\em ab initio} calculations}
\begin{small}
{\scriptsize
Which leads to the Lieb variational principle 
\[
F_{\lambda}[\rho] = \sup_{\hat{v}_{\mathrm{ext}}}\left(E_{\lambda}[\hat{v}_{\mathrm{ext}}^{\lambda}]
-\int d{\bf r}\rho({\bf r}) \hat{v}_{\mathrm{ext}}^{\lambda}({\bf r})\right).
\]
We define
\[
F_{\lambda}[\rho]= \langle \Psi_{\lambda}|\hat{T}+\lambda \hat{V}|\Psi_{\lambda}\rangle,
\]
which we rewrite as
\[
F_{\lambda}[\rho]= \langle \Psi_{\lambda}|\hat{T}|\Psi_{\lambda}\rangle+\lambda J[\rho]+E_{XC}[\rho],
\]
with the standard Hartree term
\[
J= \frac{1}{2}\int d{\bf r}_1d{\bf r}_2\rho({\bf r}_1)\rho({\bf r}_2)V(r_{12}).
\]
 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Computing $E_{XC}$ from {\em ab initio} calculations}
\begin{small}
{\scriptsize
We want to find $E_{XC}[\rho]$ in
\[
F_{\lambda}[\rho]= \langle \Psi_{\lambda}|\hat{T}|\Psi_{\lambda}\rangle+\lambda J[\rho]+E_{XC}[\rho].
\]
To do this, since we use a variational method, we can employ the Hellmann-Feynman theorem,
which states that
\[
\Delta E = \int_{\lambda_1}^{\lambda_2}d\lambda \frac{\partial E_{\lambda}}{\partial \lambda} =
\int_{\lambda_1}^{\lambda_2}d\lambda \langle \Psi_{\lambda}|\frac{\partial\hat{H}_{\lambda}}{\partial \lambda}|\Psi_{\lambda}\rangle.
\]
Setting $\lambda_1=0$ and $\lambda_2=1$ we arrive at 
\[
\Delta E = \int_{0}^{1}d\lambda \langle \Psi_{\lambda}|\hat{V}|\Psi_{\lambda}\rangle,
\] 
where the wave function at $\lambda =0$ is our single Slater determinant (no Jastrow factor). For $\lambda=1$ we can use our best variational Monte Carlo function.  Note that $\hat{V}$
is the full interaction at $\lambda=1$!
}
 \end{small}
 }




\frame[containsverbatim]
{
  \frametitle{Computing $E_{XC}$ from {\em ab initio} calculations}
\begin{small}
{\scriptsize
We wish to relate 
\[
\Delta E = \int_{0}^{1}d\lambda\langle \Psi_{\lambda}|\hat{V}|\Psi_{\lambda}\rangle,
\] 
to $E_{XC}$. 
Recalling that we defined 
\[
\langle \Psi_{\lambda}|\lambda \hat{V}|\Psi_{\lambda}\rangle=\lambda J[\rho]+E_{XC}[\rho],
\]
we rewrite our equation as
\[
E_{XC}= \int_{0}^{1}d\lambda\langle \Psi_{\lambda}|\hat{W}_{\lambda}|\Psi_{\lambda}\rangle,
\] 
where 
\[
W_{\lambda}=\langle \Psi_{\lambda}|\lambda \hat{V}|\Psi_{\lambda}\rangle-J.
\]
}
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Computing $E_{XC}$ from {\em ab initio} calculations}
\begin{small}
{\scriptsize
Using the fundamental theorem of calculus we have then
\[
\[
E_{XC}= \langle\Psi_{1}|\hat{V}|\Psi_{1}\rangle-\langle\Psi_{0}|\hat{V}|\Psi_{0}\rangle.
\]
We need thus simply to compute the expectation value of $\hat{V}$ for the single Slater
determinant $\lambda=0$ and the fully correlated wave 
function with the Jastrow factor as well
for the $\lambda=1$ case.  This is what is needed in exercise 2c). This results should then be compared with the correlation energy from the local density approximation in 2b).

The total correlation energy, including kinetic energy is then (computed at a fixed density)
equal to
\[
E_C=\langle\Psi_{1}|\hat{T}+\hat{V}|\Psi_{1}\rangle-\langle\Psi_{0}|\hat{T}+\hat{V}|\Psi_{0}\rangle.
\]
}
 \end{small}
 }






\section[Week 20]{Week 20}
\frame
{
  \frametitle{Topics for Week 20, May 16-20}
  \begin{block}{Finalize the project}
\begin{itemize}
\item Discussion of the structure of the report
\item Only project work
\end{itemize}

  \end{block}
} 

\frame
{
  \frametitle{The report}
  \begin{block}{What should it contain? A possible structure}
\begin{itemize}
\item An introduction where you explain the rational for the physics case and 
what you have done. At the end of the introduction you should give a brief
summary of the structure of the report
\item Theoretical models and technicalities. This is the methods section.
\item Results and discussion
\item Conclusions and perspectives
\item Appendix with extra material
\item Bibliography
\end{itemize}
  \end{block}
} 
  


\frame
{
  \frametitle{The report}
  \begin{block}{What should I focus on? Introduction}
You don't need to answer all questions in a chronological order.  When you write the introduction you could focus on the following aspects
\begin{itemize}
\item A central aim is to study the role of correlations due to the repulsion between the electrons.
\item To do this we have singled out three closed-shell systems with 2, 6 and 12 electrons.
\item We use variational Monte Carlo and try different trial wave functions to see how close we get to experiment/exact result for a given Hamiltonian
\item We test also the wave functions by computing onebody densities and compare these with those obtained with a non-interacting wave function.
\end{itemize}
  \end{block}
} 


\frame
{
  \frametitle{The report}
  \begin{block}{What should I focus on? Methods sections}
\begin{itemize}
\item Describe the methods (quantum mechanical and algorithms)
\item You need to explain  variational Monte Carlo and Hartree-Fock
\item The trial wave functions. Why do you choose the functions you do?
\item Why do you do importance sampling? And blocking and Conjugate gradient.
You don't need to explain in detail these methods. 
\item You need to explain how you implemented the methods and also say something about the structure of your algorithm and present some parts of your code (Slater det and Jastrow factor). 
\item You can also plug in some calculations to demonstrate your code, such as selected runs from for the two-electron case.
\end{itemize}
  \end{block}
} 


\frame
{
  \frametitle{The report}
  \begin{block}{What should I focus on? Results}
\begin{itemize}
\item You could focus on say the six-electron case
\item As an example, you should present results for the $\Delta t$ dependence
for this case but keep in 
the appendix some selected $\Delta t$ for $N=2$ and $N=12$.
\item Same applies to the blocking analysis and the conjugate gradient method
\item Same for the onebody densities, focus on $N=6$ and various wave functions. 
\item Discuss the results for different approaches to the wave functions. What do we learn?
\end{itemize}
  \end{block}
} 


\frame
{
  \frametitle{The report}
  \begin{block}{What should I focus on? Conclusions}
\begin{itemize}
\item State your main findings and interpretations
\item Try as far as possible to present perspectives for future work
\item Try to discuss the pros and cons of the methods and possible improvements
\end{itemize}
  \end{block}
} 


\frame
{
  \frametitle{The report}
  \begin{block}{What should I focus on? additional material}
\begin{itemize}
\item Additional calculations used to validate the codes
\item Selected calculations, these can be listed with 
few comments
\item Listing of the code if you feel this is necessary
\end{itemize}
You can consider moving parts of the material from the methods section to the appendix. You can also place additional material on your webpage. 
  \end{block}
} 


\frame
{
  \frametitle{The report}
  \begin{block}{What should I focus on? References}
\begin{itemize}
\item Give always references to material you base your work on, either 
scientific articles/reports or books.
\item {\em Wikipedia is not accepted as a scientific reference}. Under no circumstances.
\item Refer to articles as: name(s) of author(s), journal, volume (boldfaced), page and year
in parenthesis.
\item Refer to books as: name(s) of author(s), title of book, publisher, place and year, eventual page numbers
\end{itemize}
  \end{block}
} 
  


\frame
{
  \frametitle{The exam}
  \begin{block}{Dates and structure}
\begin{itemize}
\item Date: Friday June 10.
for day and time as soon as possible. Actual times are 9-17 both days. 
\item Duration 45 minutes
\item Give a presentation of your report, 30 mins. Slides only.
\item Then questions and feedback.
\item Your final grade will be based on the report, your presentation
and what you have done in total.
\end{itemize}
  \end{block}
} 



\section[Week 21]{Week 21}
\frame
{
  \frametitle{Topics for Week 21, May 23-27}
  \begin{block}{Last session}
\begin{itemize}
\item Summary of course
\item Discussion of the structure of the report and finalization of
report
\item Only project work
\end{itemize}

  \end{block}
} 




\end{document}


%\section{Programming classes and templates}


\frame[containsverbatim]
{
  \frametitle{What is object orientation?}
\begin{itemize}
\item    Classes are new types of datatypes specialized for
a task that may perform specific operations
\item A good way to keep track of  what needs to be
  shared between many functions
  (alternative: Global variables, massive
  argument lists)
\item   Makes debugging and code reuse simpler:
  Program is composed of several (mostly)
  independent self-contained pieces.

\end{itemize}

}


\frame[containsverbatim]
{
  \frametitle{  Use object orientation when:}
\begin{itemize}
\item  You can separate your code into logically separate sections
\item When    
    working on the same program for an extended period of time, or
    collaborating with many people
\item When writing a big, complicated program, or etc...
\end{itemize}

}


\frame[containsverbatim]
{
  \frametitle{Inheritance, what is it?}
\begin{itemize}
\item    You can make several new     
  classes inherit old classes.   They then get copies of the methods and variables in  
  the parent class.
\item   In addition they may define
  their own methods and
  variables, or override      
  methods in the base class.
\item   You may use a parent
  pointer to hold any child,     while accessing
  functionality declared in
  parent.
\end{itemize}

}



\frame[containsverbatim]
{
  \frametitle{Programming classes}
\begin{small}
{\scriptsize
In Fortran a vector or matrix start with $1$, but it is easy 
to change a vector so that it starts with zero or even a negative number.
If we have a double precision Fortran vector  which starts at $-10$ and ends at $10$, we could declare it as 
\lstinline{REAL(KIND=8) ::  vector(-10:10)}. Similarly, if we want to start at zero and end at 10 we could write
\lstinline{REAL(KIND=8) ::  vector(0:10)}.  
We have also seen that Fortran  allows us to write a matrix addition ${\bf A} = {\bf B}+{\bf C}$ as
\lstinline{A = B + C}.  This means that we have overloaded the addition operator so that it translates this operation into
two loops and an addition of two matrix elements $a_{ij} = b_{ij}+c_{ij}$.
}
\end{small}
}
\frame[containsverbatim]
{
  \frametitle{Programming classes}
\begin{small}
{\scriptsize
The way the matrix addition is written is very close to the way we express this relation mathematically. The benefit for the 
programmer is that our code is easier to read. Furthermore, such a way of coding makes it  more likely  to spot eventual 
errors as well.  


In Ansi C and C++ arrays start by default from $i=0$.  Moreover, if we  wish to add two matrices we need to explicitely write out
the two loops as
\lstset{language=c++}  
\begin{lstlisting}
   for(i=0 ; i < n ; i++) {  
      for(j=0 ; j < n ; j++) {
         a[i][j]=b[i][j]+c[i][j]
      }
   }  
\end{lstlisting} 

}
\end{small}
}
\frame[containsverbatim]
{
  \frametitle{Programming classes}
\begin{small}
{\scriptsize
However, 
the strength of C++ over programming languages like C and Fortran 77 is the possibility 
to define new data types, tailored to some particular problem.
Via new data types and overloading of operations such as addition and subtraction, we can easily define 
sets of operations and data types which allow us to write a matrix addition in exactly the same
way as we would do in Fortran.  We could also change the way we declare a C++ matrix elements $a_{ij}$, from  $a[i][j]$ 
to say $a(i,j)$, as we would do in Fortran. Similarly, we could also change the default range from $0:n-1$ to $1:n$. 

To achieve this we need to introduce two important entities in C++ programming, classes and templates.        
}
\end{small}
}
\frame[containsverbatim]
{
  \frametitle{Programming classes}
\begin{small}
{\scriptsize
The function and class declarations are fundamental concepts within C++.  Functions are abstractions
which encapsulate an algorithm or parts of it and perform specific tasks in a program. 
We have already met several examples on how to use  functions. 
Classes can be defined as abstractions which encapsulate
data and operations on these data. 
The data can be very complex data structures  and the class can contain particular functions
which operate on these data. Classes allow therefore for a higher level of abstraction in computing.
The elements (or components) of the data
type are the class data members, and the procedures are the class
member functions. 
}
\end{small}
}
\frame[containsverbatim]
{
  \frametitle{Programming classes}
Classes are user-defined tools used to create multi-purpose software which can be reused by other classes or functions.
These user-defined data types contain data (variables) and 
functions operating on the data.  

A simple example is that of a point in two dimensions.  
The data could be the $x$ and $y$ coordinates of a given  point. The functions
we define could be simple read and write functions or the possibility to compute the distance between two points.
}
\frame[containsverbatim]
{
  \frametitle{Programming classes}
C++ has a class complex in its standard
template library (STL). The standard usage in a given function could then look like 
\begin{lstlisting}
// Program to calculate addition and multiplication of two complex numbers
using namespace std;
#include <iostream>
#include <cmath>
#include <complex>
int main()
{
  complex<double> x(6.1,8.2), y(0.5,1.3);
  // write out x+y
  cout << x + y << x*y  << endl;
  return 0;
}
\end{lstlisting}
where we add and multiply two complex numbers $x=6.1+\imath 8.2$ and $y=0.5+\imath 1.3$ with the obvious results
$z=x+y=6.6+\imath 9.5$ and $z=x\cdot y= -7.61+\imath 12.03$. 
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
\begin{small}
{\scriptsize
We proceed by  splitting our task in three files.  

We define first a header file complex.h  which contains the declarations of
the class. The header file contains the class declaration (data and
functions), declaration of stand-alone functions, and all inlined
functions, starting as follows
\begin{lstlisting}
#ifndef Complex_H
#define Complex_H
//   various include statements and definitions
#include <iostream>          // Standard ANSI-C++ include files
#include <new>
#include ....

class Complex
{...
definition of variables and their character
};
//   declarations of various functions used by the class
...
#endif
\end{lstlisting}

}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
\begin{small}
{\scriptsize
Next we provide a file complex.cpp where the code and algorithms of different functions  (except inlined functions) 
declared within the class are written.
The files complex.h and complex.cpp are normally placed in a directory with other classes and libraries we have 
defined.  

Finally,we discuss here an example of a main program which uses this particular class.
An example of a program which uses our complex class is given below. In particular we would like our class to
perform tasks like declaring complex variables, writing out the real and imaginary part and performing 
algebraic operations such as adding or multiplying two complex numbers.
}
\end{small}
}
\frame[containsverbatim]
{
  \frametitle{Programming classes}
\begin{small}
{\scriptsize
\begin{lstlisting}
#include "Complex.h"
...  other include and declarations
int main ()
{
  Complex a(0.1,1.3);    // we declare a complex variable a
  Complex b(3.0), c(5.0,-2.3);  // we declare  complex variables b and c
  Complex d = b;         //  we declare  a new complex variable d 
  cout << "d=" << d << ", a=" << a << ", b=" << b << endl;
  d = a*c + b/a;  //   we add, multiply and divide two complex numbers 
  cout << "Re(d)=" << d.Re() << ", Im(d)=" << d.Im() << endl;  // write out of the real and imaginary parts
}
\end{lstlisting}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Programming classes}
We include the header file complex.h and define four different complex variables. These
are $a=0.1+\imath 1.3$, $b=3.0+\imath 0$ (note that if you don't define a value for the imaginary part  this is set to
zero), $c=5.0-\imath 2.3$ and $d=b$.  Thereafter we have defined standard algebraic operations and the member functions
of the class which allows us to print out the real and imaginary part of a given variable.
}




\frame[containsverbatim]
{
  \frametitle{Programming classes}
\begin{small}
{\scriptsize
\begin{lstlisting}
class Complex
{
private:
   double re, im; // real and imaginary part
public:
   Complex ();                              // Complex c;
   Complex (double re, double im = 0.0); // Definition of a complex variable;
   Complex (const Complex& c);              // Usage: Complex c(a);   // equate two complex variables
   Complex& operator= (const Complex& c); // c = a;   //  equate two complex variables, same as previous
....

\end{lstlisting}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Programming classes}
\begin{small}
{\scriptsize
\begin{lstlisting}
  ~Complex () {}                        // destructor
   double   Re () const;        // double real_part = a.Re();
   double   Im () const;        // double imag_part = a.Im();
   double   abs () const;       // double m = a.abs(); // modulus
   friend Complex operator+ (const Complex&  a, const Complex& b);
   friend Complex operator- (const Complex&  a, const Complex& b);
   friend Complex operator* (const Complex&  a, const Complex& b);
   friend Complex operator/ (const Complex&  a, const Complex& b);
};
\end{lstlisting}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{Programming classes}
The class is defined via the statement \lstinline{class Complex}. We must first use the key word 
\lstinline{class}, which in turn is followed by the user-defined variable name  \lstinline{Complex}. 
The body of the class, data and functions, is encapsulated  within the parentheses $\{...\};$.
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
Data and specific functions can be private, which means that they cannot be accessed from outside the class.
This means also that access cannot be inherited by other functions outside the class. If we use \lstinline{protected}
instead of \lstinline{private}, then data and functions can be inherited outside the class.

}
\frame[containsverbatim]
{
  \frametitle{Programming classes}
The key word \lstinline{public} means  that data and functions can be accessed from outside the class.
Here we have defined several functions  which can be accessed by functions outside the class.
The declaration \lstinline{friend} means that stand-alone functions can work on privately declared  variables  of the type
\lstinline{(re, im)}.  Data members of a class should be declared as private variables.
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
The first public function we encounter is a so-called   
constructor, which  tells how we declare a variable of type \lstinline{Complex} 
and how this variable is initialized. We have chose  three possibilities in the example above:
\begin{itemize}
\item A declaration like \lstinline{Complex c;} calls the member function \lstinline{Complex()}
which can have the following implementation 
\begin{lstlisting}
Complex:: Complex ()   { re = im = 0.0; }
\end{lstlisting}
meaning that it sets the real and imaginary parts to zero.  Note the way a member function is defined.
The constructor is the first function that is called when an object is instantiated.
\end{itemize}
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
\begin{itemize}
\item Another possibility  is 
\begin{lstlisting}
Complex:: Complex ()   {}
\end{lstlisting}
which means that there is no initialization of the real and imaginary parts.  The drawback is that a given compiler
can then assign random values to a given variable.
\item  A call like \lstinline{Complex a(0.1,1.3);} means that we could call the member function 
\lstinline{Complex(double, double)}as 
\begin{lstlisting}
Complex:: Complex (double re_a, double im_a)
{ re = re_a; im = im_a; }
\end{lstlisting}
\end{itemize}
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
The simplest member function are those we defined to extract 
the real and imaginary part of a variable. Here you have to recall that these are private data,
that is they invisible for users of the class.  We obtain a copy of these variables by defining the 
functions
\begin{lstlisting}
double Complex:: Re () const { return re; }} //  getting the real part
double Complex:: Im () const { return im; }  //   and the imaginary part
\end{lstlisting}
Note that we have introduced   the declaration  \lstinline{const}.  What does it mean? 
This declaration means that a variabale cannot be changed within  a called function.
}


\frame[containsverbatim]
{
  \frametitle{Programming classes}
If we define a variable as 
\lstinline{const double p = 3;} and then try to change its value, we will get an error when we
compile our program. This means that constant arguments in functions cannot be changed.
\begin{lstlisting}
// const arguments (in functions) cannot be changed:
void myfunc (const Complex& c)
{ c.re = 0.2; /* ILLEGAL!! compiler error... */  }
\end{lstlisting}
If we declare the function and try to change the value to $0.2$, the compiler will complain by sending
an error message. 
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
If we define a function to compute the absolute value of complex variable like
\begin{lstlisting}
double Complex:: abs ()  { return sqrt(re*re + im*im);}
\end{lstlisting}
without the constant declaration  and define thereafter a function 
\lstinline{myabs} as
\begin{lstlisting}
double myabs (const Complex& c)
{ return c.abs(); }   // Not ok because c.abs() is not a const func.
\end{lstlisting}
the compiler would not allow the c.abs() call in myabs
since \lstinline{Complex::abs} is not a constant member function. 
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
Constant functions cannot change the object's state.
To avoid this we declare the function \lstinline{abs} as
\begin{lstlisting}
double Complex:: abs () const { return sqrt(re*re + im*im); } 
\end{lstlisting}
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
C++ (and Fortran) allow  for overloading of operators. That means we can define algebraic operations
on for example vectors or any arbitrary object.   
As an example, a vector addition of the type  ${\bf c} = {\bf a} + {\bf b}$
means that we need to write   a small part of code with a for-loop over the dimension of the array.
We would rather like to write this statement as \lstinline{c = a+b;} as this makes the code much more
readable and close to eventual equations we want to code.  To achieve this we need to extend the definition of operators.
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
Let us study the declarations in our complex class.
In our main function we have a statement like \lstinline{d = b;}, which means
that we call \lstinline{d.operator= (b)} and we have defined a so-called assignment operator
as a part of the class defined as
\begin{lstlisting}
Complex& Complex:: operator= (const Complex& c)
{
   re = c.re;
   im = c.im;
   return *this;
}
\end{lstlisting}
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
With this function, statements like
\lstinline{Complex d = b;} or \lstinline{Complex d(b);}
make a new object $d$, which becomes a copy of $b$. 
We can make simple implementations in terms of the assignment
\begin{lstlisting}
Complex:: Complex (const Complex& c)
{ *this = c; }
\end{lstlisting}
which  is a pointer to "this object", \lstinline{*this} is the present object,
so \lstinline{*this = c;} means setting the present object equal to $c$, that is
\lstinline{this->operator= (c);}.
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
The meaning of the addition operator $+$ for Complex objects is defined in the
function
\lstinline{Complex operator+ (const Complex& a, const Complex& b); // a+b}
The compiler translates \lstinline{c = a + b;} into \lstinline{c = operator+ (a, b);}. 
Since this implies the call to function, it brings in an additional overhead. If speed
is crucial and this function call is performed inside a loop, then it is more difficult for a 
given compiler to perform optimizations of a loop.
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
The solution to this is to inline functions.   We discussed inlining in chapter 
2 of the lecture notes.
Inlining means that the function body is copied directly into
the calling code, thus avoiding calling the function.
Inlining is enabled by the inline keyword
\begin{lstlisting}
inline Complex operator+ (const Complex& a, const Complex& b)
{ return Complex (a.re + b.re, a.im + b.im); }
\end{lstlisting}
Inline functions, with complete bodies must be written in the header file  complex.h.
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
Consider  the case \lstinline{c = a + b;}
that is,  \lstinline{c.operator= (operator+ (a,b));}
If \lstinline{operator+}, \lstinline{operator=} and the constructor \lstinline{Complex(r,i)} all
are inline functions, this transforms to
\begin{lstlisting}
c.re = a.re + b.re;
c.im = a.im + b.im;
\end{lstlisting}
by the compiler, i.e., no function calls
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
The stand-alone function \lstinline{operator+} is a friend of the Complex  class
\begin{lstlisting}
class Complex
{
   ...
   friend Complex operator+ (const Complex& a, const Complex& b);
   ...
};
\end{lstlisting}
so it can read (and manipulate) the private data parts $re$ and
$im$ via
\begin{lstlisting}
inline Complex operator+ (const Complex& a, const Complex& b)
{ return Complex (a.re + b.re, a.im + b.im); }
\end{lstlisting}
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
Since we do not need to alter the re and im variables, we can
get the values by Re() and Im(), and there is no need to be a
friend function
\begin{lstlisting}
inline Complex operator+ (const Complex& a, const Complex& b)
{ return Complex (a.Re() + b.Re(), a.Im() + b.Im()); }
\end{lstlisting}
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
The multiplication functionality can now be extended to imaginary numbers by the following code
\begin{lstlisting}
inline Complex operator* (const Complex& a, const Complex& b)
{
  return Complex(a.re*b.re - a.im*b.im, a.im*b.re + a.re*b.im);
}
\end{lstlisting}
It will be convenient to inline all functions used by this operator.
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
To inline the complete expression \lstinline{a*b;}, the constructors and
\lstinline{operator=}  must also be inlined.  This can be achieved via the following piece of code
\begin{lstlisting}
inline Complex:: Complex () { re = im = 0.0; }
inline Complex:: Complex (double re_, double im_)
{ ... }
inline Complex:: Complex (const Complex& c)
{ ... }
inline Complex:: operator= (const Complex& c)
{ ... }
\end{lstlisting}
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
\begin{lstlisting}
// e, c, d are complex
e = c*d;
// first compiler translation:
e.operator= (operator* (c,d));
// result of nested inline functions
// operator=, operator*, Complex(double,double=0):
e.re = c.re*d.re - c.im*d.im;
e.im = c.im*d.re + c.re*d.im;
\end{lstlisting}
The definitions \lstinline{operator-} and \lstinline{operator/} follow the same set up.
}


\frame[containsverbatim]
{
  \frametitle{Programming classes}
Finally, if we wish to write to file or another device a complex number using the simple syntax
\lstinline{cout << c;}, we obtain this by defining
the effect of $<<$ for a Complex object as 
\begin{lstlisting}
ostream& operator<< (ostream& o, const Complex& c)
{ o << "(" << c.Re() << "," << c.Im() << ") "; return o;}
\end{lstlisting}
}


\frame[containsverbatim]
{
  \frametitle{Programming classes, templates}
What if we wanted to make a class which takes integers
or floating point numbers with single precision?
A simple way to achieve this is copy and paste our class and replace \lstinline{double} with for
example \lstinline{int}.

C++  allows us to do this automatically via the usage of templates, which 
are the C++ constructs for parameterizing parts of
classes. Class templates  is a template for producing classes. The declaration consists
of the keyword \lstinline{template} followed by a list of template arguments enclosed in brackets.
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
We can therefore make a more general class by rewriting our original example as
\begin{lstlisting}
template<class T>
class Complex
{
private:
   T re, im; // real and imaginary part
public:
   Complex ();                              // Complex c;
   Complex (T re, T im = 0); // Definition of a complex variable;
   Complex (const Complex& c);              // Usage: Complex c(a);   // equate two complex variables
   Complex& operator= (const Complex& c); // c = a;   //  equate two complex variables, same as previous

\end{lstlisting}
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
We can therefore make a more general class by rewriting our original example as
\begin{lstlisting}
  ~Complex () {}                        // destructor
   T   Re () const;        // T real_part = a.Re();
   T   Im () const;        // T imag_part = a.Im();
   T   abs () const;       // T m = a.abs(); // modulus
   friend Complex operator+ (const Complex&  a, const Complex& b);
   friend Complex operator- (const Complex&  a, const Complex& b);
   friend Complex operator* (const Complex&  a, const Complex& b);
   friend Complex operator/ (const Complex&  a, const Complex& b);
};
\end{lstlisting}
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
What it says is that \lstinline{Complex} is a parameterized type with $T$ as a parameter and $T$ has to be a type such as double
or float. 
The class complex is now a class template
and we would define variables in a code as 
\begin{lstlisting}
Complex<double> a(10.0,5.1);
Complex<int> b(1,0);
\end{lstlisting}
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
Member functions of our class are defined by preceding the name of the function with the \lstinline{template} keyword. 
Consider the function we defined as \lstinline{Complex:: Complex (double re_a, double im_a)}.
We would rewrite this function as 
\begin{lstlisting}
template<class T>
Complex<T>:: Complex (T re_a, T im_a)
{ re = re_a; im = im_a; }
\end{lstlisting}
The member functions  are otherwise defined following ordinary member function definitions.
}


\section[Week 12]{Weeks 12}
\frame
{
  \frametitle{Topics for Week 12, March 22-26}
  \begin{block}{Slater determinants and classes}
\begin{itemize}
\item Repetition from last week
\item Discussion of classes, the complex class and a matrix-vector class
\end{itemize}
Project work this week: Slater determinant!
  \end{block}
} 


\frame[containsverbatim]
{
  \frametitle{Useful equations}
\begin{small}
{\scriptsize
The $1s$ hydrogen like wave function
\[
R_{10}(r) =  2\left(\frac{Z}{a_0}\right)^{3/2}\exp{(-Zr/a_0)}= u_{10}/r 
\]
The total energy for helium (not the Hartree or Fock terms) from  the direct and the exchange term should give $5Z/8$.

The single-particle energy with no interactions should give $-Z^2/2n^2$. 

The $2s$ hydrogen-like wave function is
\[
R_{20}(r) =  2\left(\frac{Z}{2a_0}\right)^{3/2}\left(1-\frac{Zr}{2a_0}\right)\exp{(-Zr/2a_0)}= u_{20}/r 
\]
and the $2p$ hydrogen -like wave function is
\[
R_{21}(r) =  \frac{1}{\sqrt{3}}\left(\frac{Z}{2a_0}\right)^{3/2}\frac{Zr}{a_0}\exp{(-Zr/2a_0)}= u_{21}/r 
\]
We use $a_0=1$.
 }
 \end{small}
 }


\section[Week 14]{Weeks 14}
\frame
{
  \frametitle{Topics for Week 14, April 5-9}
  \begin{block}{Hartree-Fock theory and Density functional theory}
\begin{itemize}
\item Discussion of project 2, Hartree-Fock theory, wave functions and computation of 
Coulomb matrix elements.
\item Project 1: Continuation on how to implement the  Slater determinant and correlation part and the conjugate gradient method.
\item Discussion of practicalities about the Hartree-Fock equations.
\item Formal derivation of the Hartree-Fock equations. Continues next two weeks as well. 
\end{itemize}
The material discussed for the rest of the course is covered by Thijssen's book chapters 4-6.  We will use April to cover Hartree-Fock theory and Density Functional theory. May is devoted
to finalizing project 2 only. Only Lab work during May.
  \end{block}
} 



\frame
{
  \frametitle{Structure of project 2}
Project 2 deals with the following topics:
\begin{itemize}
\item Hartree-Fock calculation of the Beryllium atom. To do that one needs
\begin{enumerate}
\item  Choice of basis, discussed today: Slater orbitals and Hydrogen-like orbitals.
\item  Diagonalization of an eigenvalue problem in order to find the coefficients.
\item Computation of the Coulomb matrix elements. Since we limit ourselves to Beryllium,
we need only $l=0$ waves.
\end{enumerate}
\end{itemize}
} 




\frame
{
  \frametitle{Structure of project 2}
Finally:
\begin{itemize}
\item The Hartree-Fock solutions are in turn used in the Variational Monte Carlo code
developed in project 1. That means that we do not need vary $\alpha$.  The variational calculation
is then limited to the Jastrow factor.  We will look at different Jastrow factors and see which gives the best solution.
\end{itemize}
} 



\frame
{
  \frametitle{Structure of project 2}
The structure of the Hartree-Fock part involves
\begin{enumerate}
\item  Choice of basis, discussed today: Slater orbitals and Hydrogen-like orbitals.
Here we need a function to compute the Laguerre polynomials for Hydrogen-like orbitals.
This function is available at the webpage as laguerre.cpp, see under project 2.
\item  Diagonalization of an eigenvalue problem in order to find the coefficients.
One can use Jacobi's method or Householder's with Givens' transformations, see chapter 12
of lecture notes.
\item Computation of the Coulomb matrix elements. Since we limit ourselves to Beryllium,
we need only $l=0$ waves. Here you need to develop a program which sets up the 
matrix elements using Gaussian quadrature (chapter 7 of lecture notes).
\end{enumerate}
} 







\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: useful expressions for the Coulomb terms}
\begin{small}
{\scriptsize
We need to compute the integral  for the direct term
\begin{equation*}
\langle \alpha\beta|V|\gamma\delta\rangle=
\int \int \psi_{\alpha}^*(\mathbf{r}_1) \psi_{\beta}^*(\mathbf{r}_2) 
  \frac{1}{r_{12}}\psi_{\gamma}(\mathbf{r}_1)\psi_{\delta}(\mathbf{r}_2)  d\mathbf{r}_1d\mathbf{r}_2
\end{equation*}
and the exchange term
\begin{equation*}
\langle \alpha\beta|V|\delta\gamma\rangle=
\int \int \psi_{\alpha}^*(\mathbf{r}_1) \psi_{\beta}^*(\mathbf{r}_2) 
  \frac{1}{r_{12}}\psi_{\delta}(\mathbf{r}_1)\psi_{\gamma}(\mathbf{r}_2)  d\mathbf{r}_1d\mathbf{r}_2
\end{equation*}
Note well that spin is included in the quantum numbers $\alpha\beta\gamma\delta$, but the interaction does not affect
spin.
 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Expression for Direct Term}
\begin{small}
{\scriptsize
We need to break down the single-particle wave functions  in radial, angular and spin parts. Note that 
direct term is diagonal 
in the spin quantum numbers.
The single-particle wave functions are
\[
\psi_{\alpha}({\bf r})=\psi_{nlm_lsm_s}({\bf r})=\phi_{nlm_l}({\bf r})\xi_{m_s}(s)
\]
with 
\[
    \phi_{nlm_l}({\bf r})= R_{nl}(r)Y_{lm_l}(\hat{r})
\]
 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Explicit expression for Direct Term}
\begin{small}
{\scriptsize
The addition theorem  of the spherical harmonics  yields
\[
\sum_{m_l=-l}^{l}Y^*_{lm_l}(\theta_i,\phi_i)Y_{lm_l}(\theta_j,\phi_j) = \frac{2l+1}{4\pi}P_l(\cos{(\theta)})
\]
with
\[
cos(\theta)=cos(\theta_i)cos(\theta_j)+sin(\theta_i)sin(\theta_j)cos(\phi_i-\phi_j).
\]
The quantity $r_{ij}$ in the Coulomb interaction depends on the angles of particle $i$ and $j$.
 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Explicit expression for Direct Term}
\begin{small}
{\scriptsize
The integral over the angles can be performed by expanding $1/r_{ij}$ in terms of spherical harmonics and using the fact that
the functions $R_{nl}$ do not depend on the angles.
We have
\[
\frac{1}{r_{ij}}=\frac{1}{r_i}\sum_{l=0}^{\infty}\left(\frac{r_j}{r_i}\right)^lP_l \cos(\theta)),
\]
if $r_i > r_j$ or 
\[
\frac{1}{r_{ij}}=\frac{1}{r_j}\sum_{l=0}^{\infty}\left(\frac{r_i}{r_j}\right)^lP_l\cos(\theta)),
\]
if $r_j > r_i$.  In a compact form it reads
\[
\frac{1}{r_{ij}}=\sum_{l=0}^{\infty}\left(\frac{(r_<)^l}{(r_>)^{l+1}}\right)P_l\cos(\theta)),
\]
with $ r_{>} = max(r_i,r_j)$ and with $ r_{<} = min(r_i,r_j)$.
$P_l$ is the Legendre polynomial and 
\[
\cos(\theta)=\cos(\theta_i)\cos(\theta_j)+\sin(\theta_i)\sin(\theta_j)\cos(\phi_i-\phi_j).
\]
 }
 \end{small}
 }





\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Explicit expression for Direct Term}
\begin{small}
{\scriptsize
We can use the expression for spherical harmonics to express the interaction as
\[
\frac{1}{r_{ij}}=\sum_{l=0}^{\infty}\sum_{m_l=-l}^{l}\frac{4\pi}{2l+1}\left(\frac{(r_<)^l}{(r_>)^{l+1}}\right)Y^*_{lm_l}(\theta_i,\phi_i)Y_{lm_l}(\theta_j,\phi_j)
\]
and inserting it we obtain for the direct term
\[
\sum_{l=0}^{\infty}\sum_{m_l=-l}^{l}\frac{4\pi}{2l+1}\int r_1^2dr_1 \int r_2^2dr_2R_{n_{\alpha}l_{\alpha}}^*(r_1) R_{n_{\beta}l_{\beta}}^*(r_2) 
  \frac{(r_<)^l}{(r_>)^{l+1}}R_{n_{\gamma}l_{\gamma}}(r_1)R_{n_{\delta}l_{\delta}}(r_2)
\]
\[
\times \int d\Omega_1Y^*_{lm_l}(\theta_1,\phi_1)Y^*_{l_{\alpha}m_{\alpha}}(\theta_1,\phi_1)Y_{l_{\gamma}m_{\gamma}}(\theta_1,\phi_1)
\]
\[
\times \int d\Omega_2Y_{lm_l}(\theta_2,\phi_2)Y^*_{l_{\beta}m_{\beta}}(\theta_2,\phi_2)Y_{l_{\delta}m_{\delta}}(\theta_2,\phi_2)
\]
 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Explicit expression for Exchange term}
\begin{small}
{\scriptsize
Similarly, we get for the exchange term the following expression
\[
\sum_{l=0}^{\infty}\sum_{m_l=-l}^{l}\frac{4\pi}{2l+1}\int r_1^2dr_1 \int r_2^2dr_2R_{n_{\alpha}l_{\alpha}}^*(r_1) R_{n_{\beta}l_{\beta}}^*(r_2) 
  \frac{(r_<)^l}{(r_>)^{l+1}}R_{n_{\gamma}l_{\gamma}}(r_2)R_{n_{\delta}l_{\delta}}(r_1)
\]
\[
\times \int d\Omega_1Y^*_{lm_l}(\theta_1,\phi_1)Y^*_{l_{\alpha}m_{\alpha}}(\theta_1,\phi_1)Y_{l_{\delta}m_{\delta}}(\theta_1,\phi_1)
\]
\[
\times \int d\Omega_2Y_{lm_l}(\theta_2,\phi_2)Y^*_{l_{\beta}m_{\beta}}(\theta_2,\phi_2)Y_{l_{\gamma}m_{\gamma}}(\theta_2,\phi_2)
\]
 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Explicit expression for Direct Term of Beryllium}
\begin{small}
{\scriptsize
In our case we will only deal with particles with $l_{\alpha}=0$. In this case we have that
\[
Y_{00}=\frac{1}{\sqrt{4\pi}},
\]
and inserting it we get
\[
\sum_{l=0}^{\infty}\sum_{m_l=-l}^{l}\frac{4\pi}{2l+1}\int r_1^2dr_1 \int r_2^2dr_2R_{n_{\alpha}0}^*(r_1) R_{n_{\beta}0}^*(r_2) 
  \frac{(r_<)^l}{(r_>)^{l+1}}R_{n_{\gamma}0}(r_1)R_{n_{\delta}0}(r_2)
\]
\[
\times \int d\Omega_1Y^*_{lm_l}(\theta_1,\phi_1)Y^*_{00}Y_{00}\int d\Omega_2Y_{lm_l}(\theta_2,\phi_2)Y^*_{00}Y_{00}
\]
which becomes 
\[
\sum_{l=0}^{\infty}\sum_{m_l=-l}^{l}\frac{1}{2l+1}\int r_1^2dr_1 \int r_2^2dr_2R_{n_{\alpha}0}^*(r_1) R_{n_{\beta}0}^*(r_2) 
  \frac{(r_<)^l}{(r_>)^{l+1}}R_{n_{\gamma}0}(r_1)R_{n_{\delta}0}(r_2)
\]
\[
\times \int d\Omega_1Y^*_{lm_l}(\theta_1,\phi_1)Y_{00}\int d\Omega_2Y_{lm_l}(\theta_2,\phi_2)Y_{00}
\]
 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Explicit expression for Direct Term of Beryllium}
\begin{small}
{\scriptsize
Using the orthogonality relation of spherical harmonics
\[
\int d\Omega_1Y^*_{l'm_l'}(\theta_1,\phi_1)Y_{lm_l}(\theta_1,\phi_1)=\delta_{l,l'}\delta_{m_l,m_l'},
\]
we can rewrite 
\[
\sum_{l=0}^{\infty}\sum_{m_l=-l}^{l}\frac{1}{2l+1}\int r_1^2dr_1 \int r_2^2dr_2R_{n_{\alpha}0}^*(r_1) R_{n_{\beta}0}^*(r_2) 
  \frac{(r_<)^l}{(r_>)^{l+1}}R_{n_{\gamma}0}(r_1)R_{n_{\delta}0}(r_2)
\]
\[
\times \int d\Omega_1Y^*_{lm_l}(\theta_1,\phi_1)Y_{00}\int d\Omega_2Y_{lm_l}(\theta_2,\phi_2)Y_{00}
\]
as
\[
\sum_{l=0}^{\infty}\sum_{m_l=-l}^{l}\frac{1}{2l+1}\int r_1^2dr_1 \int r_2^2dr_2R_{n_{\alpha}0}^*(r_1) R_{n_{\beta}0}^*(r_2) 
  \frac{(r_<)^l}{(r_>)^{l+1}}R_{n_{\gamma}0}(r_1)R_{n_{\delta}0}(r_2)\delta_{l,0}\delta_{m_l,0},
\]
resulting in
\[
\int r_1^2dr_1 \int r_2^2dr_2R_{n_{\alpha}0}^*(r_1) R_{n_{\beta}0}^*(r_2) 
  \frac{1}{(r_>)}R_{n_{\gamma}0}(r_1)R_{n_{\delta}0}(r_2)
\]
 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Explicit expression for Direct Term of Beryllium}
\begin{small}
{\scriptsize
The direct matrix elements that we need are then simply given by a double integral
\[
\int r_1^2dr_1 \int r_2^2dr_2R_{n_{\alpha}0}^*(r_1) R_{n_{\beta}0}^*(r_2) 
  \frac{1}{(r_>)}R_{n_{\gamma}0}(r_1)R_{n_{\delta}0}(r_2)
\]
\begin{itemize}
\item This integral can be solved once and for all using hydrogenic-like wave functions
by solving the above double integral using Gaussian quadrature (see chapter 7 of lecture notes).
\item These elements should then be stored and looked up every time they are needed in the Hartree-Fock
calculation.
\item Exercise: find the corresponding matrix element for the exchange part.
\end{itemize}
 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Simplification of the direct term}
\begin{small}
{\scriptsize
As an exercise, show that you can simplify the integral
\[
\int r_1^2dr_1 \int r_2^2dr_2R_{n_{\alpha}0}^*(r_1) R_{n_{\beta}0}^*(r_2) 
  \frac{1}{(r_>)}R_{n_{\gamma}0}(r_1)R_{n_{\delta}0}(r_2)
\]
as
\[
\int_0^{\infty} r_1^2dr_1R_{n_{\alpha}0}^*(r_1)R_{n_{\gamma}0}(r_1) 
\left[\frac{1}{(r_1)}\int_0^{r_1} r_2^2dr_2 R_{n_{\beta}0}^*(r_2) 
  R_{n_{\delta}0}(r_2)+\int_{r_1}^{\infty} r_2dr_2 R_{n_{\beta}0}^*(r_2) 
  R_{n_{\delta}0}(r_2)\right].
\]
 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Single-particle wave functions}
\begin{small}
{\scriptsize
As basis functions for our calculations we will use hydrogenic like functions. 
In project 2 we need only the radial part since the spherical harmonics for $s$-waves are rather simple.
Our radial wave functions are
\[
R_{n0}(r)=\left(\frac{2Z}{n}\right)^{3/2}\sqrt{\frac{(n-1)!}{2n\times n!}}L_{n-1}^1(\frac{2Z}{n})\exp{(-\frac{Zr}{n})},
\]
with energies $-Z^2/2n^2$.
A function for computing the generalized Laguerre  polynomials $L_{n-1}^1(\frac{2Z}{n})$ is provided at the webpage of
the course under the link of project 2. 
We will use these functions to solve the Hartree-Fock problem for Beryllium.
 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Single-particle wave functions}
\begin{small}
{\scriptsize
When we have finished the Hartree-Fock calculations, it will be useful to parameterize our solutions
in terms of the nodeless Slater-type orbitals (STO).  In our case we will use so-called node-dependent solutions
given by
\[
  \Psi_{nlm_l}(r,\theta,\phi) = {\cal N}r^{n_{_{eff}}-1}
  e^{\frac{Z_{_{eff}}\rho}{n_{_{eff}}}}Y_{lm_l}(\theta,\phi).
\]

Here ${\cal N}$ is a normalization constant,
$Y_{lm_l}$ is a spherical harmonic
and $\rho=r/a_0$.
Such parameterizations exit in the literature.  For Beryllium we can use
\[
R^{\mathrm{STO}}_{10}(r)=N_{10}\exp{(-3.6848r)}
\]
and
\[
R^{\mathrm{STO}}_{20}(r)=N_{20}r\exp{(-1.9120r/2)}
\]
Using these values in the Slater determinant for Beryllium, with no Jastrow factor you should get -14.573 a.u.
for the ground state energy.
 }
 \end{small}
 }


\section[Week 15]{Weeks 15}
\frame
{
  \frametitle{Topics for Week 15, April 12-16}
  \begin{block}{Hartree-Fock theory and Density functional theory}
\begin{itemize}
\item Discussion of project 2, Hartree-Fock theory, wave functions and computation of 
Coulomb matrix elements.
\item Project 1: Continuation on how to implement the  Slater determinant and correlation part and the conjugate gradient method.
\item Discussion of practicalities about the Hartree-Fock equations.
\item Formal derivation of the Hartree-Fock equations. Continues next two weeks as well. 
\end{itemize}
The material discussed for the rest of the course is covered by Thijssen's book chapters 4-6.  We will use April to cover Hartree-Fock theory and Density Functional theory. May is devoted
to finalizing project 2 only. Only Lab work during May.
  \end{block}
} 

\frame
{
  \frametitle{Problems with neon for VMC}
\begin{small}
{\scriptsize
In the standard textbook case one uses spherical coordinates  in order to get the hydrogen-like wave functions
     \[
        x=rsin\theta cos\phi,  
      \]
      \[
        y=rsin\theta sin\phi,
     \]
and
     \[
        z=rcos\theta.
     \]
The reason we introduce spherical coordinates is the spherical symmetry of the Coulomb potential
\[
    \frac{e^2}{4\pi\epsilon_0r}=\frac{e^2}{4\pi\epsilon_0\sqrt{x^2+y^2+z^2}},
\]
where we have used $r=\sqrt{x^2+y^2+z^2}$. 
It is not possible to find a separable solution of the type
\[
    \psi(x,y,z)=\psi(x)\psi(y)\psi(z).
\]
However, with spherical coordinates we can find a solution
of the form
\[
   \psi(r,\theta,\phi)=R(r)P(\theta)F(\phi).
\]
}
\end{small}
}




\frame
{
  \frametitle{Problems with neon for VMC}
\begin{small}
{\scriptsize
The angle-dependent differential equations result in the spherical harmonic functions as
solutions, with quantum numbers $l$ and $m_l$.
These functions are given by
\[
    Y_{lm_l}(\theta,\phi)=P(\theta)F(\phi)=\sqrt{\frac{(2l+1)(l-m_l)!}{4\pi (l+m_l)!}}
                      P_l^{m_l}(cos(\theta))\exp{(im_l\phi)},
\]
with $P_l^{m_l}$ being the associated Legendre polynomials
They can be rewritten as 
\[
   Y_{lm_l}(\theta,\phi)=sin^{|m_l|}(\theta) \times (\mathrm{polynom}(cos\theta))\exp{(im_l\phi)},
\]
}
\end{small}
}


\frame
{
  \frametitle{Problems with neon for VMC}
\begin{small}
{\scriptsize
We have the following selected examples
\[
   Y_{00}=\sqrt{\frac{1}{4\pi}},
\]
for $l=m_l=0$, 
\[
   Y_{10}=\sqrt{\frac{3}{4\pi}}cos(\theta),
\]
for $l=1$ og $m_l=0$, 
\[
   Y_{1\pm 1}=\sqrt{\frac{3}{8\pi}}sin(\theta)exp(\pm i\phi),
\]
for  $l=1$ og $m_l=\pm 1$. 
}
\end{small}
}



\frame
{
  \frametitle{Problems with neon for VMC}
\begin{small}
{\scriptsize
A problem with the spherical harmonics is that they are complex. The introduction of
\emph{solid harmonics} allows the use
of real orbital wave-functions for a wide range of applications. The
complex solid harmonics ${\cal Y}_{lm_l}(\mathbf{r})$ are related to
the spherical harmonics  $Y_{lm_l}(\mathbf{r})$ through
\begin{equation*}
  {\cal Y}_{lm_l}(\mathbf{r}) = r^l Y_{lm_l}(\mathbf{r}).
\end{equation*}
By factoring out the leading $r$-dependency of the radial-function
\begin{equation*}
  {\cal R}_{nl}(\mathbf{r}) = r^{-l} R_{nl}(\mathbf{r}),
\end{equation*}
we obtain 
\begin{equation*}
  \Psi_{nlm_l}(r,\theta, \phi) %=R_{nl}(r) \cdot Y_{lm_l}(\theta,\phi)
  = {\cal R}_{nl}(\mathbf{r})\cdot{\cal Y}_{lm_l}(\mathbf{r}).
%\label{totalSolidHydrogenWavefunction}
\end{equation*}
}
\end{small}
}










\frame
{
  \frametitle{Problems with neon for VMC}
\begin{small}
{\scriptsize
For the theoretical development of the \emph{real solid harmonics} we first 
express the complex solid harmonics, $C_{lm_l}$, by (complex) Cartesian
coordinates, and arrive at the real solid harmonics, $S_{lm_l}$, through
the unitary transformation
\begin{equation*}
  \left( \begin{split} &\phantom{i} S_{lm_l} \\ 
    &S_{l,-m_l} \end{split} \right) 
  = \frac{1}{\sqrt{2}} \left(        \begin{split}
    (-1)^m_l \phantom{a} & \phantom{aa} 1 \\ 
    -(-1)^m_l i & \phantom{aa} i       \end{split} \right)  
  \left( \begin{split} &\phantom{i} C_{lm_l} \\ 
    &C_{l,-m_l} \end{split} \right).
\end{equation*}
}
\end{small}
}



\frame
{
  \frametitle{Problems with neon for VMC}
\begin{small}
{\scriptsize
This transformation will not alter any physical quantities that are
degenerate in the subspace consisting of opposite magnetic quantum
numbers (the angular momentum $l$ is equal for both these cases). This
means for example that the above transformation does not alter the
energies, unless an external magnetic field is applied to the
system. Henceforth, we will use the solid harmonics, and note that
changing the spherical potential beyond the Coulomb potential will not
alter the solid harmonics.
}
\end{small}
}



\frame
{
  \frametitle{Problems with neon for VMC}
\begin{small}
{\scriptsize
We have defined 
\begin{equation*}
  {\cal Y}_{lm_l}(\mathbf{r}) = r^l Y_{lm_l}(\mathbf{r}).
\end{equation*}
The real-valued spherical harmonics are defined as
\[
S_{l0} =  \sqrt{\frac{4\pi}{2l+1}} {\cal Y}_{l0}(\mathbf{r}),
\]
\[
S_{lm_l} =  (-1)^{m_l}\sqrt{\frac{8\pi}{2l+1}} \mathrm{Re}{\cal Y}_{l0}(\mathbf{r}),
\]

\[
S_{lm_l} =  (-1)^{m_l}\sqrt{\frac{8\pi}{2l+1}} \mathrm{Im}{\cal Y}_{l0}(\mathbf{r}),
\]
for $m_l> 0$.

}
\end{small}
}




\frame
{
  \frametitle{Problems with neon for VMC}
\begin{small}
{\scriptsize
 The lowest-order real solid harmonics are
listed in here
\begin{center} {\large \bf Real Solid Harmonics} \\ 
$\phantom{a}$ \\
\begin{tabular}{ccccc}
\hline\\ 
$m_l\backslash l$ & \phantom{AA}0\phantom{AA}
& \phantom{AA}1\phantom{AA} & \phantom{AA}2\phantom{AA} &
\phantom{AA}3\phantom{AA} \\ 
\hline\\ 
+3& & &
&$\frac{1}{2}\sqrt{\frac{5}{2}}(x^2-3y^2)x$ \\ [7pt] 
+2& & &$\frac{1}{2}\sqrt{3}(x^2-y^2)$&$\frac{1}{2}\sqrt{15}(x^2-y^2)z$
\\ [7pt] 
+1& &x&$\sqrt{3}xz$
&$\frac{1}{2}\sqrt{\frac{3}{2}}(5z^2-r^2)x$ \\ [7pt] 
0&1&y&$\frac{1}{2}(3z^2-r^2)$       &$\frac{1}{2}(5z^2-3r^2)x$ \\
 [7pt] 
-1& &z&$\sqrt{3}yz$
&$\frac{1}{2}\sqrt{\frac{3}{2}}(5z^2-r^2)y$ \\ [7pt] 
-2& & &$\sqrt{3}xy$                  &$\sqrt{15}xyz$ \\ [7pt] 
-3& & &
&$\frac{1}{2}\sqrt{\frac{5}{2}}(3x^2-y^2)y$ \\ [7pt] 
\hline
\end{tabular} 
\end{center}

}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Variational Calculus and Lagrangian Multiplier}
\begin{small}
{\scriptsize
If we generalize the Euler-Lagrange equations to more variables 
and introduce $N^2$ Lagrange multipliers which we denote by 
$\epsilon_{\mu\nu}$, we can write the variational equation for the functional of $E$
\[
  \delta E - \sum_{{\mu}=1}^N\sum_{{\nu}=1}^N \epsilon_{\mu\nu} \delta
  \int \psi_{\mu}^* \psi_{\nu} = 0.
\]
For the orthogonal wave functions $\psi_{\mu}$ this reduces to
\[
  \delta E - \sum_{{\mu}=1}^N \epsilon_{\mu} \delta
  \int \psi_{\mu}^* \psi_{\mu} = 0.
\]
 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Variational Calculus and Lagrangian Multiplier, back to Hartree-Fock}
\begin{small}
{\scriptsize
Our functional is written as 
\[
  E[\Phi] = \sum_{\mu=1}^N \int \psi_{\mu}^*(\mathbf{r}_i)\hat{h_i}\psi_{\mu}(\mathbf{r}_i) d\mathbf{r}_i 
  + \frac{1}{2}\sum_{\mu=1}^N\sum_{\nu=1}^N
   \left[ \int \psi_{\mu}^*(\mathbf{r}_i)\psi_{\nu}^*(\mathbf{r}_j)\frac{1} 
    {r_{ij}}\psi_{\mu}(\mathbf{r}_i)\psi_{\nu}(\mathbf{r}_j)
    d\mathbf{r}_id\mathbf{r}_j \right.
\]
\[ \left.
  - \int \psi_{\mu}^*(\mathbf{r}_i)\psi_{\nu}^*(\mathbf{r}_j)
  \frac{1}{r_{ij}}\psi_{\mu}(\mathbf{r}_j)\psi_{\nu}(\mathbf{r}_i)
  d\mathbf{r}_id\mathbf{r}_j\right]
\]
The more compact version is
\[
  E[\Phi] 
  = \sum_{\mu=1}^N \langle \mu | h | \mu\rangle+ \frac{1}{2}\sum_{\mu=1}^N\sum_{\nu=1}^N\left[\langle \mu\nu |\frac{1}{r_{ij}}|\mu\nu\rangle-\langle \mu\nu |\frac{1}{r_{ij}}|\nu\mu\rangle\right].
\]
 }
 \end{small}
 }




\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Variational Calculus and Lagrangian Multiplier}
\begin{small}
{\scriptsize
Variation with respect to the single-particle wave functions $\psi_{\mu}$ yields then

\begin{equation*}
\begin{split}
  \sum_{\mu=1}^N \int \delta\psi_{\mu}^*\hat{h_i}\psi_{\mu}
  d\mathbf{r}_i  
  + \frac{1}{2}\sum_{{\mu}=1}^N\sum_{{\nu}=1}^N \left[ \int
  \delta\psi_{\mu}^*\psi_{\nu}^*\frac{1} 
  {r_{ij}}\psi_{\mu}\psi_{\nu} d\mathbf{r}_id\mathbf{r}_j- \int
  \delta\psi_{\mu}^*\psi_{\nu}^*\frac{1}{r_{ij}}\psi_{\nu}\psi_{\mu}
  d\mathbf{r}_id\mathbf{r}_j \right] & \\
  + \sum_{\mu=1}^N \int \psi_{\mu}^*\hat{h_i}\delta\psi_{\mu}
  d\mathbf{r}_i 
  + \frac{1}{2}\sum_{{\mu}=1}^N\sum_{{\nu}=1}^N \left[ \int
  \psi_{\mu}^*\psi_{\nu}^*\frac{1} 
  {r_{ij}}\delta\psi_{\mu}\psi_{\nu} d\mathbf{r}_id\mathbf{r}_j- \int
  \psi_{\mu}^*\psi_{\nu}^*\frac{1}{r_{ij}}\psi_{\nu}\delta\psi_{\mu}
  d\mathbf{r}_id\mathbf{r}_j \right] & \\
  -  \sum_{{\mu}=1}^N E_{\mu} \int \delta\psi_{\mu}^*
  \psi_{\mu}d\mathbf{r}_i
  -  \sum_{{\mu}=1}^N E_{\mu} \int \psi_{\mu}^*
  \delta\psi_{\mu}d\mathbf{r}_i & = 0.
\end{split}
\end{equation*}

 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Variational Calculus and Lagrangian Multiplier}
\begin{small}
{\scriptsize
Although the variations $\delta\psi$ and $\delta\psi^*$ are not
independent, they may in fact be treated as such, so that the 
terms dependent on either $\delta\psi$ and $\delta\psi^*$ individually 
may be set equal to zero. To see this, simply 
replace the arbitrary variation $\delta\psi$ by $i\delta\psi$, so that
$\delta\psi^*$ is replaced by $-i\delta\psi^*$, and combine the two
equations. We thus arrive at the Hartree-Fock equations
\[
  \begin{split}
    \left[ -\frac{1}{2}\nabla_i^2-\frac{Z}{r_i} + \sum_{{\nu}=1}^N
      \int \psi_{\nu}^*(\mathbf{r}_j)\frac{1}{r_{ij}}
      \psi_{\nu}(\mathbf{r}_j)d\mathbf{r}_j \right]
    \psi_{\mu}(\mathbf{r}_i)  & \\
    - \left[ \sum_{{\nu}=1}^N \int
      \psi_{\nu}^*(\mathbf{r}_j) 
      \frac{1}{r_{ij}}\psi_{\mu}(\mathbf{r}_j) d\mathbf{r}_j
      \right] \psi_{\nu}(\mathbf{r}_i)  & 
  = \epsilon_{\mu} \psi_{\mu}(\mathbf{r}_i).
  \end{split}
\]
Notice that the integration $\int d\mathbf{r}_j$ implies an
integration over the spatial coordinates $\mathbf{r_j}$ and a summation
over the spin-coordinate of electron $j$.
 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Variational Calculus and Lagrangian Multiplier}
\begin{small}
{\scriptsize
The two first terms are the one-body kinetic energy and the
electron-nucleus potential. The third or
\emph{direct} term is the averaged electronic repulsion of the other
electrons. This term is identical to the Coulomb integral introduced in
the simple perturbative approach to the helium atom. As written, the
term includes the 'self-interaction' of 
electrons when $i=j$. The self-interaction is cancelled in the fourth
term, or the \emph{exchange} term. The exchange term results from our
inclusion of the Pauli principle and the assumed determinantal form of
the wave-function. The effect of exchange is for electrons of
like-spin to avoid each other.
 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Variational Calculus and Lagrangian Multiplier}
\begin{small}
{\scriptsize
  A theoretically convenient form of the
Hartree-Fock equation is to regard the direct and exchange operator
defined through 
\begin{equation*}
  V_{\mu}^{d}(\mathbf{r}_i) = \int \psi_{\mu}^*(\mathbf{r}_j) 
  \frac{1}{r_{ij}}\psi_{\mu}(\mathbf{r}_j) d\mathbf{r}_j
\end{equation*}
and
\begin{equation*}
  V_{\mu}^{ex}(\mathbf{r}_i) g(\mathbf{r}_i) 
  = \left(\int \psi_{\mu}^*(\mathbf{r}_j) 
  \frac{1}{r_{ij}}g(\mathbf{r}_j) d\mathbf{r}_j
  \right)\psi_{\mu}(\mathbf{r}_i),
\end{equation*}
respectively. 
 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Variational Calculus and Lagrangian Multiplier}
\begin{small}
{\scriptsize
The function $g(\mathbf{r}_i)$ is an arbitrary function,
and by the substitution $g(\mathbf{r}_i) = \psi_{\nu}(\mathbf{r}_i)$
we get
\begin{equation*}
  V_{\mu}^{ex}(\mathbf{r}_i) \psi_{\nu}(\mathbf{r}_i) 
  = \left(\int \psi_{\mu}^*(\mathbf{r}_j) 
  \frac{1}{r_{ij}}\psi_{\nu}(\mathbf{r}_j)
  d\mathbf{r}_j\right)\psi_{\mu}(\mathbf{r}_i).
\end{equation*}
 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Variational Calculus and Lagrangian Multiplier}
\begin{small}
{\scriptsize
We may then rewrite the Hartree-Fock equations as
\[
  H_i^{HF} \psi_{\nu}(\mathbf{r}_i) = \epsilon_{\nu}\psi_{\nu}(\mathbf{r}_i),
\]
with
\[
  H_i^{HF}= h_i + \sum_{\mu=1}^NV_{\mu}^{d}(\mathbf{r}_i) -
  \sum_{\mu=1}^NV_{\mu}^{ex}(\mathbf{r}_i),
\]

and where $h_i$ is the one-body part
 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Explicit expression for Direct Term}
\begin{small}
{\scriptsize
We want to show first that 
\begin{equation*}
  V_{\mu}^{d}(\mathbf{r}_i) = \int \psi_{\mu}^*(\mathbf{r}_j) 
  \frac{1}{r_{ij}}\psi_{\mu}(\mathbf{r}_j) d\mathbf{r}_j
\end{equation*}
is 
\[
V_{\mu}^{d}(\mathbf{r}_i)=V_{nl}^{d}(r_i) = \sum_{n'l'}2(2l'+1)\int_0^{\infty}|u_{n'l'}(r_j)|^2\frac{1}{r_{>}}dr_j
\]
with $ r_{>} = max(r_i,r_j)$.
 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Explicit expression for Direct Term}
\begin{small}
{\scriptsize
We need to break down the single-particle wave functions  in radial, angular and spin parts. Note that direct term is diagonal 
in the spin quantum numbers.
The single-particle wave functions are
\[
\psi_{\alpha}({\bf r})=\psi_{nlm_lsm_s}({\bf r})=\phi_{nlm_l}({\bf r})\xi_{m_s}(s)
\]
with 
\[
    \phi_{nlm_l}({\bf r})= R_{nl}(r)Y_{lm_l}(\hat{r})
\]
and we defined $u_{nl}(r) = rR_{nl}(r)$.
The direct term is, with a factor two from the spin degrees of freedom gives for a single shell
\[
V_{n'l'}^{d}(r_i) = 2\sum_{m_l'=-l'}^{l}\int_0^{\infty}|\phi_{n'l'm_l'}(r_j)|^2\frac{1}{r_{ij}}d{\bf r}_j
\]
which reads
\[
V_{n'l'}^{d}(r_i) = 2\int_0^{\infty}|u_{n'l'}(r_j)|^2\frac{1}{r_{ij}}dr_j\sum_{m_l'=-l'}^{l}|Y_{lm_l'}(\theta_j,\phi_j)|^2d\Omega_j,
\]
with $d\Omega_j$  the angular part $d(cos\theta_j)d\phi_j$. 
 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Explicit expression for Direct Term}
\begin{small}
{\scriptsize
The addition theorem  ofthe spherical harmonics  yields
\[
\sum_{m_l'=-l'}^{l}|Y_{lm_l'}(\theta_j,\phi_j)|^2 = \frac{2l'+1}{4\pi},
\]
resulting in
\[
V_{n'l'}^{d}(r_i) = \frac{2(2l'+1)}{4\pi}\int_0^{\infty}|u_{n'l'}(r_j)|^2\frac{1}{r_{ij}}dr_jd\Omega_j.
\]
The quantity $r_{ij}$ depends on the angles of particle $i$ and $j$.
 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Explicit expression for Direct Term}
\begin{small}
{\scriptsize
The integral over the angles can be performed by expanding $1/r_{ij}$ in terms of spherical harmonics and using the fact that
the functions $u_{n'l'}$ do not depend on the angles.
We have
\[
\frac{1}{r_{ij}}=\frac{1}{r_i}\sum_{l=0}^{\infty}\left(\frac{r_j}{r_i}\right)^lP_l cos(\theta)),
\]
if $r_i > r_j$ or 
\[
\frac{1}{r_{ij}}=\frac{1}{r_j}\sum_{l=0}^{\infty}\left(\frac{r_i}{r_j}\right)^lP_lcos(\theta)),
\]
if $r_j > r_i$.  In a compact form it reads
\[
\frac{1}{r_{ij}}=\sum_{l=0}^{\infty}\left(\frac{(r_<)^l}{(r_>)^{l+1}}\right)P_lcos(\theta)),
\]
with $ r_{>} = max(r_i,r_j)$ and with $ r_{<} = min(r_i,r_j)$.
$P_l$ is the Legendre polynomial and 
\[
cos(\theta)=cos(\theta_i)cos(\theta_j)+sin(\theta_i)sin(\theta_j)cos(\phi_i-\phi_j).
\]
 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Explicit expression for Direct Term}
\begin{small}
{\scriptsize
We can use the expression for spherical harmonics to express the interaction as
\[
\frac{1}{r_{ij}}=\sum_{l=0}^{\infty}\sum_{m_l'=-l'}^{l}\left(\frac{(r_<)^l}{(r_>)^{l+1}}\right)Y^*_{lm_l'}(\theta_i,\phi_i)Y_{lm_l'}(\theta_j,\phi_j)
\]
and inserting it we obtain the final expression as
\[
\Phi(r_i) = \sum_{n'l'}2(2l'+1)\int_0^{\infty}|u_{n'l'}(r_j)|^2\frac{1}{r_{>}}dr_j
\]
with $ r_{>} = max(r_i,r_j)$.
 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Explicit expression for Exchange Term}
\begin{small}
{\scriptsize
Exercise: show that the exchange part can be written as 
\[
F_{nl}(r_i) = \sum_{n'l'}\sum_{\lambda=|l-l'|}^{l+l'}\frac{2l'+1}{2\lambda+1}<ll'00|\lambda 0>^2
\left[\int_0^{\infty} u_{n'l'}^*(r_j)\frac{(r_{<})^{\lambda}}{(r_{>})^{\lambda+1}}u_{nl}(r_j)dr_j\right]u_{n'l'}(r_i)
\]
with $ r_{>} = max(r_i,r_j)$ and 
with $ r_{<} = min(r_i,r_j)$.
 }
 \end{small}
 }

\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Explicit expression for Exchange Term}
\begin{small}
{\scriptsize
The Clebsch-Gordan coefficient can be written as 
\[
\langle ll'00|\lambda 0\rangle=\frac{1}{2}\sqrt{2\lambda+1}(1+(-1)^{2g})(-1)^g\sqrt{\frac{(2g-2l)!(2g-2l')!(2g-2l')!}{(2g+1)!}}
\]
\[
\times \frac{g!}{(g-l)!(g-l')!(g-\lambda)!}
\]
with $2g=2l+\lambda$. You can write a small function for this expression or use the enclosed programs.
 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Explicit expressions for various Atoms}
\begin{small}
{\scriptsize
We need to discuss how the Hartree-Fock equations look like for helium, beryllium and neon.

The general equations take the form
\[
  \begin{split}
    \left[ -\frac{1}{2}\nabla_i^2-\frac{Z}{r_i} + \sum_{{\nu}=1}^N
      \int \psi_{\nu}^*(\mathbf{r}_j)\frac{1}{r_{ij}}
      \psi_{\nu}(\mathbf{r}_j)d\mathbf{r}_j \right]
    \psi_{\mu}(\mathbf{r}_i)  & \\
    - \left[ \sum_{{\nu}=1}^N \int
      \psi_{\nu}^*(\mathbf{r}_j) 
      \frac{1}{r_{ij}}\psi_{\mu}(\mathbf{r}_j) d\mathbf{r}_j
      \right] \psi_{\nu}(\mathbf{r}_i)  & 
  = \epsilon_{\mu} \psi_{\mu}(\mathbf{r}_i).
  \end{split}
\]
Notice that the integration $\int d\mathbf{r}_j$ implies an
integration over the spatial coordinates $\mathbf{r_j}$ and a summation
over the spin-coordinate of electron $j$.
 }
 \end{small}
 }




\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Explicit expressions for various Atoms}
\begin{small}
{\scriptsize
  A theoretically convenient form of the
Hartree-Fock equation is to regard the direct and exchange operator
defined through 
\begin{equation*}
  V_{\mu}^{d}(\mathbf{r}_i) = \int \psi_{\mu}^*(\mathbf{r}_j) 
  \frac{1}{r_{ij}}\psi_{\mu}(\mathbf{r}_j) d\mathbf{r}_j
\end{equation*}
and
\begin{equation*}
  V_{\mu}^{ex}(\mathbf{r}_i) g(\mathbf{r}_i) 
  = \left(\int \psi_{\mu}^*(\mathbf{r}_j) 
  \frac{1}{r_{ij}}g(\mathbf{r}_j) d\mathbf{r}_j
  \right)\psi_{\mu}(\mathbf{r}_i),
\end{equation*}
respectively. 
 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Hartree-Fock Equations}
\begin{small}
{\scriptsize
We may then rewrite the Hartree-Fock equations as
\[
  H_i^{HF} \psi_{\nu}(\mathbf{r}_i) = \epsilon_{\nu}\psi_{\nu}(\mathbf{r}_i),
\]
with
\[
  H_i^{HF}= h_i + \sum_{\mu=1}^NV_{\mu}^{d}(\mathbf{r}_i) -
  \sum_{\mu=1}^NV_{\mu}^{ex}(\mathbf{r}_i),
\]
and where $h_i$ is the one-body part.
 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Explicit expressions for various Atoms, helium}
\begin{small}
{\scriptsize
The Slater determinant for helium reads
with two electrons in the $1s$ state
\[
   \Phi({\bf r}_1,{\bf r}_2,\alpha,\beta)=\frac{1}{\sqrt{2}}
\left| \begin{array}{cc} \psi_{\alpha}({\bf r}_1)& \psi_{\alpha}({\bf r}_2)\\\psi_{\beta}({\bf r}_1)&\psi_{\beta}({\bf r}_2)\end{array} \right|,
\]
with $\alpha=nlm_lsm_s=1001/21/2$ and $\beta=nlm_lsm_s=1001/2-1/2$  or using $m_s=1/2=\uparrow$ and $m_s=-1/2=\downarrow$ as 
$\alpha=nlm_lsm_s=1001/2\uparrow$ and $\beta=nlm_lsm_s=1001/2\downarrow$. 
Writing out the Slater determinant we obtain
\[
\Phi({\bf r}_1,{\bf r}_2,\alpha,\beta)=
\frac{1}{\sqrt{2}}\left[
\psi_{\alpha}({\bf r}_1)\psi_{\beta}({\bf r}_2)-
\psi_{\beta}({\bf r}_1)\psi_{\gamma}({\bf r}_2)\right],
\]
and we see that the Slater determinant is antisymmetric with respect to the permutation of two particles, that is
\[
\Phi({\bf r}_1,{\bf r}_2,\alpha,\beta)=-\Phi({\bf r}_2,{\bf r}_1,\alpha,\beta),
\]
 }
 \end{small}
 }




\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Explicit expressions for various Atoms, helium}
\begin{small}
{\scriptsize
In the derivations of the direct and exchange terms we have not discussed the role of the electron spin.

Let us introduce 
\[
  \psi_{nlm_lsm_s} = \phi_{nlm_l}({\bf r})\xi_{m_s}(s)
\]
with $s$ is the spin ($1/2$ for electrons), $m_s$ is the spin projection $m_s=\pm 1/2$, and the spatial part is
\[
   \phi_{nlm_l}({\bf r}) =  R_{nl}(r)Y_{lm_l}(\hat{{\bf r}})
\]
with $Y$ the spherical harmonics and $u_{nl} = rR_{nl}$.
We have for helium
\[
\Phi({\bf r}_1,{\bf r}_2,\alpha,\beta)=
\frac{1}{\sqrt{2}}\phi_{100}({\bf r}_1)\phi_{100}({\bf r}_2)\left[
\xi_{\uparrow}(1)\xi_{\downarrow}(2)-\xi_{\uparrow}(2)\xi_{\downarrow}(1)\right],
\]
}
 \end{small}
 }





\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Explicit expressions for various Atoms, helium}
\begin{small}
{\scriptsize
The direct term acts on
\[
\frac{1}{\sqrt{2}}\phi_{100}({\bf r}_1)\phi_{100}({\bf r}_2)
\xi_{\uparrow}(1)\xi_{\downarrow}(2)
\]
while the exchange term acts on 
\[
-\frac{1}{\sqrt{2}}\phi_{100}({\bf r}_1)\phi_{100}({\bf r}_2)\xi_{\uparrow}(2)\xi_{\downarrow}(1).
\]
How do these terms get translated into the Hartree and the Fock  terms?
 }
 \end{small}
 }




\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Explicit expressions for various Atoms, helium}
\begin{small}
{\scriptsize
The Hartree term
\begin{equation*}
  V_{\mu}^{d}(\mathbf{r}_i) = \int \psi_{\mu}^*(\mathbf{r}_j) 
  \frac{1}{r_{ij}}\psi_{\mu}(\mathbf{r}_j) d\mathbf{r}_j,
\end{equation*}
acts on $\psi_{\lambda}(\mathbf{r}_i)=\phi_{nlm_l}({\bf r}_i)\xi_{m_s}(s_i)$, that is  it results in 
\[
  V_{\mu}^{d}(\mathbf{r}_i)\psi_{\lambda}(\mathbf{r}_i) = \left(\int \psi_{\mu}^*(\mathbf{r}_j) 
  \frac{1}{r_{ij}}\psi_{\mu}(\mathbf{r}_j) d\mathbf{r}_j\right)\psi_{\lambda}(\mathbf{r}_i),
\]
and accounting for spins we have
\[
  V_{nlm_l\uparrow}^{d}(\mathbf{r}_i)\psi_{\lambda}(\mathbf{r}_i) = \left(\int \psi_{nlm_l\uparrow}^*(\mathbf{r}_j) 
  \frac{1}{r_{ij}}\psi_{nlm_l\uparrow}(\mathbf{r}_j) d\mathbf{r}_j\right)\psi_{\lambda}(\mathbf{r}_i),
\]
and 
\[
  V_{nlm_l\downarrow}^{d}(\mathbf{r}_i)\psi_{\lambda}(\mathbf{r}_i) = \left(\int \psi_{nlm_l\downarrow}^*(\mathbf{r}_j) 
  \frac{1}{r_{ij}}\psi_{nlm_l\downarrow}(\mathbf{r}_j) d\mathbf{r}_j\right)\psi_{\lambda}(\mathbf{r}_i),
\]
 }
 \end{small}
 }





\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Explicit expressions for various Atoms, helium}
\begin{small}
{\scriptsize
If the state we act on has spin up, we obtain two terms  from the Hartree part.  
\[
  \sum_{\mu=1}^NV_{\mu}^{d}(\mathbf{r}_i),
\]
and since the interaction does not depend on spin we end up with
a total contribution for helium
\[
  \sum_{\mu=1}^NV_{\mu}^{d}(\mathbf{r}_i)\psi_{\lambda}(\mathbf{r}_i)=\left(2\int \phi_{100}^*(\mathbf{r}_j) 
  \frac{1}{r_{ij}}\phi_{100}(\mathbf{r}_j) d\mathbf{r}_j\right)\psi_{\lambda}(\mathbf{r}_i),
\] 
one from spin up and one from spin down.  
Since the energy for spin up or spin down is the same we can then write the action of the Hartree term as
\[
  \sum_{\mu=1}^NV_{\mu}^{d}(\mathbf{r}_i)\psi_{\lambda}(\mathbf{r}_i)=\left(2\int \phi_{100}^*(\mathbf{r}_j) 
  \frac{1}{r_{ij}}\phi_{100}(\mathbf{r}_j) d\mathbf{r}_j\right)\psi_{100\uparrow}(\mathbf{r}_i).
\] 
(the spin in $\psi_{100\uparrow}$ is irrelevant)
 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Explicit expressions for various Atoms, helium}
\begin{small}
{\scriptsize
What we need to code for helium is then 
\[
\Phi(r_i)u_{10}= 2V_{10}^d(r_i)u_{10}(r_i) = 2\int_0^{\infty}|u_{10}(r_j)|^2\frac{1}{r_{>}}dr_j)u_{10}(r_i).
\]
with $ r_{>} = max(r_i,r_j)$.
What about the exchange or Fock term
\begin{equation*}
  V_{\mu}^{ex}(\mathbf{r}_i) \psi_{\lambda}(\mathbf{r}_i) 
  = \left(\int \psi_{\mu}^*(\mathbf{r}_j) 
  \frac{1}{r_{ij}}\psi_{\lambda}(\mathbf{r}_j) d\mathbf{r}_j
  \right)\psi_{\mu}(\mathbf{r}_i)?
\end{equation*}
 }
 \end{small}
 }




\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Explicit expressions for various Atoms, helium}
\begin{small}
{\scriptsize
We must be careful here with
\begin{equation*}
  V_{\mu}^{ex}(\mathbf{r}_i) \psi_{\lambda}(\mathbf{r}_i) 
  = \left(\int \psi_{\mu}^*(\mathbf{r}_j) 
  \frac{1}{r_{ij}}\psi_{\lambda}(\mathbf{r}_j) d\mathbf{r}_j
  \right)\psi_{\mu}(\mathbf{r}_i),
\end{equation*}
because the spins of $\mu$ and $\lambda$ have to be the same due to the
constraint
\[
 \langle s_{\mu} m_s^{\mu} |  s_{\lambda} m_s^{\lambda} \rangle = \delta_{m_s^{\mu},m_s^{\lambda}}.
\]
This means that if $m_s^{\mu}=\uparrow$ then   $m_s^{\lambda}=\uparrow$ and if 
$m_s^{\mu}=\downarrow$ then   $m_s^{\lambda}=\downarrow$.   That is
\begin{equation*}
  V_{\mu}^{ex}(\mathbf{r}_i) \psi_{\lambda}(\mathbf{r}_i) 
  = \delta_{m_s^{\mu},m_s^{\lambda}}\left(\int \psi_{\mu}^*(\mathbf{r}_j) 
  \frac{1}{r_{ij}}\psi_{\lambda}(\mathbf{r}_j) d\mathbf{r}_j
  \right)\psi_{\mu}(\mathbf{r}_i),
\end{equation*}
 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Explicit expressions for various Atoms, helium}
\begin{small}
{\scriptsize
The consequence is that for the $1s\uparrow$ (and the same for $1s\downarrow$) state we get only one contribution  from the Fock 
term
\[
 \sum_{\mu=1}^NV_{\mu}^{ex}(\mathbf{r}_i)\psi_{\lambda}(\mathbf{r}_i),
\]
\[
  \sum_{\mu=1}^NV_{\mu}^{ex}(\mathbf{r}_i) \psi_{100\uparrow}(\mathbf{r}_i) 
  = \delta_{m_s^{\mu},\uparrow}}\left(\int \psi_{\mu}^*(\mathbf{r}_j) 
  \frac{1}{r_{ij}}\psi_{100\uparrow}(\mathbf{r}_j) d\mathbf{r}_j
  \right)\psi_{\mu}(\mathbf{r}_i),
\]
resulting in 
\[
  \sum_{\mu=1}^NV_{\mu}^{ex}(\mathbf{r}_i) \psi_{100\uparrow}(\mathbf{r}_i) 
  = \left(\int \psi_{100\uparrow}^*(\mathbf{r}_j) 
  \frac{1}{r_{ij}}\psi_{100\uparrow}(\mathbf{r}_j) d\mathbf{r}_j
  \right)\psi_{100\uparrow}(\mathbf{r}_i).
\]
 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Explicit expressions for various Atoms, helium}
\begin{small}
{\scriptsize
The final Fock term for helium is then
\[
  \sum_{\mu=1}^NV_{\mu}^{ex}(\mathbf{r}_i) \psi_{100\uparrow}(\mathbf{r}_i) 
  = \left(\int \psi_{100\uparrow}^*(\mathbf{r}_j) 
  \frac{1}{r_{ij}}\psi_{100\uparrow}(\mathbf{r}_j) d\mathbf{r}_j
  \right)\psi_{100\uparrow}(\mathbf{r}_i),
\]
which is exactly the same as the Hartree term except for a factor of $2$. Else the integral is the same. 
We can then write the differential equation
\[
 \left( -\frac{1}{2} \frac{d^2}{dr^2} +\frac{l (l + 1)}{2r^2}-\frac{2}{r}+ \Phi_{nl}(r)-F_{nl}(r)\right ) u_{nl}(r)  = e_{nl} u_{nl}(r) .
\]
as
\[
  \left(-\frac{1}{2} \frac{d^2}{dr^2} 
       +\frac{l (l + 1)}{2r^2}-\frac{2}{r}+ 2V_{10}^{d}(r)\right ) u_{10}(r)-V_{10}^{ex}(r)  = e_{10} u_{10}(r), 
\]
or
\[
  \left(-\frac{1}{2} \frac{d^2}{dr^2} -\frac{2}{r}+ V_{10}^{d}(r)\right ) u_{10}(r)  = e_{10} u_{10}(r), 
\]
since $l=0$.
The shorthand $V_{10}^{ex}(r)$ contains the $1s$ wave function and can be dangerous later!
 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Explicit expressions for various Atoms, beryllium}
\begin{small}
{\scriptsize
The expression we have obtained are independent of  the spin projections and we have skipped them in the equations.
Last week's exercise was to derive the corresponding equations for beryllium, with two electrons in $1s$ as in helium
but now also two electrons in $2s$.   

The Slater determinant takes the form  
\[
   \Phi({\bf r}_1,{\bf r}_2,,{\bf r}_3,{\bf r}_4, \alpha,\beta,\gamma,\delta)=\frac{1}{\sqrt{4!}}
\left| \begin{array}{cccc} \psi_{100\uparrow}({\bf r}_1)& \psi_{100\uparrow}({\bf r}_2)& \psi_{100\uparrow}({\bf r}_3)&\psi_{100\uparrow}({\bf r}_4) \\
\psi_{100\downarrow}({\bf r}_1)& \psi_{100\downarrow}({\bf r}_2)& \psi_{100\downarrow}({\bf r}_3)&\psi_{100\downarrow}({\bf r}_4) \\
\psi_{200\uparrow}({\bf r}_1)& \psi_{200\uparrow}({\bf r}_2)& \psi_{200\uparrow}({\bf r}_3)&\psi_{200\uparrow}({\bf r}_4) \\
\psi_{200\downarrow}({\bf r}_1)& \psi_{200\downarrow}({\bf r}_2)& \psi_{200\downarrow}({\bf r}_3)&\psi_{200\downarrow}({\bf r}_4) \end{array} \right|,
\]
 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Explicit expressions for various Atoms, beryllium}
\begin{small}
{\scriptsize
When we now spell out the Hartree-Fock equations we get two coupled differential equations, one for $u_{10}$ and one for $u_{20}$.

The $1s$ wave function has the same Hartree-Fock contribution as in helium for the $1s$ state, but the $2s$ state gives two times the Hartree term 
and one time the Fock term.

That is we get
\[
  \sum_{\mu=1}^NV_{\mu}^{d}(\mathbf{r}_i)\psi_{100\uparrow}(\mathbf{r}_i)=2\int_0^{\infty}d\mathbf{r}_j\left( \phi_{100}^*(\mathbf{r}_j) 
  \frac{1}{r_{ij}}\phi_{100}(\mathbf{r}_j)+\phi_{200}^*(\mathbf{r}_j) 
  \frac{1}{r_{ij}}\phi_{200}(\mathbf{r}_j) \right)\psi_{100\uparrow}(\mathbf{r}_i)
\] 
\[
= (2V_{10}^d(\mathbf{r}_i)+2V_{20}^d(\mathbf{r}_i))\psi_{100\uparrow}(\mathbf{r}_i)
\]
for the Hartree part.
 }
 \end{small}
 }





\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Explicit expressions for various Atoms, beryllium}
\begin{small}
{\scriptsize
For the Fock term we get (we fix the spin)
\[
  \sum_{\mu=1}^NV_{\mu}^{ex}(\mathbf{r}_i)\psi_{100\uparrow}(\mathbf{r}_i)=
\int_0^{\infty}d\mathbf{r}_j\phi_{100}^*(\mathbf{r}_j) 
  \frac{1}{r_{ij}}\phi_{100}(\mathbf{r}_j)\psi_{100\uparrow}(\mathbf{r}_i)+  \]
\[
\int_0^{\infty}d\mathbf{r}_j\phi_{200}^*(\mathbf{r}_j) 
  \frac{1}{r_{ij}}\phi_{100}(\mathbf{r}_j) \psi_{200\uparrow}(\mathbf{r}_i)=V_{10}^{ex}(\mathbf{r}_i)+V_{20}^{ex}(\mathbf{r}_i).
\] 
The first term is the same as we have for the Hartree term with $1s$ except the factor of two.
The final differential equation is
\[
  \left(-\frac{1}{2} \frac{d^2}{dr^2}-\frac{4}{r}+ V_{10}^{d}(r)+2V_{20}^d(r)\right ) u_{10}(r)-V_{20}^{ex}(r)  = e_{10} u_{10}(r). 
\]
Note again that the $V_{20}^{ex}(r)$ contains the $1s$ function in the integral, that is 
\[
V_{20}^{ex}(r)=\int_0^{\infty}d\mathbf{r}_j\phi_{200}^*(\mathbf{r}_j)\frac{1}{r-r_j}\phi_{100}(\mathbf{r}_j) \psi_{200\uparrow}(\mathbf{r}).
\]
 }
 \end{small}
 }




\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Explicit expressions for various Atoms, beryllium}
\begin{small}
{\scriptsize
The $2s$ wave function obtains the following Hartree term  (recall that the interaction has no spin dependence)
\[
  \sum_{\mu=1}^NV_{\mu}^{d}(\mathbf{r}_i)\psi_{200\uparrow}(\mathbf{r}_i)=2\int_0^{\infty}d\mathbf{r}_j\left( \phi_{100}^*(\mathbf{r}_j) 
  \frac{1}{r_{ij}}\phi_{100}(\mathbf{r}_j)+\phi_{200}^*(\mathbf{r}_j) 
  \frac{1}{r_{ij}}\phi_{200}(\mathbf{r}_j) \right)\psi_{200\uparrow}(\mathbf{r}_i)=
\]
\[
(2V_{10}^d(\mathbf{r}_i)+2V_{20}^d(\mathbf{r}_i))\psi_{200\uparrow}(\mathbf{r}_i)
\] 
 }
 \end{small}
 }





\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Explicit expressions for various Atoms, beryllium}
\begin{small}
{\scriptsize
For the Fock term we get 
\[
  \sum_{\mu=1}^NV_{\mu}^{ex}(\mathbf{r}_i)\psi_{200\uparrow}(\mathbf{r}_i)=
\int_0^{\infty}d\mathbf{r}_j\phi_{100}^*(\mathbf{r}_j) 
  \frac{1}{r_{ij}}\phi_{200}(\mathbf{r}_j)\psi_{100\uparrow}(\mathbf{r}_i)+ 
\]
\[
\int_0^{\infty}d\mathbf{r}_j\phi_{200}^*(\mathbf{r}_j) 
  \frac{1}{r_{ij}}\phi_{200}(\mathbf{r}_j)\psi_{200\uparrow}(\mathbf{r}_i)= V_{10}^{ex}(\mathbf{r}_i)+V_{20}^{ex}(\mathbf{r}_i) \] 
The second term is the same as we have for the Hartree term with $2s$.
The final differential equation is
\[
  \left(-\frac{1}{2} \frac{d^2}{dr^2}-\frac{4}{r}+ 2V_{10}^{d}(r)+V_{20}^d(r)\right ) u_{20}(r)-V_{10}^{ex}(r)  = e_{20} u_{20}(r). 
\]
Note again that the $V_{10}^{ex}(r)$ contains the $2s$ function in the integral, that is 
\[
V_{10}^{ex}(r)=\int_0^{\infty}d\mathbf{r}_j\phi_{100}^*(\mathbf{r}_j)\frac{1}{r-r_j}\phi_{200}(\mathbf{r}_j) \psi_{100\uparrow}(\mathbf{r}).
\]
 }
 \end{small}
 }




\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Final expressions for  beryllium}
\begin{small}
{\scriptsize
We have  two coupled differential equations
\[
  \left(-\frac{1}{2} \frac{d^2}{dr^2}-\frac{4}{r}+ V_{10}^{d}(r)+2V_{20}^d(r)\right ) u_{10}(r)-V_{20}^{ex}(r)  = e_{10} u_{10}(r), 
\] 
and
\[
  \left(-\frac{1}{2} \frac{d^2}{dr^2}-\frac{4}{r}+ 2V_{10}^{d}(r)+V_{20}^d(r)\right ) u_{20}(r)-V_{10}^{ex}(r)  = e_{20} u_{20}(r). 
\]
Recall again that the interaction does not depend  on spin. This means that the single-particle energies and single-particle function 
$u$ do not depend on spin.   Also

Exercise: derive the equivalent expressions for neon.  
 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Hartree-Fock: Final expressions for  neon}
\begin{small}
{\scriptsize
Exercise:  fill in the missing parts
\[
  \left(-\frac{1}{2} \frac{d^2}{dr^2}-\frac{10}{r}+ V_{10}^{d}(r)+2V_{20}^d(r)+?\right ) u_{10}(r)-V_{20}^{ex}(r) +? = e_{10} u_{10}(r), 
\] 
and
\[
  \left(-\frac{1}{2} \frac{d^2}{dr^2}-\frac{10}{r}+ 2V_{10}^{d}(r)+V_{20}^d(r)+?\right ) u_{20}(r)-V_{10}^{ex}(r)+?  = e_{20} u_{20}(r). 
\]
and 
\[
  \left(-\frac{1}{2} \frac{d^2}{dr^2}+\frac{2}{2r^2}-\frac{10}{r}+ 2V_{10}^{d}(r)+2V_{20}^d(r)+?\right ) u_{21}(r)-V_{10}^{ex}(r)-V_{20}^{ex}(r)+?  = e_{21} u_{21}(r). 
\]
 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Useful equations}
\begin{small}
{\scriptsize
The $1s$ hydrogen like wave function
\[
R_{10}(r) =  2\left(\frac{Z}{a_0}\right)^{3/2}\exp{(-Zr/a_0)}= u_{10}/r 
\]
The total energy for helium (not the Hartree or Fock terms) from  the direct and the exchange term should give $5Z/8$.

The single-particle energy with no interactions should give $-Z^2/2n^2$. 

The $2s$ hydrogen-like wave function is
\[
R_{20}(r) =  2\left(\frac{Z}{2a_0}\right)^{3/2}\left(1-\frac{Zr}{2a_0}\right)\exp{(-Zr/2a_0)}= u_{20}/r 
\]
and the $2p$ hydrogen -like wave function is
\[
R_{21}(r) =  \frac{1}{\sqrt{3}}\left(\frac{Z}{2a_0}\right)^{3/2}\frac{Zr}{a_0}\exp{(-Zr/2a_0)}= u_{21}/r 
\]
We use $a_0=1$.
 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Effective Charge and first Iteration}
\begin{small}
{\scriptsize
If  we compute the total energy  of the helium atom with the function 
\[
R_{10}(r) =  2\left(\frac{Z}{a_0}\right)^{3/2}\exp{(-Zr/a_0)}= u_{10}/r, 
\]
as a trial single-particle wave fuction, we obtain a total energy  (one-body and two-body) 
\[ 
E[Z]= Z^2-4Z+\frac{5}{8}Z.
\] 
The minimum is not at $Z=2$.  Take the derivative wrt $Z$ and we find that the minimum is at 
\[
   Z=2-\frac{5}{16} = 1.6875
\]
and represents an optimal  effective charge.
When we do the Hartree-Fock calculations and use the optimal single-particle wave function  in a variational
Monte Carlo calculation, we should have the wave function calculated at the optimal value. 
 }
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Hartree-Fock by varying the coefficients of a wave function expansion}
\begin{small}
{\scriptsize
For closed shell atoms it is natural to consider the spin-orbitals as
paired. For example, two $1s$ orbitals with different spin have the same
spatial wave-function, but orthogonal spin functions. For open-shell
atoms two procedures are commonly used; the 
\emph{restricted Hartree-Fock} (RHF) and 
\emph{unrestricted Hartree-Fock} (UHF). 
In RHF all the electrons except those occupying open-shell orbitals
are forced to occupy doubly occupied spatial orbitals, while in UHF all
orbitals are treated independently. The UHF, of course, yields a lower
variational energy than the RHF formalism. One disadvantage of the
UHF over the RHF, is that whereas the RHF wave function is an
eigenfunction of $S^2$, the UHF function is not; that is, the
total spin angular momentum is not a well-defined quantity for a UHL
wave-function. Here we limit our attention to closed shell RHF's,
and show how the coupled HF equations may be turned into a matrix
problem by expressing the spin-orbitals using known sets of basis
functions.
 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Hartree-Fock by varying the coefficients of a wave function expansion}
\begin{small}
{\scriptsize
In principle, a complete set of basis functions must be used to
represent spin-orbitals exactly, but this is not computationally
feasible. A given finite set of basis functions is, due to the
incompleteness of the basis set, associated with 
a \emph{basis-set truncation error}. The limiting HF energy, with
truncation error equal to zero, will be referred to as the
\emph{Hartree-Fock limit}.
 }
 \end{small}
 }

\frame[containsverbatim]
{
  \frametitle{Hartree-Fock by varying the coefficients of a wave function expansion}
\begin{small}
{\scriptsize
The computational time depends on the number of
basis-functions and of the difficulty in computing the integrals of
both the Fock matrix and the overlap matrix. Therefore we wish to keep
the number of basis functions as low as possible and choose the
basis-functions cleverly. By cleverly we mean that the
truncation error should be kept as low as possible, and that the
computation of the matrix elements of both the overlap and the Fock
matrices should not be too time consuming.
 }
 \end{small}
 }

\frame[containsverbatim]
{
  \frametitle{Hartree-Fock by varying the coefficients of a wave function expansion}
\begin{small}
{\scriptsize
One choice of basis functions are the so-called \emph{Slater type orbitals} (STO).
They are defined as

\begin{equation}
  \Psi_{nlm_l}(r,\theta,\phi) = {\cal N}r^{n_{_{eff}}-1}
  e^{\frac{Z_{_{eff}}\rho}{n_{_{eff}}}}Y_{lm_l}(\theta,\phi).
\label{STO}
\end{equation}

Here ${\cal N}$ is a normalization constant that for the purpose of
basis set expansion may be put into the unknown $c_{i\mu}$'s,
$Y_{lm_l}$ is a spherical harmonic
and $\rho=r/a_0$.

 }
 \end{small}
 }


\section[Week 16]{Weeks 16}
\frame
{
  \frametitle{Topics for Week 16, April 19-23}
  \begin{block}{Hartree-Fock theory and Density functional theory}
\begin{itemize}
\item Discussion of project 2, Hartree-Fock theory, wave functions and computation of 
Coulomb matrix elements.
\item Project 1: Continuation on how to implement the  Slater determinant and correlation part and the conjugate gradient method.
\item Discussion of Hartree-Fock program
\item 
\end{itemize}
  \end{block}
} 


\frame[containsverbatim]
{
  \frametitle{Hartree-Fock code}
\begin{small}
{\scriptsize
Some useful global variables (brute force coding)
\begin{lstlisting}
int Z = 4;
const int basiscutoff = 20;  // Need to check this
const int nmax = basiscutoff/2;
double coulombIntegrals[nmax][nmax][nmax][nmax];
\end{lstlisting}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Hartree-Fock code}
\begin{small}
{\scriptsize
\begin{lstlisting}
//Radial wave functions with n=1,2.. and  l=0 (s-waves)
double radial(int n, double r) {
  
  return pow(2.0*Z/n,1.5)*sqrt(1.0/(2*n*n))*LaguerreGeneral(n-1,1,2*Z*r/n)*exp(-Z*r/n);
}

\end{lstlisting}
}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{Hartree-Fock code, direct term}
\[
\int_0^{\infty} r_1^2dr_1R_{n_{\alpha}0}^*(r_1)R_{n_{\gamma}0}(r_1)
\]
\[ 
\times\left[\frac{1}{(r_1)}\int_0^{r_1} r_2^2dr_2 R_{n_{\beta}0}^*(r_2) 
  R_{n_{\delta}0}(r_2)+\int_{r_1}^{\infty} r_2dr_2 R_{n_{\beta}0}^*(r_2) 
  R_{n_{\delta}0}(r_2)\right].
\]
}


\frame[containsverbatim]
{
  \frametitle{Hartree-Fock code}
\begin{small}
{\scriptsize
\begin{lstlisting}
//Function to compute the inner integrals of the Coulomb matrix elements
double innerIntegrals(double r1, int n2, int n4, double upperlimit, int N) {

  double sum = 0;
  double x[N]; //Integration points
  double w[N]; //Integration weights
\end{lstlisting}
\[
\frac{1}{(r_1)}\int_0^{r_1} r_2^2dr_2 R_{n_{\beta}0}^*(r_2) 
  R_{n_{\delta}0}(r_2)
\]
\begin{lstlisting}
  //First integral
  gauleg(0,r1,x,w,N);
  for (int i = 0; i < N; i++)
    sum += w[i]*x[i]*x[i]*radial(n2,x[i])*radial(n4,x[i]);
  sum /= r1;
}
\end{lstlisting}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Hartree-Fock code}
\begin{small}
{\scriptsize
\[
\int_{r_1}^{\infty} r_2dr_2 R_{n_{\beta}0}^*(r_2) 
  R_{n_{\delta}0}(r_2)
\]
\begin{lstlisting}
  //Second integral
  gauleg(r1,upperlimit,x,w,N);
  for (int i = 0; i < N; i++)
    sum += w[i]*x[i]*radial(n2,x[i])*radial(n4,x[i]);

  return sum;
}
\end{lstlisting}
}
\end{small}
}





\frame[containsverbatim]
{
  \frametitle{Hartree-Fock code}
\begin{small}
{\scriptsize
\begin{lstlisting}
//Function for the Coulomb integral <n1,n2|V|n3,n4>
//This is specialized to l=0, only the quantum number n is needed
double coulombIntegral(int n1, int n2, int n3, int n4) {
  int N = 100; //Number of integration points
  double sum = 0;
  double x[N]; //Integration points
  double w[N]; //Integration weights
  double upperlimit = 100; //Upper limit (cutoff, in principle infinite)
  //Compute the integral
  gauleg(0,upperlimit,x,w,N);
  for (int i = 0; i < N; i++)
    sum += w[i]*x[i]*x[i]*radial(n1,x[i])*radial(n3,x[i])*innerIntegrals(x[i],n2,n4,upperlimit,N);
  return sum;
}

\end{lstlisting}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Hartree-Fock code}
\begin{small}
{\scriptsize
We need matrix element with greek indices (hydrogen-like wave functions)
\[
\langle ab|V|cd\rangle_{AS}=
\sum_{{\alpha\beta\gamma\delta}} C^*_{a\alpha}C^*_{b\beta}C_{a\gamma}C_{b\delta}\langle \alpha\beta|V|\gamma\delta\rangle_{AS}.
\]
\begin{lstlisting}
//Function which returns the matrix element <alpha,beta|V|gamma,delta>
double matrixElement(int alpha, int beta, int gamma, int delta) {

  //Return zero if orthogonality due to spin value
  if (alpha%2 != gamma%2 || beta%2 != delta%2)
    return 0;
  else //Return the relevant Coulomb integral
    return coulombIntegrals[alpha/2][beta/2][gamma/2][delta/2];
}

\end{lstlisting}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Hartree-Fock code}
\begin{small}
{\scriptsize
\begin{lstlisting}
//Function that returns the anti-symmetrized matrix element
//  <alpha,beta|V|gamma,delta>_AS
double matrixElementAS(int alpha, int beta, int gamma, int delta) {

  return matrixElement(alpha,beta,gamma,delta) - matrixElement(alpha,beta,delta,gamma);
}

\end{lstlisting}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Hartree-Fock code}
\begin{small}
{\scriptsize
\[
  E[\Psi] 
  = \sum_{a=1}^N \sum_{\alpha\beta} C^*_{a\alpha}C_{a\beta}\langle \alpha | h | \beta \rangle +
  \frac{1}{2}\sum_{ab=1}^N\sum_{{\alpha\beta\gamma\delta}} C^*_{a\alpha}C^*_{b\beta}C_{a\gamma}C_{b\delta}\langle \alpha\beta|V|\gamma\delta\rangle_{AS}.
\]
\begin{lstlisting}
//Function to compute the Hartree-Fock energy
double calcEnergy(double** C) {
  
  double energy = 0;
  for (int a = 0; a < Z; a++) {
    for (int beta = 0; beta < basiscutoff; beta++) 
      energy -=  C[a][beta]*C[a][beta] * Z*Z / (2.0*(beta/2+1)*(beta/2+1));
  }
}
\end{lstlisting}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Hartree-Fock code}
\begin{small}
{\scriptsize
\begin{lstlisting}
//Function to compute the Hartree-Fock energy, continues
  for (int a = 0; a < Z; a++)
    for (int b = 0; b < Z; b++)
      for (int mu = 0; mu < basiscutoff; mu++)
	for (int nu = 0; nu < basiscutoff; nu++)
	  for (int beta = 0; beta < basiscutoff; beta++)
	    for (int delta = 0; delta < basiscutoff; delta++)
	      energy += .5*C[a][mu]*C[b][nu]*C[a][beta]*C[b][delta]*matrixElementAS(mu,nu,beta,delta);

  return energy;
}
\end{lstlisting}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Hartree-Fock code, main part}
\begin{small}
{\scriptsize
\begin{lstlisting}
int main() {
.....
  //Compute the Coulomb matrix elements
  for (int n1 = 1; n1 <= nmax; n1++)
    for (int n2 = 1; n2 <= nmax; n2++)
      for (int n3 = n1; n3 <= nmax; n3++)
	for (int n4 = n2; n4 <= nmax; n4++)
	  coulombIntegrals[n3-1][n2-1][n1-1][n4-1] = coulombIntegrals[n1-1][n4-1][n3-1][n2-1] 
	    = coulombIntegrals[n3-1][n4-1][n1-1][n2-1] = coulombIntegrals[n1-1][n2-1][n3-1][n4-1] = coulombIntegral(n1,n2,n3,n4);
\end{lstlisting}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Hartree-Fock code, main part, coefficients $C$}
\begin{small}
{\scriptsize
\begin{lstlisting}
  //Declare the matrix C, initialize with coefficient as a pure 
//  hydrogenic state with no mixing
  double** C = (double**)matrix(Z,basiscutoff,sizeof(double));
  for (int i = 0; i < Z; i++)
    for (int j = 0; j < basiscutoff; j++)
      C[i][j] = 0;
  for (int i = 0; i < Z; i++)
    C[i][i] = 1;
\end{lstlisting}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Hartree-Fock code}
\begin{small}
{\scriptsize
\begin{lstlisting}
  //Declare the Hartree-Fock matrix
  double** hHF = (double**)matrix(basiscutoff,basiscutoff,sizeof(double));

  //We need two vectors to keep track of the tri-diagonal matrix in Householder's //algorithm and the final eigenvalues
  double* d = new double[basiscutoff];
  double* e = new double[basiscutoff];

  //Energy at present and previous iterations. If self-consistency is reached
  // they should be equal within a small chosen numerical lower limit 
  double energy = 0;
  double energyprev;

\end{lstlisting}
}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{Hartree-Fock matrix}
\[
h_{\alpha\gamma}^{HF}=\langle \alpha | h | \gamma \rangle+
\sum_{a=1}^N\sum_{\beta\delta} C^*_{a\beta}C_{a\delta}\langle \alpha\beta|V|\gamma\delta\rangle_{AS},
\]
we can rewrite the new equations as 
\[
\sum_{\gamma}h_{\alpha\gamma}^{HF}C_{k\gamma}=\epsilon_kC_{k\alpha}.
\]
}


\frame[containsverbatim]
{
  \frametitle{Hartree-Fock code, the iteration itself}
\begin{small}
{\scriptsize
\begin{lstlisting}
  do {
    energyprev = energy;
    //Set up the Hartree-Fock matrix
    for (int alpha = 0; alpha < basiscutoff; alpha++) {
      for (int gamma = 0; gamma < basiscutoff; gamma++) {
	//Matrix element <alpha|h|gamma>
	if (alpha == gamma)
	  hHF[alpha][gamma] = -Z*Z / (2.0*(alpha/2+1)*(alpha/2+1)); // E = -Z^2/(2n^2)
\end{lstlisting}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Hartree-Fock code, the iteration itself}
\begin{small}
{\scriptsize
\begin{lstlisting}
	else
	  hHF[alpha][gamma] = 0;
	for (int a = 0; a < Z; a++)
	  for (int beta = 0; beta < basiscutoff; beta++) 
	    for (int delta = 0; delta < basiscutoff; delta++)
	      hHF[alpha][gamma] += C[a][beta]*C[a][delta]*matrixElementAS(alpha,beta,gamma,delta);
      }
    }
    
\end{lstlisting}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Hartree-Fock code, the iteration}
\begin{small}
{\scriptsize
\begin{lstlisting}
    // Find eigenvalues and eigenvectors, no sorting
    tred2(hHF,basiscutoff,d,e);
    tqli(d,e,basiscutoff,hHF);
    //Sort eigenvectors in order that lowest energy comes first
    sort(hHF,d,basiscutoff);
    
    //The Eigenvectors are stored in the rows of C
    for (int k = 0; k < Z; k++)
      for (int epsilon = 0; epsilon < basiscutoff; epsilon++)
	C[k][epsilon] = hHF[epsilon][k];
    //Compute the energy. Can you think of an alternative strategy?
    energy = calcEnergy(C);
    cout << energy << endl;
  } while(abs(energy-energyprev) > 1e-6);
\end{lstlisting}
}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{Hartree-Fock code}
\begin{small}
{\scriptsize
\begin{lstlisting}
  ofstream orb1s("orb1s"); ofstream orb2s("orb2s");
  //Write 1s and 2s wave functions to files
  double sum;
  for (double r = 0; r < 5; r += .01) {
    //1s
    sum = 0;
    for (int alpha = 0; alpha < basiscutoff; alpha++)
      sum += C[0][alpha]*radial(alpha/2+1,r);

    orb1s << setprecision(8) << r << " " << sum << endl;
    //2s
    sum = 0;
    for (int alpha = 0; alpha < basiscutoff; alpha++)
      sum += C[2][alpha]*radial(alpha/2+1,r);
    orb2s << setprecision(8) << r << " " << sum << endl;
  }
\end{lstlisting}
}
\end{small}
}





\frame[containsverbatim]
{
  \frametitle{Hartree-Fock code, sorting of eigenvalues and vectors}
\begin{small}
{\scriptsize
\begin{lstlisting}
void sort(double** A, double* d, int n) {
  double* temp = new double[n];
  double tempval;
  do {
    for (int i = 0; i < n-1; i++) {
      if (d[i] > d[i+1]) {
	tempval = d[i];
	d[i] = d[i+1];
	d[i+1] = tempval;
	for (int j = 0; j < n; j++)
	  temp[j] = A[j][i];
	for (int j = 0; j < n; j++)
	  A[j][i] = A[j][i+1];
	for (int j = 0; j < n; j++)
	  A[j][i+1] = temp[j];
      }
    }
  } while(!isSorted(d,n));
  delete[] temp;
}
\end{lstlisting}
}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{Hartree-Fock code, sorting of eigenvalues and vectors}
\begin{small}
{\scriptsize
\begin{lstlisting}
//Function to sort eigenvalues
bool isSorted(double* d, int n) {

  for (int i = 0; i < n-1; i++)
    if (d[i] > d[i+1])
      return false;

  return true;
}

\end{lstlisting}
}
\end{small}
}



\section[Week 17]{Weeks 17}
\frame
{
  \frametitle{Topics for Week 17, April 26-30}
  \begin{block}{Hartree-Fock theory and Density functional theory}
This is the last formal lecture. The rest of the period (May) is devoted
to finalizing the projects and to write the report. Only lab in this period.
\begin{itemize}
\item Repetition from last week with a discussion of project 2, Hartree-Fock theory, wave functions and computation of 
Coulomb matrix elements.
\item How to write the final report
\item  Brief overview of density functional theory 
\end{itemize}
  \end{block}
} 


\frame
{
  \frametitle{The report}
  \begin{block}{What should it contain? A possible structure}
\begin{itemize}
\item An introduction where you explain the rational for the physics case and 
what you have done. At the end of the introduction you should give a brief
summary of the structure of the report
\item Theoretical models and technicalities. This is the methods section.
\item Results and discussion
\item Conclusions and perspectives
\item Appendix with extra material
\item Bibliography
\end{itemize}
  \end{block}
} 
  


\frame
{
  \frametitle{The report}
  \begin{block}{What should I focus on? Introduction}
You don't need to answer all questions in projects 1 and 2 in a chronological order.  When you write the introduction you could focus on the following aspects
\begin{itemize}
\item A central aim is to study the role of correlations due to the repulsion between the electrons.
\item To do this we have singled out three closed-shell atoms
\item We use variational Monte Carlo and try different trial wave functions to see how close we get to experiment/exact result for a given Hamiltonian
\item We test also the wave functions by computing onebody densities and compare these with those obtained with a non-interacting wave function.
\item We perform also the simplest possible many-body calculations, namely
Hartree-Fock and use these wave functions in an improved description of the 
Slater determinant
\item We focus mainly on beryllium, but have also run calculations for helium and neon.
\end{itemize}
  \end{block}
} 


\frame
{
  \frametitle{The report}
  \begin{block}{What should I focus on? Methods sections}
\begin{itemize}
\item Describe the methods (quantum mechanical and algorithms)
\item You need to explain  variational Monte Carlo and Hartree-Fock
\item The trial wave functions. Why do you choose the functions you do?
\item Why do you do importance sampling? And blocking and Conjugate gradient.
You don't need to explain in detail these methods. 
\item You need to explain how you implemented the methods and also say something about the structure of your algorithm and present some parts of your code (Slater det and Jastrow factor). 
\item You can also plug in some calculations to demonstrate your code, such as selected runs from exercises 1a-1d for helium.
\end{itemize}
  \end{block}
} 


\frame
{
  \frametitle{The report}
  \begin{block}{What should I focus on? Results}
\begin{itemize}
\item Focus on beryllium
\item As an example, you should present results for the $\Delta t$ dependence
for beryllium only but keep in the appendix some selected $\Delta t$ for helium and neon.
\item Same applies to the blocking analysis and the conjugate gradient method
\item Same for the onebody densities, focus on beryllium and various wave functions. 
\item Discuss the results from the two many-body methods and the various wave functions. What do we learn?
\end{itemize}
  \end{block}
} 


\frame
{
  \frametitle{The report}
  \begin{block}{What should I focus on? Conclusions}
\begin{itemize}
\item State your main findings and interpretations
\item Try as far as possible to present perspectives for future work
\item Try to discuss the pros and cons of the methods and possible improvements
\end{itemize}
  \end{block}
} 


\frame
{
  \frametitle{The report}
  \begin{block}{What should I focus on? additional material}
\begin{itemize}
\item Additional calculations used to validate the codes
\item Selected calculations for helium and neon, these can be listed with 
few comments
\item Listing of the code if you feel this is necessary
\end{itemize}
You can consider moving parts of the material from the methods section to the appendix. You can also place additional material on your webpage. 
  \end{block}
} 


\frame
{
  \frametitle{The report}
  \begin{block}{What should I focus on? References}
\begin{itemize}
\item Give always references to material you base your work on, either 
scientific articles/reports or books.
\item {\em Wikipedia is not accepted as a scientific reference}. Under no circumstances.
\item Refer to articles as: name(s) of author(s), journal, volume (boldfaced), page and year
in parenthesis.
\item Refer to books as: name(s) of author(s), title of book, publisher, place and year, eventual page numbers
\end{itemize}
  \end{block}
} 
  


\frame
{
  \frametitle{The exam}
  \begin{block}{Dates and structure}
\begin{itemize}
\item Two days: June 11 and June 14. Are these ok? Send me your wishes
for day and time as soon as possible. Actual times are 9-17 both days. 
\item Duration 45 minutes
\item Give a presentation of your report, 30 mins. Slides only.
\item Then questions and feedback.
\item Your final grade will be based on the report, your presentation
and what you have done in total.
\end{itemize}
  \end{block}
} 


\end{document}











 














